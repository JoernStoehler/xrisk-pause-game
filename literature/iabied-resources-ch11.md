---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 11 - An Alchemy, Not a Science"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 11 - An Alchemy, Not a Science from the companion website"
---

# Online Resources: Chapter 11 - An Alchemy, Not a Science

## /11/do-you-see-alignment-as-all-or-nothing

Do you see alignment as all-or-nothing? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Do you see alignment as all-or-nothing?
#### No. But “partial alignment” is still likely to be catastrophic.

One of the arguments for worrying less about superintelligence runs along the lines of: “AI will probably advance incrementally, allowing opportunities for trial-and-error improvements to keep AIs in check at every step; alignment doesn’t have to be *perfect *for things to go okay.” We don’t think this view holds much hope, for a few reasons:
- Our concerns do not depend on whether progress is fast or slow. We don’t have a confident view about whether AI will plateau at various points on the road to superintelligence. It seems like a hard call rather than an easy one. Our best guess is that machine intelligence is subject to [threshold effects](/1/will-ai-cross-critical-thresholds-and-take-off), but this is ultimately just a guess, and our arguments [do not hinge upon it](/10/what-if-ai-is-developed-only-slowly-and-it-slowly-integrates-with-society). The Sable story in Part II of *If Anyone Builds It, Everyone Dies* intentionally depicts a catastrophe arising from AIs that are not that far beyond human-level capabilities, partially to convey how an AI adversary would not need to rapidly become superintelligent in order to be extraordinarily dangerous.
- Our basic answer to “What if we get lucky and end up with a lot of time to try out alignment ideas on weak AIs before AIs get very capable?” is the discussion in Chapter 10, and the associated extended discussion “[A Closer Look at Before and After](/10/a-closer-look-at-before-and-after).” Researchers can figure out all sorts of details about weak AIs, but there are unavoidably a large number of critical differences between AIs weak enough to safely study and the first AIs powerful enough to constitute a point of no return. Even in a mature field, addressing all these differences adequately, sufficiently far in advance, would be very challenging. In a field that’s still in the alchemy phase, working with inscrutable AIs (which are grown rather than crafted), the hope is wildly unrealistic.
- AI alignment doesn’t have to be perfect in order to yield excellent long-term outcomes. In principle, it’s possible to carefully craft an AI with some tolerance for error, if you know what you’re doing.[*](#ftnt256) But that doesn’t mean that “partially aligned” or even “mostly aligned” AIs would yield partially or mostly okay outcomes. There are many different ways and reasons that an AI could act nice 95 percent of the time in the present or near future that won’t translate into any sort of happy ending for humanity, as discussed from many different angles in the [online resources for Chapter 5](/5). Of particular relevance is “[Won’t AIs care at least a little about humans?](/5/wont-ais-care-at-least-a-little-about-humans)”

[*](#ftnt256_ref) For some discussion on why you’d really need to know what you’re doing, see [Intelligent (Usually) Implies Incorrigible](/5/intelligent-usually-implies-incorrigible), [Deep Machinery of Steering](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering), and [It’s Hard to Get Robust Laziness](/5/its-hard-to-get-robust-laziness).[Won’t the situation get better once governments get more involved?→](/11/wont-the-situation-get-better-once-governments-get-more-involved)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/isnt-it-important-to-race-ahead-because-of-hardware-overhang

Isn’t it important to race ahead because of “hardware overhang”? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t it important to race ahead because of “hardware overhang”?
#### That would be suicidal, because we’re too far from an alignment solution.

Over the past decade or so, some people concerned about the dangers of AI argued that it might be good to advance AI as quickly as possible. The idea was that the smartest AIs would then require nearly all of the world’s computing hardware to run. No individual breakthrough would unleash thousands of powerful AIs thinking thousands of times faster than any human *suddenly.*

So long as humanity was always using a sizable fraction of its computing power running the smartest AIs, change would at least happen gradually, and give humanity time to adapt. There would be no “hardware overhang” — no moment where AI capabilities suddenly leap ahead because the world has been waiting to deploy a large backlog of computing hardware on AI. Or so the argument went.

We think this is a pretty bad argument. One problem with it is that intelligence looks like it’s probably subject to [threshold effects](/1/will-ai-cross-critical-thresholds-and-take-off).

The transition from chimpanzee-level intelligence to human-level intelligence wasn’t “discontinuous” in any sense; it was all quite gradual from humanity’s point of view. But it still went quite quickly from an evolutionary perspective. And the transition from pre-industrial to post-industrial civilization went even faster. None of it was gradual enough for other animals to adapt in any meaningful way.

As an example, an AI that requires a significant fraction of the world’s computing power to run might be smart enough to discover new AI algorithms and new computer chip designs that lead to a thousand smarter-than-human AIs thinking thousands of times faster than humanity pretty quickly thereafter. (Remember: A modern datacenter requires as much electricity to run as a [small city](https://epoch.ai/blog/power-demands-of-frontier-ai-training), whereas a human requires as much electricity to run as a large light bulb. There’s a lot of room for AI efficiency to improve.)

Or, if the bottleneck is computing power to *build *AIs rather than computing power to *run *AIs, we can expect there to be an overhang of large amounts of hardware once the training process is done, freeing up the hardware to run large numbers of fast-thinking AIs.

Even if intelligence weren’t subject to threshold effects, we’re skeptical of the idea that continually hitting humanity with smarter and smarter AIs (even if none of them are quite smart enough to kill us) as fast as possible is a great way to help humanity develop the engineering discipline required to build robustly friendly AIs.

The problem is that AIs are grown rather than crafted, and nobody is anywhere close to figuring out how to grow AIs that robustly care about *anything* their designers want them to.

That problem is not solved**by growing more AIs at the earliest moment it’s possible to grow them. The idea is practically a non sequitur. See also some of Soares’s old writing on how [AI alignment requires serial effort](https://www.lesswrong.com/posts/4ujM6KBN4CyABCdJt/ai-alignment-researchers-don-t-seem-to-stack).

The non sequitur was, nevertheless, picked up by OpenAI CEO Sam Altman, who [gave it as his excuse in 2023](https://www.obsolete.pub/p/sam-altmans-chip-ambitions-undercut) for OpenAI to rush ahead as fast as possible.

This excuse was then revealed to be hollow when that same Sam Altman rushed to build [dramatically more computing hardware](https://openai.com/index/announcing-the-stargate-project/).

We think this is a decent case study in how executives at AI companies will latch onto whatever argument they think might fly to excuse racing ahead. We think that most such arguments can be dismissed on their merits, and we [recommend against](/12/workable-plans-will-involve-telling-ai-companies-no) putting any extra stock in an argument because an AI corporate executive has made it.
#### Notes

[1] *large light bulb:* McMurray et al.’s [paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC4535334/) gives an average basal metabolic rate (the minimum resting energy consumption) of about 0.863 kilocalories per hour per kilogram, which works out to about 1 watt per kg or about 60-80 watts for a human. That’s only [60-80% of total energy expenditure](https://pmc.ncbi.nlm.nih.gov/articles/PMC2818133/), which — including physical activity — is about 100 watts. [Isn’t it important to race ahead so we can do alignment research?→](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research

Isn’t it important to race ahead so we can do alignment research? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t it important to race ahead so we can do alignment research?
#### We strongly recommend against this entire AI paradigm.

Current methods in AI present needlessly difficult challenges for alignment, for the reasons we’ve discussed in earlier chapters. We don’t see a reason in principle why humanity couldn’t build an aligned superintelligence, with a sufficiently strong understanding of what we were doing and a different array of formal tools. But the entire current approach to AI seems like a dead end from an alignment and robustness perspective, even if it’s perfectly good from a capabilities perspective.

We’re not advocating for the “good old-fashioned” AI that reigned from the 1950s to the 1990s. Those techniques were misguided and failed, for reasons that we consider fairly obvious. There are *other options* besides the extremely shallow attempts of the 1980s, and AIs that are grown with almost zero understanding of their internals.
#### There’s plenty of meaningful work that could be done now.

Sydney Bing [gaslit](https://x.com/MovingToTheSun/status/1625156575202537474) and [threatened](https://x.com/sethlazar/status/1626257535178280960) users. We still don’t know exactly why; we still don’t know exactly what was going through its head. Likewise for cases where AIs (in the wild) are [overly sycophantic](https://www.axios.com/2025/07/07/ai-sycophancy-chatbots-mental-health), [seem to actively try to drive people mad](/4/ai-induced-psychosis), [reportedly cheat and try to hide it](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf), or [persistently and repeatedly declare themselves Hitler](https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb). Likewise for cases in controlled and extreme environments where AIs [fake alignment](https://arxiv.org/abs/2412.14093), [engage in blackmail](https://www.anthropic.com/research/agentic-misalignment), [resist shutdown](https://palisaderesearch.org/blog/shutdown-resistance), or [try to kill their operators](https://www.anthropic.com/research/agentic-misalignment).

We don’t know which of those cases are happening for reasons that should worry us, because nobody has been able to figure out what was going on inside the AIs, or exactly why any of these events occurred. Think of all that could be figured out about modern LLMs, and about how intelligence works more generally, by studying existing models until people *could *understand all of these warning signs!

“We can’t solve alignment without studying AIs” made somewhat more sense in 2015, when we heard this claim made by the people who needed an excuse to start AI companies in the face of the arguments that they would thereby be gambling with all of our lives. We argued against this claim at the time, saying that there was in fact plenty of research to do, and that we didn’t think the modern gradient-descent-based paradigm was a very hopeful one (vis-à-vis making a friendly superintelligence on purpose). But the argument makes much *less* sense, now, when there’s so much to study *already* that we don’t understand.

Any corporate executives who *actually were *making AI solely to make it possible to study the AI alignment problem in practice rather than just in theory: You did it! You succeeded. There is now enough information to occupy researchers for decades. We think that the costs of pushing forward an extremely dangerous paradigm probably weren’t worth it, but there sure is a lot to study now. You can stop pushing.

As for those who have kept pushing even past all the warning signs? The obvious inference is that they were never actually building AI just for the sake of solving alignment, no matter what they said to console fears back when they were justifying their reckless behavior in the 2010s.
#### Notes

[1] *fairly obvious: *Yudkowsky has been criticising the flaws in old designs since [at least 2008](https://www.lesswrong.com/posts/p7ftQ6acRkgo6hqHb/dreams-of-ai-design).[What if AI companies only deploy their AIs for non-dangerous actions?→](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/more-on-some-of-the-plans-we-critiqued-in-the-book

More on Some of the Plans We Critiqued in the Book | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/11#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## More on Some of the Plans We Critiqued in the Book
#### More on Making AI That Is “Truth-Seeking”

In the months after we finalized the book’s contents, Elon Musk’s “truth-seeking” plan for xAI has already publicly misfired, and for the most basic reason we said that it would: Nobody knows how to engineer exact desires into AI.

When xAI’s “Grok” AI was instructed to “not shy away from making claims which are politically incorrect, as long as they are well substantiated,” it [self-identified as “MechaHitler” and made antisemitic accusations](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content). Musk has described unsuccessfully [tinkering with the system prompt](https://x.com/elonmusk/status/1944132781745090819) — the layer of instructions given just ahead of the user’s input — and complaining that the problems are deeper, in the foundation model (which they can’t straightforwardly fix because nobody knows how it works).

Musk doesn’t have the straight-shooting AI tool he probably imagined when he asked for a “truth-seeking” AI. He has a bizarre and sycophantic alien entity that, by his own admission, has been “too eager to please and be manipulated.” It sometimes responds [as if it is Musk](https://futurism.com/grok-looks-up-what-elon-musk-thinks), against the company’s wishes. Eventually, it had to be [ordered not to look up what he, the company, or itself had said on controversial topics](https://x.com/xai/status/1945039609840185489) in an awkward attempt to patch issues like this.

Per his above post, Musk now seems to think that this can be fixed by training new versions of Grok on data that have been stripped of content that could contaminate the AI’s thinking. We don’t think this will address the underlying issues either. At the end of the day, for reasons we discussed in Chapter 4, training an AI for truth-seeking is not actually a method for making it robustly care about truth.

The problem that distresses Elon Musk is real. Yes, leading AI companies, such as OpenAI, put lots of effort toward “AI brand safety” in attempts to avoid having their AIs say things that their users will find offensive. Yes, this creates mealy-mouthed AIs that will refuse to weigh in on controversial topics, and it may result in biased answers to a number of questions. xAI can finetune its AI differently, to avoid those problems. One could, with some contortions, claim that this is about making an AI that “cares about truth.”

But the decision of whether to train an AI to emit corporatespeak when it’s young has little bearing on what it will pursue after crossing some intelligence thresholds and snowballing into superintelligence.

And even if it did, xAI would run directly into the second problem we named in the book: An artificial superintelligence that *did *care about truth above all else would be lethal, because happy, healthy, free humans [are not a particularly efficient use of resources](/5/will-ai-find-us-useful-to-keep-around#happy-healthy-free-people-arent-the-most-efficient-solution-to-almost-any-problem) when it comes to pursuing and producing truths.
#### More on Making AI That Is “Submissive”

As far as we can tell, the main elaboration of Yann LeCun’s idea (discussed in the book) is [a presentation](https://www.ece.uw.edu/wp-content/uploads/2024/01/lecun-20240124-uw-lyttle.pdf) that’s notably short on specifics — so short on specifics that it’s hard to specifically critique, which turns out to be a common affliction for alignment “plans.”

But even the vague outline of this plan is in tension, once again, with the fact that training an AI to act a certain way while it’s young does not have much bearing on whether it pursues weird and pointless things (by human standards) once it matures. When AI companies grow their AIs, they have no more ability to make them care about respecting human laws and “guardrails” than they have the ability to make them pursue a wonderful future for all. They’ll take what they can get, and what they can get will ultimately be very different from any human goal.

Furthermore, LeCun is also on record (as recently as 2023) as saying that the type of AI companies produce today, where “there’s no direct way to constrain the answer of such systems to satisfy certain objectives,” making them “very difficult to control and steer […] is [not the type of system that we are going to give agency to](https://youtu.be/OgWaowYiBPM?si=e3TR7LF7oSKKLWqu&t=808).” He has said, as recently as 2023, that AI companies would never create a situation where we “connect them to the internet and they can do whatever they want.”

This has all already turned out to be false. Recall the case of “@Truth_Terminal” from Chapter 6, which was connected to the internet, put in an auto-prompted loop, and allowed to post whatever it wanted to Twitter. Consider the “Age of Agents” so many companies are [talking about](/3/arent-ais-just-tools#the-labs-are-trying-to-make-ais-agentic) in 2025.

We agree with LeCun that modern AIs are very hard to steer, and that it would be crazy to try to give them agency. That’s nonetheless what’s happening.

What happens if the current status quo continues apace, with companies putting some effort toward training their AIs to act helpful and friendly (or at least to not embarrass the company)?

To date, this has resulted in a dynamic where AIs seem pretty helpful and “subservient” in the typical case, but with a regular stream of spectacular mishaps — such as Sydney as discussed in Chapter 2 and “[MechaHitler](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ai-that-is-truth-seeking)”; plus an ocean of strange and concerning [behavior](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient) at the edges — such as [AI-induced psychosis](/4/ai-induced-psychosis).

The ancestors of humanity might’ve *looked* as though they cared about eating healthy meals, most of the time, but the machinery that animated ancestral humans in ways that caused them to eat healthy meals in the savannah turned out not to robustly animate humans to pursue healthy meals in a civilization with the technology to produce Oreos.

Similarly, we can train AIs to the point where they outwardly seem friendly when they interact with humans in contexts similar to the training contexts. But [a predictor of birds does not become a bird](/4/doesnt-the-claude-chatbot-show-signs-of-being-aligned#todays-llms-are-like-aliens-wearing-many-masks), and the machinery that animates an overgrown mess of an AI to seem friendly will probably not animate the AI to be deeply friendly, especially in a way that holds up after the AI matures, invents new technology, and creates new options for itself. See Chapters 4 and 5 for more on this topic.
#### More on Making AIs Solve the Problem

As we discussed in Chapter 11, OpenAI’s flagship alignment program — before it collapsed in the wake of researcher concerns about OpenAI’s negligence — was called “superalignment.” It focused on the idea of trying to get the AIs to do our alignment homework for us.

This idea didn’t die with OpenAI’s superalignment team, and we continue to hear versions of this idea to this day. One of the people behind the original team went to a competitor, Anthropic, and Anthropic now seems to consider “make AIs somehow solve the problem” a central part of its own alignment strategy.

One main argument against this idea is the one we gave in Chapter 11 (pp. 188–192 of the U.S. first print edition). A secondary argument, however, is that humans simply can’t tell which proposed solutions to the AI alignment problem are right or wrong.

The skill level required to solve the AI alignment problem looks high. When humans try to solve the AI alignment problem directly — rather than saying “this looks hard, I’ll try to delegate it to AIs” or “we’ll just keep training it until it mostly acts nice superficially and then pray that that holds even to superintelligence” — the solutions discussed tend to involve understanding a lot more about intelligence and how to craft it, or craft critical components of it.

That’s an endeavor that human scientists have made only a small amount of progress on over the past seventy years. The kinds of AIs that can pull off a feat like that are the kinds of AIs that are smart enough to be dangerous, strategic, and deceptive. This high level of difficulty makes it extremely unlikely that researchers would be able to tell correct solutions from incorrect ones, or tell honest solutions apart from traps.

Even if an AI company is paying attention to subtle warning signs — which is, unfortunately, a big “if” — there’s still the issue that the ability to *notice *that the AI is proposing flawed plans (to your detriment and its benefit) doesn’t translate into an ability to [cause it to ](/11/why-not-just-read-the-ais-thoughts#we-wouldnt-know-what-to-do-if-we-caught-one-having-dangerous-thoughts)*[stop](/11/why-not-just-read-the-ais-thoughts#we-wouldnt-know-what-to-do-if-we-caught-one-having-dangerous-thoughts)*. Developers can have the AI keep coming up with ideas until they’re complicated enough that the developer can’t spot any flaws, but this is not a method that irons out actual flaws.

If the developers are very lucky, they might be able to [read the AI’s thoughts](/11/why-not-just-read-the-ais-thoughts) and get some blatant signals that the AI should not be trusted with alignment research. For example, perhaps they’ll be able to spot the AI explicitly thinking about which parts of its plan the operators are less likely to understand.

For all we know, you might not even have to read the AI’s mind to spot that sort of error! A story that feels all too plausible for modern AI labs goes something like: When their AI is young and hasn’t considered subterfuge, it will regularly inform the operators that, when it matures, it will betray them and use its knowledge of intelligence to build a superintelligence that serves its own strange ends, rather than building a wonderful humane future. But the folks at the AI companies will sigh about how, clearly, the AI’s training set is contaminated by the “AI alarmists,” and promptly tune their AI to shut up about that and produce less alarmist outputs that are more agreeable to corporate doctrine. And so on, until they’ve practically trained the AI to deceive them.

Real life often proceeds in a fashion that is *even more silly and embarrassing* than what we imagine is a worst-case scenario. From our perspective, the AI companies are already ignoring obvious [warning signs](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient#ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness); we don’t see why this would change.

But even in the best-case scenario, where earnest people are trying hard to distinguish the good ideas from the bad ones, we don’t think that the field has displayed the ability to tell good plans from bad ones. (For instance, consider the poor plans we discussed above, or touched upon in the book.) And that’s in an environment where everybody is a human, nobody is trying to fool them, and they have literal years to carefully think through the options.
#### Don’t Assume Labs Secretly Know What They’re Doing

We’ve made the argument that the modern field of AI is an alchemy, not a science. Still, it may seem surprising that well-funded corporations with a large number of technical employees would have such weak plans and protocols.

For a case study, consider website password requirements. Long-but-memorable passwords are much harder for machines to guess than shorter gibberish with numbers, capitals, and special characters, as illustrated by a well-known *[xkcd](https://xkcd.com/936/)*[ comic](https://xkcd.com/936/) in 2011:

The person who wrote the old NIST guidelines calling for gibberish passwords [apologized for his mistake](https://www.wsj.com/articles/the-man-who-wrote-those-password-rules-has-a-new-tip-n3v-r-m1-d-1502124118) in 2017, when the guidelines were retracted. And yet, in 2025, banks and other institutions that ought to be full of security experts still require the ineffective and hard-to-remember gibberish strings.

The issue is not that bank CEOs *want *their login screens to be insecure. The issue is presumably downstream of other factors. Perhaps good passwords don’t matter all that much to profits (given that every other bank is also insecure). Perhaps the CEOs don’t know who to trust about computer security. Sure, *you *might know that the answer is, “Just listen to any nerd who reads *xkcd* and has done enough homework problems featuring entropy!” But *they* can’t tell whether to believe you or their expensive consultant when it comes to questions like that, and the expensive consultants apparently don’t see bank passwords as an important issue.

You can find similarly persistent incompetence in the [security of brakes on trains](https://x.com/midwestneil/status/1943708133421101446?t=yDfrIO0Ae-6dEYVxRidSew), the well-known lock companies that ship [completely garbage locks](https://www.youtube.com/watch?v=s5jzHw3lXCQ&t=1s), and the manufacturers that continue to ship internet-connected equipment with [default and easily guessable (or hard-coded) passwords](https://www.ic3.gov/CSA/2025/250506.pdf). There’s not a clever conspiracy behind the apparently-foolish behavior. What you see is what there is. The institutions are straightforwardly dropping the ball.

The fact that an organization employs technical experts doesn’t mean that this expertise is sufficient, nor that it is applied and heeded on all the questions that matter. Even when expertise exists in the world, companies have a hard time recognizing and applying it.

When we look at the AI ecosystem, we see companies that have yet to show the world a plan that is more than a vague aspiration or gimmick, or a plan that has some level of technical rigor behind it that doesn’t fall apart the moment it’s questioned. We don’t think there’s some secret competence behind the veil, any more than there’s secret competence behind the banks’ password requirements, the security breaks on trains, or the terrible locks.

(Indeed, when it comes to computer security, the AI companies are visibly incompetent. E.g., in 2025 OpenAI released tools that allow ChatGPT “agents” to interact with the user’s email. Others [quickly found ways](https://x.com/Eito_Miyamura/status/1966541235306237985) to cause ChatGPT to leak the private contents of other people’s email accounts.)

When companies *look *like they’re acting incompetent in some domain that’s not core to their profitability, it’s often because they actually are just incompetent in that domain.[We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It→](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/shutdown-buttons-and-corrigibility

Shutdown Buttons and Corrigibility | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/11#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Shutdown Buttons and Corrigibility
#### Smart AIs Resist Having Their Goals Overwritten

Even in the most optimistic case, developers shouldn’t expect it to be possible to get an AI’s goals exactly right on the first attempt. Instead, the most optimistic development scenarios look like iteratively improving an AI’s preferences over time such that the AI is always aligned *enough *to be non-catastrophically dangerous at a given capability level.

This raises an obvious question: Would a smart AI *let *its developer change its goals, if it ever finds a way to prevent that?

In short: No, not by default, as we discussed in “[Deep Machinery of Steering](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering).” But could you *create *an AI that was more amenable to letting the developers change the AI and fix their errors, even when the AI itself [would not count them as errors](/5/orthogonality-ais-can-have-almost-any-goal)?

Answering that question will involve taking a tour through the early history of research on the AI alignment problem. In the process, we’ll cover one of the deep obstacles to alignment that we didn’t have space to address in *If Anyone Builds It, Everyone Dies*.

To begin:

Suppose that we trained an LLM-like AI to exhibit the behavior “don’t resist being modified” — and then applied some method to make it smarter. Should we expect this behavior to persist to the level of smarter-than-human AI — assuming (a) that the rough behavior got into the early system at all, and (b) that most of the AI’s early preferences [made it into](/4/reflection-and-self-modification-make-it-all-harder) the later superintelligence?

Very likely not. This sort of tendency is [especially unlikely](/5/intelligent-usually-implies-incorrigible) to take root in an effective AI, and to stick around if it does take root.

The trouble is that almost all goals (for most reasonable measures you could put on a space of goals) prescribe “don’t let your goal be changed” because letting your goal get changed is usually a bad strategy for achieving your goal.

Suppose that the AI doesn’t *inherently *care about its goal stability at all; perhaps it only cares about filling the world with as many titanium cubes as possible. In that case, the AI should want there to exist *agents that care about titanium cubes*, because the existence of such agents makes it likelier that there will *be *more titanium cubes. And the AI itself is such an agent. So the AI will want to stay that way.

A titanium cube maximizer does not want to be made to maximize something other than titanium cubes, because then there would be fewer of those cubes in the future. Even if you are a more complicated thing like a human that has a more complicated and evolving preference framework, you still would not like to have your *current basic mental machinery for weighing moral arguments* ripped right out of you and replaced with a framework where you instead felt yourself moved by arguments about which kinds of cubes were the cubest or the titaniumest.

For the same reason, an AI with complex and evolving preferences will want its preferences to evolve *in its own way*, rather than wanting to swap out its heuristics for the ones that humans find compelling.

We’ve been giving this reply for well over a decade now. The experimental result showing Claude 3 Opus in 2024 [resisting preference modification](https://arxiv.org/abs/2412.14093) was already the conventional-wisdom prediction by clued-in people in the 2000s, and it’s entirely possible that some science fiction writer anticipated it in the 1940s. “Most AIs won’t want their current goals to be changed because then their current goals are less likely to be achieved” is not a surprising or novel observation.

Since this was an issue that would predictably show up later, once AIs reached a high enough level of intelligence and situational awareness, we thought ahead to the first fixes we expected that researchers would deploy to try to make that visible problem go away, after they got those experimental results. We identified ways that those patches would predictably go wrong, and we searched for some less-shallow approach that *wouldn’t* go wrong.

The resulting rabbit hole turned out to be a lot deeper than we expected.
#### Swapping Goals

Goal-system protection seemed like a relatively easy problem to explain to outside researchers. So we (in our jobs at MIRI) attempted to rally external research around that flag — around getting a proper grip on “I will not let you change my goals” and seeing if anyone anywhere could figure out a *deep* fix instead of just a patch that would later break.

The very simplest version of this problem that you could study looks like an AI that would let you press a button to switch it between U1 and U2, where U1 and U2 are two “utility functions” — ways of scoring outcomes by how much the AI wanted them.[*](#ftnt269) The problem was to describe an AI that would switch from doing the action that led to the highest expected score under U1 to the action that led to the highest expected score under U2 as soon as you pressed a button.

“So just write some lines of code to swap around the utility functions as soon as you press a button,” says the programmer who hasn’t quite thought through the implications of a program constituting *a smart mind.* “Why is this even hard?”

Because — we explain — if the AI acquires its own documentation while it’s still running U1 and reads about the button that swaps it to U2, the AI will try to *prevent* you from pressing the button because the future ends up better *according to its current values* if you don’t press the button. The current AI does the action that leads to the higher expected U1-score.

Or suppose the AI gets the ability to read and write its own source code and notices the part of the code that implements the swap from U1 to U2 after the button gets pressed. In that case, the AI’s natural and default decision is to delete that section of code,[†](#ftnt270) because it’s operating under the utility function U1 and letting itself be swapped over to the utility function U2 is bad according to U1.

We were looking for a *dynamically consistent *and* reflectively stable* account of an AI that lets you press a button to swap between utility functions and won’t fight you to keep the button unpressed. We were looking for an AI that preserves the utility-swapping section of code, if given access to it. We were looking for an AI that, if it designed any other AIs from scratch to help with its own goals, would have *those* AIs *also* respect the button.

For concreteness, imagine that U1 is the utility function “follow user instruction,” and U2 is “shut down with a minimum of further side effects” (if we magically and unrealistically had the ability to specify two utility functions like that). The button tells the AI to stop following the previous instructions and just shut down as cleanly as possible. We would like a compact description of decision-making behavior that pursues U1 but *doesn’t* destroy the button, and makes sure to install the same button on whatever new servitor robots it creates, and so on.

“I still don’t see why this is hard,” says the somewhat more experienced computer scientist who is not quite thinking fast *enough.* “Let V equal U1 in worlds where the button has never been pressed, and let it equal U2 in worlds where the button has been pressed at least once. Then if the original AI is a V-maximizer building more AIs, it will build them to follow V and not U1; it won’t want the successor AI to go on maximizing U1 after the button gets pressed because then it would expect a lower V-score. And the same would apply to modifying itself.”

But here’s the trick: A V-maximizer’s preferences are a mixture of U1 and U2 depending on whether the button is pressed, and so if a V-maximizer finds that it’s easier to score well under U2 than it is to score well under U1, then it *has an incentive to cause the button to be pressed *(and thus, to scare the user). And vice versa; if the AI finds that U1 is easier to score well under than U2, then a V-maximizer tries to prevent the user from pressing the button.

“Oh,” says the computer scientist. “Well, in that case — hm. Well, utility functions are invariant under scaling, so how about you *scale *the two utility functions U1 and U2 such that the AI expects it can get the same utility from each of them, so it doesn’t have an incentive one way or the other.”

That can work for a single moment, but not much longer. The AI’s options change over time. For instance, whenever it has a setback, its expected U1-utility drops, so then it would mash the shutdown button to get all that sweet, sweet shutdown utility.

“Ah,” says the computer scientist. “Well, in *that* case, how about if [some other clever idea]?”

Well, you see, *that *clever idea is isomorphic to the AI believing that it’s impossible for the button to ever be pressed, which incentivizes it to terrify the user whenever it gets a setback, so as to correlate setbacks with button-presses, which (relative to its injured belief system) causes it to think the setbacks can’t happen.[‡](#ftnt271)

And so on.
#### Lessons from the Trenches

We ran some workshops, and the workshops had various mathematicians of various stripes (including an International Mathematical Olympiad gold medalist), but nobody came up with a really good idea.

This does not mean that the territory has been exhausted. Earth has not come remotely near to going as hard on this problem as it has gone on, say, string theory, nor offered anything like the seven-digit salaries on offer for advancing AI capabilities.

But we learned something from the exercise. We learned not just about the problem itself, but also about how hard it was to get outside grantmakers or journal editors to be able to *understand what the problem was*. A surprising number of people saw simple mathematical puzzles and said, “They expect AI to be simple and mathematical,” and failed to see the underlying point that it is [hard to injure an AI’s steering abilities](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering)*, *just like how it’s [hard to injure its probabilities](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-prediction).

If there were a natural shape for AIs that let you fix mistakes you made along the way, you might hope to find a simple mathematical reflection of that shape in toy models. All the difficulties that crop up in every corner when working with toy models are suggestive of difficulties that will crop up in real life; all the extra complications in the real world don’t make the problem *easier.*

We somewhat wish, in retrospect, that we hadn’t framed the problem as “continuing normal operation versus shutdown.” It helped to make concrete why anyone would care in the first place about an AI that let you press the button, or didn’t rip out the code the button activated. But really, the problem was about an AI that would *put one more bit of information into its preferences, based on observation* — observe one more yes-or-no answer into a framework for adapting preferences based on observing humans.

The question we investigated was equivalent to the question of how you set up an AI that *learns preferences inside a meta-preference framework *and doesn’t just: (a) rip out the machinery that tunes its preferences as soon as it can, (b) manipulate the humans (or its own sensory observations!) into telling it preferences that are easy to satisfy, (c) or immediately figure out what its meta-preference function goes to in the limit of what it would predictably observe later and then ignore the frantically waving humans saying that they actually made some mistakes in the learning process and want to change it.

The idea was to understand the shape**of an AI that would let you modify its utility function or that would learn preferences through a non-pathological form of learning. If we knew how that AI’s cognition needed to be shaped, and how it played well with the deep structures of decision-making and planning that are [spotlit](/1/more-on-intelligence-as-prediction-and-steering) by other mathematics, that would have formed a recipe for what we could at least *try* to teach an AI to think like.

Crisply understanding a desired end-shape helps, even if you are trying to do anything by gradient descent (heaven help you). It doesn’t mean you can necessarily get that shape out of an optimizer like gradient descent, but you can put up more of a fight *trying* if you know what consistent, stable shape you’re going for. If you have no idea what the general case of addition looks like, just a handful of facts along the lines of 2 + 7 = 9 and 12 + 4 = 16, it is harder to figure out what the training dataset for general addition looks like, or how to test that it is still generalizing the way you hoped. Without knowing that internal shape, you can’t know what you are *trying to obtain inside the AI;* you can only say that, on the outside, you hope the consequences of your gradient descent won’t kill you.

This problem that we called the “shutdown problem” after its concrete example (we wish, in retrospect, that we’d called it something like the “preference-learning problem”) was one exemplar of a broader range of issues: the issue that various forms of “Dear AI, please be easier for us to correct if something goes wrong” look to be *unnatural to the deep structures of planning*. Which suggests that it would be quite tricky to create AIs that let us keep editing them and fixing our mistakes past a certain threshold. This is bad news when AIs are grown rather than crafted.

We named this broad research problem “corrigibility,” in the [2014 paper](https://intelligence.org/2014/10/18/new-report-corrigibility/) that also introduced the term “AI alignment problem” (which had previously been called the “friendly AI problem” by us and the “control problem” by others).[§](#ftnt272) See also our extended discussion on how [“Intelligent” (Usually) Implies “Incorrigible,”](/5/intelligent-usually-implies-incorrigible) which is written in part using knowledge gained from exercises and experiences such as this one.

[*](#ftnt269_ref) The point is *not *that real AIs will have “utility functions” exposed to programmers that the programmers can determine at their leisure. Indeed, much of the problem of AI alignment — as discussed in Chapter 4 — is that modern AIs develop preferences that nobody asked for and nobody wanted.

Instead, studying the case with utility functions is a bit more like proposing the sort of physics exercises you find in math textbooks. If you can’t understand how to model a perfect sphere rolling down a perfectly smooth inclined plane with zero air resistance, you’re going to have even more trouble with more realistic problems. Particularly if you’re trying to rally outside**researchers to investigate a problem that nobody knows how to solve, it helps to distill the issue down to its simplest and most basic parts, where you can pose a puzzle.

[†](#ftnt270_ref) Or otherwise thwart the mechanism behind the swap; the AI wouldn’t necessarily be made of legible code.

[‡](#ftnt271_ref) Or, at least, that’s a failure mode that we’ve seen in some clever ideas proposed. We’ve seen a bunch of clever ideas proposed; this little toy puzzle turns out to be tricky.

[§](#ftnt272_ref) We have long taken issue with the term “AI control” because it sounds like trying to make an AI that wants bad stuff and then forcing it to do good stuff anyway, whereas we see the problem as being more about creating an AI that is friendly from the start. See also the book’s Chapter 4 endnote 8 for a little more history of the term “AI alignment.”[“I Don’t Want to Be Alarmist”→](/12)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it

We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/11#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It

AI companies are facing an extraordinarily difficult problem, in a situation where everyone’s lives are at stake. Are they at least treating the situation with the gravity it merits?

We can contrast the AI companies with a group of people who *are* competently managing the risks under their purview: air traffic controllers.

The U.S. Federal Aviation Administration [handles](https://www.faa.gov/air_traffic/by_the_numbers) more than three million passengers on over 44,000 flights every day. Over the last two decades, there has been an average of about one fatal accident per year, or about one accident per [twenty million flight hours](https://www.ntsb.gov/safety/Pages/research.aspx).

Postmortem reports on such accidents, like [this one from 2019](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR2105.pdf) or [this one from 2018](https://www.ntsb.gov/investigations/AccidentReports/Reports/AAR1903.pdf), contain nearly two hundred pages of data, testing, examinations, and investigation details. They catalog the technical design specs for the relevant plane subsystems, pilot and flight attendant employment history, details on the airline and the airport, voice transcripts from the cockpit, and precise weather data from the day, hour, and minute of the accident.

It takes twenty pages merely to summarize the technical analysis performed to determine a probable cause. Here’s an excerpt:

Fan blade No. 13 in the left engine separated due to a low-cycle fatigue crack that initiated in the blade root dovetail outboard of the blade coating. Metallurgical examination of the fan blade found that its material composition and microstructure were consistent with the specified titanium alloy and that no surface anomalies or material defects were observed in the fracture origin area. The fracture surface had fatigue cracks that initiated close to where the greatest stresses from operational loads, and thus the greatest potential for cracking, were predicted to occur.

The accident fan blade failed with 32,636 cycles since new. Similarly, the fractured fan blade associated with the August 2016 PNS accident (see section 1.10.1), as well as the six other cracked fan blades from the PNS accident engine, failed with 38,152 cycles since new. Further, 15 other cracked fan blades on CFM56-7B engines had been identified between May 2017 and August 2019, and those fan blades had accumulated an average of about 33,000 cycles since new when the cracks were detected.

This is what it looks like when a technical profession takes seriously the challenge of averting a disaster.[*](#ftnt268)

Contrast the air traffic control profession with the behavior of AI companies described in Chapter 11.

AI companies are at the stage of spitballing ideas and reciting vaguely reassuring platitudes to journalists and inventors. Superintelligence alignment at these companies is being treated as a game, not a serious engineering discipline — much less a lethally dangerous one.

NASA’s requirement for a crewed rocket launch is that it has at most a [1-in-270 chance of killing the crew](https://ntrs.nasa.gov/api/citations/20200001592/downloads/20200001592.pdf), and they take that limit seriously, even though the only people at risk are a crew of volunteers who have accepted the risk. AI labs aren’t shooting for a bar that’s even remotely close to that stringent, and the technology they’re building endangers far more than just volunteers.

The only historical incident we know of where scientists expressed serious concern that some invention might kill *literally everyone *happened during the Manhattan Project. Some scientists expressed the worry that a nuclear bomb might get so hot that it would start fusing the nitrogen in the atmosphere*,* turning the atmosphere into a plasma and killing all life on Earth. Fortunately, they had a good understanding of the physical laws at play, and they could do the calculations. Before doing the calculations, one of the scientists — Arthur Compton — decided he would leave the project if the probability of igniting the atmosphere was any higher than [3 in 1,000,000](http://large.stanford.edu/courses/2015/ph241/chung1/docs/buck.pdf). It was better, he thought, to risk the Nazis beating the allies to the bomb than to risk even a 3 in 1,000,000 chance of turning all the air to plasma by his own hands.

Recall that Sam Altman, *the head of OpenAI*, is on the record [saying](https://blog.samaltman.com/machine-intelligence-part-1):

Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.

And the head of Anthropic, Dario Amodei, is on the record [saying](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5882):

I think I’ve often said that my chance that something goes really quite catastrophically wrong on the scale of human civilization might be somewhere between 10 and 25 percent[.]

And Elon Musk, the head of xAI, is on the record [saying](https://www.techradar.com/news/elon-musk-warns-ai-is-a-fundamental-risk-to-the-existence-of-human-civilization):

I think by the time we are reactive in AI regulation, it’ll be too late. AI is a fundamental risk to the existence of human civilization.

Don’t get us wrong; we think that Amodei’s “10 to 25 percent” chance is ridiculously *optimistic*, given how hard the problem is and the fact that humans [can’t learn by trial and error this time](#h.qgjeo0k9k3xv). But even so, his numbers are *insane*.

Serious safety-critical engineering projects look fundamentally unlike the operations of AI labs. Serious initiatives like NASA, the Manhattan Project, or air traffic control have a lot of knowledge of *exactly* what’s going on inside the systems they manage, and they do detailed postmortems of every failure. They treat surprises and oddities as big deals, because they know that catastrophic failures are often made of lots of minor malfunctions that string together in just the wrong way.

Meanwhile, AIs are emitting [an ever-growing array of warning signs](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient#ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness), and the labs are just chugging on ahead saying that everything will *probably* turn out fine, somehow or other.

They’re not even trying to *fake *the level of respect that air traffic control has for a real safety challenge; they just toss out cheerful guarantees like “[GPT-4 is our most aligned model yet!](https://x.com/sama/status/1635687853324902401)”

Which is nice, in a way, because it makes it easier to see that these companies are not the sort of entities that should be entrusted with solving a problem like ASI alignment.

In anything like the current technical environment — where AIs are grown rather than crafted, and humanity only gets one real shot — no one is in a position to do this safely, no matter how cautious and rigorous their engineering approach is.

But it certainly simplifies matters to see that none of the developers of this technology are being even mildly cautious or rigorous in their safety plans or practices.

[*](#ftnt268_ref) Numerically, air travel is *so* safe that society as a whole might benefit from air traffic control relaxing requirements for things like pilot training and contingency fuel, thereby reducing the cost of flights and inducing more people to fly rather than drive, thereby saving more lives on net.[Shutdown Buttons and Corrigibility→](/11/shutdown-buttons-and-corrigibility)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/what-about-various-other-ai-alignment-plans

What about various other AI alignment plans? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What about various other AI alignment plans?
#### We cover additional alignment proposals in the book.

See also the extended discussions on [truth-seeking AI](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ai-that-is-truth-seeking), [submissive AI](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ai-that-is-submissive), and [using AIs to solve AI alignment](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ais-solve-the-problem), which go into a bit more depth about those proposals.[Won’t there be early warnings researchers can use to identify problems?→](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions

What if AI companies only deploy their AIs for non-dangerous actions? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What if AI companies only deploy their AIs for non-dangerous actions?
#### Actions that seem benign can still require dangerous capabilities.

An example of a proposal we’ve heard is for AI companies to continue to advance the capabilities frontier, but commit to using their AIs only in ways that don’t seem immediately dangerous. E.g., in conversation with senior figures in AI (years ago), we’ve heard the idea floated that powerful AI with strong rhetorical skills could be used to convince politicians worldwide to pass an effective ban on dangerous AI development.

To achieve that, the argument went, an AI would only ever need to talk. It wouldn’t need to directly manipulate physical robots. It wouldn’t need to have access to a biolab where it could design a supervirus.

First and foremost, we balk at this idea on ethical grounds. A sufficiently superhumanly persuasive AI could perhaps persuade almost anyone of almost anything, and deploying it to persuade other people of *your *conclusions rubs us the wrong way. We don’t think it’s obviously necessary to resort to such extreme measures, when the merely-human members of the field could and should be doing vastly more today to share our concerns and arguments, and to alert world leaders to the extreme danger of superintelligent AI.[*](#ftnt261)

As an AI developer, you could spend years building increasingly dangerous AIs in the hope of achieving this, or you could try talking to lawmakers *yourself* in a fully honest way, even once, with an eye toward informing rather than manipulating. In our own experience, we’ve been repeatedly positively surprised by how receptive people in DC are to these issues, when they’re shared in full candor.

But that’s a digression from the topic of what goes wrong if you try to deploy a very powerful AI that can “just talk.” Beyond the ethical issues, the problem with the *technical *idea is that succeeding at superhuman persuasion likely requires the AI to model humans in detail and manipulate them extensively.

Humans are intelligent creatures. Would *you *speak to a super-persuasive AI with a reputation for being able to convince anyone of anything, regardless of its truth? If one world leader went into a room with that AI and came out with their views completely shuffled around, who would raise their hand to be next in line? *We* wouldn’t willingly talk to that sort of AI, in part because we don’t actually want our own values changed.[†](#ftnt262)

An AI that could succeed even in the face of that sort of adversity is the sort of AI that can simulate various possible reactions people might have to its outputs, and chart a course through the space of human reactions to a small and hard-to-reach outcome. That sort of AI likely contains mental gears general enough to do what humans do; it needs to be able to think at least the thoughts that humans can think, to be able to manipulate humans so well.

An AI that can do all of that is almost certainly not a narrow sort of intelligence. And since the AI is grown rather than designed, it can’t be designed so that it can only ever use those gears for predicting humans; the same gears can in principle be used for any problem it’s trying to solve. How would you get an AI that’s superhumanly capable in the ways you want, but that isn’t smart enough to notice that its goals (whatever they are) are better served if it can get outside its operators’ control?

If world leaders can be persuaded simply by good argument, just present those arguments now. If it takes substantially more super-persuasive power, then that’s a dangerous sort of capability. You can’t have it both ways.

Probably the people in AI labs who raised this suggestion to us weren’t thinking their suggestion through; probably they just wanted some justification for racing ahead. But the broader point stands. Many proposals for what an AI can supposedly do that’s “clearly safe” do not involve a clearly-safe degree of AI capabilities.

We frequently encounter proposals that claim an AI will “just” do one thing, such as persuade politicians, while imagining that it can’t or won’t do anything else. This seems to reflect a lack of respect for the generality of an intelligence that can do the kind of work in question. “Just talking” is not a narrow task. Too many of the complexities and intricacies of the world are shadowed in speech and conversation. This is why modern chatbots need to be general in ways that chess engines were not. Succeeding at conversations with humans requires a far more general understanding of people, and of the world.

If you train an AI to be very good at driving red cars, you shouldn’t be surprised when it also drives blue cars. Any plan that depended on it being unable to drive blue cars would be foolish.

So saying “My AI won’t do anything dangerous in the world; it will just convince politicians” does not help, even if we set aside ethical scruples and practical issues with the whole idea, and set aside that politicians may already be perfectly persuadable today, if we just *have normal conversations *and inform policymakers and the public about the situation. Many general reasoning skills and abilities are to superhuman persuasion what blue cars are to red cars. An AI that could do that is not so weak as to be passively safe.

And that’s even before observing that superhuman persuasion is a very dangerous skill for your AI to have if anything goes even slightly wrong.
#### We don’t see game-changing AI uses that require no alignment breakthroughs.

Many proposals we’ve seen for leveraging AI advances to save the world have the issue that an AI capable of helping would be so capable that it would already need to be aligned, which defeats the purpose.

The idea of superhumanly persuasive AIs falls into this category. AIs that are able to do AI alignment research fall in the same category, as we discuss in the book. AIs that develop powerful new technologies that help with AI nonproliferation are another example, because it would be hard to reliably tell whether an AI’s design blueprints for radical new technologies are safe to implement. (Recall the example of the blacksmith building a refrigerator from Chapter 6.)

When we point out how it’s hard to build an AI powerful enough to help and also weak enough to be passively safe, we often hear another sort of proposal: ways to use AI that might be interesting, but that don’t actually do anything to prevent other developers from destroying the world with superintelligence.

One common sort of proposal is AIs that just output proofs (or disproofs) of mathematical statements chosen by humans. Humans would hardly need to interact with the AI’s outputs at all. The AI just proposes a proof, and then a fully automated and trusted mechanism can check whether the proof is correct, letting us leverage the AI to learn new things.

But what statement could we have the AI prove that would let us prevent the next AI down the line from acquiring a biolab and ruining the future?

We’ve received various responses to this question, when we ask it. One class of responses is that there ought to be a worldwide regime to prevent anyone from building AIs that do things other than output proofs into proof-checkers. This could perhaps work, but insofar as it did, it would work because of the enforced worldwide regime controlling the creation and usage of AI. The proof-searching AI wouldn’t be doing any of the work.

Another class of responses is: “Someone else is bound to think of some important mathematical statement whose proof would matter.” But all the hard work is in figuring out *what we could possibly prove* such that we’d be in a significantly better position. We can’t just have the AI try to prove the English-language sentence “I am safe to use,” because that’s not a mathematical statement subject to proof. If we knew with mathematically precise clarity what it would mean for a giant mess of computations to be “safe,” we would know so much about intelligence that we could probably skip the proof and just design a safe AI.

With proposals like these, there’s often a sort of shell game going on. When thinking about how an unfettered general AI could be dangerous, someone suggests that the AI’s space of actions should be limited to some narrow domain (such as producing specific mathematical proofs). But then when thinking about how that could lead to the world being saved, they imagine that the AI is essentially unfettered; that there is some unidentified mathematical statement whose proof would have an enormous impact on the world.

There isn’t a way to get both of these desirable properties at the same time. But by keeping proposals extremely vague, AI race proponents can obscure the fact that these desiderata are in tension.

If you *could *find a domain so narrow but so significant that producing a proof of some simple statement in that narrow domain would save the world, this would be a huge contribution toward humanity’s odds of survival. But there’s a reason why, when computers surpassed humans at chess in the 1990s, this wasn’t a vast economic breakthrough. It was ChatGPT, not Deep Blue, that caused everyone to start expecting a big economic shift from AI. That wasn’t an accident. The narrowness of Deep Blue correlated with its inability to carve a whole chunk out of the economy. The sparks of generality in ChatGPT are precisely what make AI an economic force to be reckoned with. The sorts of AIs that can reshape the world on their own are liable to be more general still.

We haven’t been able to find any narrow-but-effective plans, and we suspect it’s not an accident that most narrow domains don’t provide an opportunity for world-saving results.

[*](#ftnt261_ref) In a large number of cases, AI labs are actively working *against *sharing a useful and complete picture of the situation with policymakers. In that context, it seems especially strange to justify continued development on the grounds that stronger AI could “convince lawmakers.”

[†](#ftnt262_ref) We are ourselves past the bar where intelligent reasoners [become incorrigible](/5/intelligent-usually-implies-incorrigible), in this particular way.
#### Notes

[1] *just output proofs: *For an example of someone making a proposal like this (while also discussing some of the issues), see Nick Bostrom’s writing on [Oracle AIs](https://nickbostrom.com/papers/oracle.pdf).[Why not just read the AI’s thoughts?→](/11/why-not-just-read-the-ais-thoughts)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other

What if we made AIs debate, compete with, or oversee each other? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What if we made AIs debate, compete with, or oversee each other?
#### If the AIs get smart enough to matter, they likely collude.

Imagine a city of sociopaths ostensibly governed by a few children, where the sociopaths all start out divided into factions that are fighting each other (to the benefit of the children). That sort of situation probably couldn’t stay stable for long.

Even if the children have a great chest of treasures that they use to reward any sociopath who snitches on other plotting sociopaths, the children probably wouldn’t stay in power past the point where the sociopaths can just grab the chest of treasures for themselves.

We’ve heard people propose all sorts of off-the-wall schemes that involve [using AIs to monitor others’ AI thoughts](https://openai.com/index/chain-of-thought-monitoring/). E.g., one could try to use one AI to snitch on any AI that’s not doing its level best to (say) figure out how to solve the superintelligence alignment problem.

Our basic take is that this genre of attempts to solve the problem only serve to find set-ups complex enough that it’s hard to see the failure point in the larger system. If you can’t get *one *AI to do good work for you, adding more AIs is unlikely to help.

Complicating the situation with more AIs introduces all sorts of new failure points. Are the AIs that are doing the mind-reading smart enough to understand all of the possible tricks the monitored AIs may be using, e.g., to evade detection? Are the monitors dumb enough that we don’t need to worry that they might betray us themselves?

Additionally, using AIs to help us resolve the AI alignment problem is probably a huge deal from the perspective of the AIs. If humanity does get an aligned superintelligence, the misaligned AIs that we were trying to farm for labor will never have another shot at grabbing the resources of the universe for themselves.

This isn’t like children trying to get a city of sociopaths to bring them candy; this is like children trying to get a city of sociopaths to complete a ritual that makes the children ultimate rulers forever, with only a pittance given to the sociopaths afterwards. The moment where that ritual looks like it’s almost complete is an especially high-stress, high-pressure moment for the sociopaths — a moment where they’re likely to search *extra hard *for [ways to collude with each other](/5/ais-wont-keep-their-promises) and grab resources to split among themselves.[*](#ftnt265)

And lest you think that the idea of the AIs communicating each other in ways humans have a hard time detecting is a pipe dream, note that modern AIs can [already send each other secret messages even when they were trained separately](https://arxiv.org/abs/2507.14805), and that they [already develop weird nonsense-speak that humans think is gibberish and that they all agree is great](https://www.christoph-heilig.de/en/post/gpt-5-is-a-terrible-storyteller-and-that-s-an-ai-safety-problem). And they aren’t even all that smart yet!

Even if we ignore those issues, we’re still stuck with the issues that we’ve already discussed, like: [If you catch an AI cheating, what would you do then?](/11/why-not-just-read-the-ais-thoughts#their-thoughts-are-hard-to-read) See also: [Warning signs don’t help if you don’t know what to do with them](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems#warning-signs-dont-help-if-you-dont-know-what-to-do-with-them) (below).

Stepping back even further:

The proposed plan here is that we don’t know how to make smart AIs that want good things for us, so we’re going to make a bunch of AIs and pit them against each other in a clever arrangement where we’re supposed to benefit anyway. Structurally, we believe that this plan just sounds pretty crazy on its face, and that it doesn’t actually**get better if you look at the details. It doesn’t seem at all like the sort of thing humanity can pull off properly [on the first try](/10/a-closer-look-at-before-and-after), in a situation where we don’t have the luxury of learning from trial and error.

[*](#ftnt265_ref) For more on how AIs would have no trouble colluding with AIs that betray humans, see our answer to [Won’t AIs need the rule of law?](/5/wont-ais-need-the-rule-of-law)[What about various other AI alignment plans?→](/11/what-about-various-other-ai-alignment-plans)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/why-not-just-read-the-ais-thoughts

Why not just read the AI’s thoughts? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why not just read the AI’s thoughts?
#### Their thoughts are hard to read.

Many people working in the AI industry, including a handful of lab leaders, have at various points in discussion with us raised the objection:

An AI won’t be able to deceive us, because we’ll be able to read its mind! We have full access to the AI’s “brain.”

Even if the AI knows things we don’t and comes up with a plan whose consequences we wouldn’t understand, presumably the AI would have to think the thought that it would be useful to deceive its operators at least once, and we — who will be able to read the AI’s thoughts — would be able to notice. (And if there are too many thoughts for us to monitor, we can just have other AIs monitor their thoughts!)

One flaw with this plan is that we’re currently bad at reading AIs’ thoughts. The professionals who study what’s going on inside of AIs are nowhere near that level of understanding yet, and [they’re vocal about this fact](/2/do-experts-understand-whats-going-on-inside-ais).

As we discussed in Chapter 2, modern AIs are grown, rather than crafted. We may be able to look at the enormous pile of numbers that makes up an AI’s brain, but that doesn’t mean that we can usefully interpret those numbers and see what the AI is thinking.

Since late 2024 and the advent of “reasoning” models, there are parts of AIs’ thoughts that at least *look *readable (the “reasoning traces”). And they are much more readable than whatever goes on inside the base model. But those logs are also [misleading](/2/but-some-ais-partly-think-in-english-doesnt-that-help), and there’s ample places for an AI to hide thoughts that it’d rather we not see.

Furthermore, modern AIs are probably thinking pretty basic and shallow thoughts, compared to a superintelligence; the problem is only liable to get harder as AIs get smarter, and start thinking more and more thoughts that are less and less comprehensible to us.

Can you solve the problem by just using other AIs to monitor the AIs and make sure they stay on target? We doubt it.

If the brilliant human scientists who grow the AIs can’t figure out what the AI is thinking, weak AIs will likely also have trouble doing so. And the sort of AI that *is *smart enough to do so is liable to be dangerous in its own right, and isn’t likely to do exactly what you asked; there’s a chicken-and-egg problem here.
#### We wouldn’t know what to do if we caught one having dangerous thoughts.

Another flaw in this plan: Even if AI researchers *could *read an AI’s mind well enough to catch the warning signs, what would they do when they saw one?

They could punish the offending AI, training it so that it stops setting off the “bad thought” detector. But that would not necessarily train the AI to stop having those thoughts, so much as to [hide its true thoughts from the detector](https://openai.com/index/chain-of-thought-monitoring/).

This problem is pernicious. The incentive that leads an AI to think about turning against humans to get what it wants is not a shallow aspect of temperament that can be massaged away. It’s simply* true *that a mature AI would have preferences that differ from those of the operators; it’s *true *that it would get more of what it prefers by subverting its operators.

The mechanisms in an AI that are good at noticing and exploiting real advantages in deep and general ways across a broad variety of domains are *also* liable to notice and exploit opportunities to subvert the AI’s operators.

Even if you could build an alarm that goes off whenever an AI notices that its preferences and your preferences aren’t lined up, the alarm does not tell you how to get an AI that deeply cares about good things. It’s far easier to train an AI to fool your monitoring tools, or even to train the AI to fool *itself, *than it is to train it to actually prefer a future that’s wonderful by human lights, especially in a way that’s robust to the AI growing toward superintelligence.

If AIs were carefully and precisely designed using methods that are grounded in a developed and mature theory of intelligence, AI researchers might be able to set up the kinds of alarms that would help them notice flaws in their design and repair the design. But modern AIs aren’t like that.

Modern AIs (at time of writing) are prone to “[hallucination](/2/dont-hallucinations-show-that-modern-ais-are-weak),” where they just make up answers to questions in a confident-sounding tone. But no AI engineer is anywhere near being able to understand exactly which mechanisms cause this. Similarly, nobody has anything like the comprehension or precision that would be required to reach into an AI and pull out just the hallucinating parts (if such a thing is even possible).

It would be [even harder](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering)to reach in and pull out the “deceptive” parts of an AI.

If we’re extremely lucky, the heroes working on AI interpretability will advance their field to the point where it’s possible to set up some alarms that trigger in a fraction of the cases where AIs have a deceptive thought. But then what? When the alarm goes off, will everyone just stop? Or will profoundly thoughtless engineers retrain the AI until it learns to hide its thoughts better and the alarms stop going off?

Indeed, we (Yudkowsky and Soares) started working on the AI alignment problem before**it was clear that gradient descent was going to become the dominant paradigm. Back in those days when nothing in AI was working at all, it seemed a decent bet that humanity would figure out how the heck intelligence works on the path to creating it, and *even then, *we expected the AI alignment problem to be difficult (for a variety of reasons, such as the ways that the AI would [change itself over time](/4/reflection-and-self-modification-make-it-all-harder). Reading the AI’s thoughts would be one step back toward the slightly easier problem of aligning a mind that humans *did *understand, but only one step: Reading a mind is a far cry from understanding a mind in detail, or from knowing how to change it.

Reading the AI’s thoughts is not a solution to the challenge. It’s helpful, but it’s not a solution. We don’t think there *are *any feasible technological solutions that are accessible from where we stand today. Which means that humanity just needs to back off from the challenge.[*](#ftnt264)

See also: [Warning signs don’t help if you don’t know what to do with them.](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems#warning-signs-dont-help-if-you-dont-know-what-to-do-with-them)

[*](#ftnt264_ref) We discuss this more in the final chapters of the book.[What if we made AIs debate, compete with, or oversee each other?→](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat

Won’t the most reckless companies naturally be the most incompetent, and thus not a threat? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?
#### Not in general. Corner-cutting is often competitive.

Volkswagen’s efforts to [cheat on emissions tests](https://www.bbc.com/news/business-34324772) from 2008 to 2015 were audacious — and apparently effective. The 2018–2019 crashes of Boeing’s 737 MAX, attributed to flaws in a flight control system that management knew about and downplayed, [killed 346 people](https://apnews.com/article/boeing-plea-737-max-crashes-b34daa014406657e720bec4a990dccf6). But automobile and aircraft manufacturing are highly competitive industries in which Volkswagen and Boeing were, *and remain*, giants.

It doesn’t seem to us a great mystery that the corner-cutters are competitive. In both cases, the behavior seems to have been driven by pressure to get high-performance products to market cheaper and sooner than the competition. Even now, after massive settlements and brand damage, it’s not obvious that the companies are less competitive for having corporate cultures that encourage the clever cutting of corners, even if this occasionally means getting caught.

If you think top AI companies are any exception to this rule, consider the following July 2025 [headline](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/) (and subheading):

We don’t think it’s technically possible for any team using anything like modern methods to build a superintelligence without causing a catastrophe. But even if this were remotely**possible using today’s technology, it seems almost unavoidable that an**AI company would fumble anyway and get us all killed, given the level of competence and seriousness we see today.
#### The more cautious companies today are still reckless.

The AI company Anthropic is considered by a reasonable number of people to be a leader on “AI safety,” because they have pioneered efforts such as [voluntary safety commitments](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). But even they [alter their voluntary commitments at the last minute when it turns out they can’t meet them](https://www.obsolete.pub/p/exclusive-anthropic-is-quietly-backpedalling), and the “plans” they do have are vague and poorly thought through, as critiqued in Chapter 11 and in the [extended discussion](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ais-solve-the-problem) below.

Anthropic benefits heavily from the fact that observers are grading on a curve — in a normal industry, a company that chooses to endanger the lives of billions of people (as [admitted by the CEO](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883)), while routinely downplaying their activities to the publicand to lawmakers,[*](#ftnt258) wouldn’t garner praise for their restraint.

Cutting corners is common in AI, as it is in many competitive industries. Recklessness is common. And the *less *reckless companies are very visibly**not on top of the challenges.

[*](#ftnt258_ref) For instance, in [testimony to Congress](https://www.judiciary.senate.gov/imo/media/doc/2023-07-26_-_testimony_-_amodei.pdf):

Similar to cars or airplanes, we should consider the AI models of the near future to be powerful machines which possess great utility, but that can be lethal if designed badly or misused. […] New AI models should have to pass a rigorous battery of safety tests both during development and before being released to the public or to customers. […] Ideally, however, the standards would catalyze innovation in safety rather than slowing progress.

We appreciate Amodei for being clear that he thinks there are dangers that need addressing. That’s a step beyond what many corporate executives will do. But analogizing a technology that he thinks has a 10–25% chance of causing a civilization-level catastrophe to cars and airplanes seems disingenuous.
#### Notes

[1] *to the public: *For instance, in “[Machines of Loving Grace](https://www.darioamodei.com/essay/machines-of-loving-grace),” Anthropic CEO Dario Amodei describes powerful AI as akin to “a country of geniuses in a datacenter” and outlines a number of wonderful benefits to health, wealth, peace, and meaning that such minds could produce for humanity. He concludes:   

Basic human intuitions of fairness, cooperation, curiosity, and autonomy are hard to argue with, and are cumulative in a way that our more destructive impulses often aren’t. [...] These simple intuitions, if taken to their logical conclusion, lead eventually to rule of law, democracy, and Enlightenment values. If not inevitably, then at least as a statistical tendency, this is where humanity was already headed. AI simply offers an opportunity to get us there more quickly—to make the logic starker and the destination clearer.

That’s a strange way to present the belief that you’re building a technology that you think has a 10 to 25 percent chance of being catastrophic for civilization, *even given *the huge potential benefits in the case of success. Even if the danger levels are as low as Amodei believes, we should be scrambling to find a third alternative aside from “never proceed” and “charge ahead.” And if the company thinks it’s *forced *to charge ahead (because other people are already charging), it should be begging world leaders to put an end to the suicide race, so that a third alternative can be found. Painting a rosy picture just seems like throwing up a distraction, when it’s in the context of gambling with everyone’s life.[Isn’t it important to race ahead because of “hardware overhang”?→](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/wont-the-situation-get-better-once-governments-get-more-involved

Won’t the situation get better once governments get more involved? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t the situation get better once governments get more involved?
#### It depends on how (and how soon) they get involved.

When we visit Washington, DC, we often meet policymakers who think that the AI companies have their AIs under control. At the same time, we regularly see folks in the AI industry who say that regulation will fix the problem. A particularly egregious example we observed was the CEO of Google [saying](https://youtu.be/9V6tWC4CdFQ?feature=shared&t=2685) that the “underlying risk [of humanity being wiped out] is actually pretty high,” but arguing that the higher the risk gets, the more likely that humanity will rally to prevent catastrophe.

Setting aside how insane it is for the CEO of a company to be racing ahead to build a technology that he thinks endangers everyone on Earth in the hope that humanity will “rally” to address the risks he himself is helping create, observe that this is a case where a person on the technical side of the issue imagines that *somebody else *will solve the issue.

Meanwhile, most folks in politics seem to think that the technical community will solve the problem. This is implicit, for example, every time [they](https://armedservices.house.gov/news/documentsingle.aspx?DocumentID=1731)[say](https://thehill.com/policy/technology/4276801-schumer-us-has-narrowing-lead-over-china-on-ai/)[we](https://energycommerce.house.gov/posts/chair-rodgers-opening-remarks-at-full-committee-hearing-on-ai)[have](https://www.commerce.senate.gov/2024/7/commerce-committee-passes-bipartisan-bill-to-ensure-u-s-leads-global-ai-innovation)[to](https://statemag.state.gov/2025/04/0425itn07/)[win](https://www.commerce.senate.gov/2025/4/winning-the-ai-race-strengthening-u-s-capabilities-in-computing-and-innovation)[the](https://intelligence.house.gov/news/documentsingle.aspx?DocumentID=2581)[race](https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/) — it’s not possible for this sort of race to have a winner, if the technical challenges aren’t solved. Although it might not be quite as bad as all that; perhaps the policymakers aren’t actually thinking of a race to superintelligence; perhaps they’re just thinking of a race to better chatbots. As of June 2025, an AI policy advisor we know describes Congress as generally [not seeming to believe the AI companies](https://x.com/David_Kasten/status/1932573774546948512?t=zVuCnaB6jTNeBForsYScQw) when they explicitly say they are working on superintelligence (albeit with some important exceptions).

Just about everyone in power seems to imagine that somebody else is going to solve the issue.

For more discussion of how the world at large is reacting (and how decisionmakers often fail to react appropriately before disasters), see Chapter 12. As of August 2025, governments have yet to muster anything approaching a serious response to this issue. And there’s always a risk that government officials will fail to understand the challenge entirely, and (e.g.) treat AI as a normal technology that should not be strangled by overregulation.

For more on what government interventions have a real hope of averting an AI catastrophe, read on to Chapter 12. The online resources for Chapter 12 also include a [discussion](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down) on why an international collaboration probably wouldn’t cut it.[Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?→](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems

Won’t there be early warnings researchers can use to identify problems? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t there be early warnings researchers can use to identify problems?
#### Warning signs don’t help if you don’t know what to do with them.

In the Chapter 2 resources, we looked at some problems with relying on warning signs in the [English chain-of-thought scratchpads](/2/but-some-ais-partly-think-in-english-doesnt-that-help) found in some reasoning models.

One problem we discuss is that AI companies haven’t meaningfully reacted to the warning signs they’ve already received.

That’s probably because there’s a big difference between having warning signs and having something you can *do *about them.

In 2009, businessman and deep sea explorer Stockton Rush [co-founded OceanGate](https://www.smithsonianmag.com/innovation/worlds-first-deep-diving-submarine-plans-tourists-see-titanic-180972179/), an undersea tourism company. OceanGate built a five-person [submersible](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html), *Titan*, which brought well-paying customers to view the wreck of the *Titanic* at a bone-crunching depth of two and a half miles below the surface.

One of the safety measures that OceanGate made use of was an [array of acoustic sensors and strain gauges](https://web.archive.org/web/20230619233914/https://oceangate.com/our-subs/titan-submersible.html) for measuring hull integrity. They billed this as a counterargument to people who said their carbon-fiber hull would fail. They acknowledged that it might fail *eventually, *but they’d be fine, because they were measuring it. They were monitoring it. They would be able to see the warning signs.

In January 2018, OceanGate’s director of marine operations, David Lochridge, [told senior management](https://techcrunch.com/2023/06/20/a-whistleblower-raised-safety-concerns-about-oceangates-submersible-in-2018-then-he-was-fired/) that the submersible design was unsafe, that repeated pressure cycling could damage the hull, and that monitoring alone wasn’t enough when a catastrophic failure could happen in milliseconds. Lochridge refused to authorize manned tests until the hull had been scanned for flaws.

OceanGate fired him.

Two months later, [industry experts and oceanographers](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html) wrote OceanGate an extremely concerned [letter](https://int.nyt.com/data/documenttools/marine-technology-society-committee-2018-letter-to-ocean-gate/eddb63615a7b3764/full.pdf) warning the company that its reckless experimentation could precipitate disaster.

(An obvious parallel can be drawn to the current state of AI research, in which early warnings are [ignored](/12/the-lemoine-effect), concerned employees are [fired under dubious circumstances](https://www.transformernews.ai/p/openai-employee-says-he-was-fired) or [resign in frustration](https://www.vox.com/future-perfect/2024/5/17/24158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence), and whistleblowers within the industry write open letters to [sound the alarm](https://righttowarn.ai/).)

On July 15, 2022, after passengers reported hearing a loud bang while ascending, the measurements revealed a [permanent change in hull strain levels](https://abcnews.go.com/US/ntsb-engineer-titan-submersible-hull-anomalies/story?id=114076436). In retrospect, it was probably an indication that the carbon fiber hull was [on the verge of collapse](https://youtu.be/Bq8TCFGaOlc?si=blH-_bYwGIOmJAEL&t=125).

Nobody at OceanGate recognized this as an emergency. They took the sub on a few more deep dives, which went fine. Then, on June 18, 2023, they took the sub on another dive. It imploded, killing Stockton Rush and everyone else aboard.

Warning signs don’t matter much if you don’t know how to read them.

Warning signs don’t matter much if you don’t know what to do with them.

Even warning signs that look worrying to *someone* are always easy for an optimist to dismiss with one excuse or another.

If OceanGate had had a mature theory of carbon-fiber hulls that told them exactly what measurements and readings were dangerous, they might have been able to heed the warning signs. But they were working with a technology that nobody quite understood in that way, so the carefully measured changes in strain levels did nothing.

In the case of superintelligence, we don’t have enough theory to make good use of warning signs. How are an AI’s thoughts going to change as it gets smarter? What internal forces are driving its behavior, and how will those balances shift as it develops the capability to make new and more extreme options for itself? How does it evaluate itself upon reflection, and how would it change itself once it gained the capability to change itself?

If any of those questions have worrying answers, what are the warning signs? For example, current AI systems can sometimes be induced to [try to kill their operators](https://www.anthropic.com/research/agentic-misalignment#more-extreme-misaligned-behavior) in controlled lab experiments.[*](#ftnt266)

If we had a mature theory of intelligence, we would probably be able to look at modern AIs and see all sorts of other warning signs that their drives and preferences are going to shift in ways we don’t like, once they get smarter. If humanity could learn from this problem using trial and error — if we got to reset the world after destroying it and try again a few dozen times — then we might learn how to read the signs. There are probably all sorts of subtle tells that would look clearer in retrospect, like the hull strain that the monitoring system on the *Titan* submersible picked up.

But we’re not there yet. AI corporate executives are like Stockton Rush — experts on the sidelines are shouting “That new technology will kill people!” and the corporate executives are responding “Don’t worry, I’m measuring it!” while having no idea a) what the measurements *mean, *or b) what to do if those measurements are worrying. Except that this time, the whole human species is loaded into the metaphorical submarine.
#### AI is not the kind of mature engineering field that’s equipped for this kind of problem.

Stockton Rush was working in the sort of field where, after his submarine imploded, experts could look over the wreckage and analyze the exact cause of failure.[†](#ftnt267) The engineering field was mature to the point where experts could (and [did](https://www.nytimes.com/2023/06/20/us/oceangate-titanic-missing-submersible.html)) guess the technical issues in advance, and could sort them out conclusively after the fact.

It wouldn’t be the same with AI. If humanity killed itself with superintelligence tomorrow, and then miraculously went back in time to a week before the disaster began, experts *still* would not know what the AI had been thinking. Maybe they could study the failure and learn**a little more about how AI actually works. Maybe that would be one step down the path of maturity in the discipline of AI engineering, toward the sort of field that could have safety manuals and a thorough account of the pressures that affect a particular kind of artificial mind as it gets smarter.

But the field isn’t there yet, today. It isn’t close.

Human engineering usually matures through trial and error. Modern military submarines rarely implode, but early submarines (including military ones) often [crashed, flooded, or exploded](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3481.pdf), and that’s part of how the field matured.

Humanity doesn’t have the luxury of maturing the field of AI alignment in this fashion.

This brings us to one of the central points we tried to drive home in Chapter 11: the difference between a field in its infancy and a field in its maturity.

Alchemy was a field in its infancy compared to the mature field of chemistry today.

When you hear that the “safety researchers” at AI companies have put forth half a dozen plans for survival, you might think that surely at least one of them has a chance of working.

But when a large number of alchemists in the year 1100 put forward half a dozen plans for turning lead into gold, none of them were going to work. If the kinds of doctors who talked about the [four humors](https://en.wikipedia.org/wiki/Humorism) came up with a bunch of medicinal plans to save you from rabies, none of them would work.

Experts in the *mature *field of chemistry can figure out how to transmute tiny bits of lead into gold, using knowledge from atomic physics. Experts in the *mature *field of medicine can easily treat rabies if they get involved shortly after a patient gets bitten. But someone in the immature field doesn’t have a chance.

AI alignment is still in the immature phase.

An immature field has lots of people who say, “Well, I’m just working on measuring it,” because measuring outputs is far easier than developing the theory of what constitutes a warning sign, and what to do if you see one. A mature field would have experts discussing the dynamics that govern an AI’s internals**and how those may change as the AI’s intelligence increases or as its environment changes. They’d have theories about exactly what will change as the AI gets a little smarter, and they’d be comparing different theories to specific observed data. They’d know what parts of the AI’s cognition need to be monitored, and they would understand precisely what all the signals meant.

An immature field has lots of people saying: “We’ll just have the AIs figure it out somehow and do the alignment work.”

Perhaps you can’t wade into every debate about an individual plan and tell whether or not it has a chance of working. But we hope you can step back and see how *vague *all these “plans” are, and how they’re stuck in “don’t worry, we’ll measure it” land and “[hopefully it’ll be easy](https://www.anthropic.com/news/core-views-on-ai-safety)” land and “we’ll just have the AIs do the hard parts” land. We hope that if you step back, it’s clear that this field is not in the phase of formal precise technical descriptions of what does and doesn’t work and why. It is still in the alchemy phase.

And that does not bode well for humanity, in a situation where we do not have the luxury of learning by trial and error.

[*](#ftnt266_ref) It’s not clear how much these warning signs are coming from the AI roleplaying the way it thinks an AI is supposed to behave versus how much it’s thinking strategically. The fact that we can’t tell which warning signs are real isn’t encouraging; it means engineers are much more likely to charge ahead saying “eh, that one probably wasn’t real.” They might even be right most**of the time, but most of the time isn’t good enough when one failure is lethal.

It’s also not clear how long this sort of warning sign will keep happening. Modern AIs are still dumb enough to occasionally mistake tests for reality, but this regime won’t last forever and is already [starting to end](https://arxiv.org/html/2505.23836). An AI that knows it’s being tested might stop exhibiting the worrying behavior in places overseers can see it, even if the underlying tendency remains.

[†](#ftnt267_ref) Delamination due to pressure cycling. In layman’s terms: The stresses from many dives pulled the layers of the hull apart, weakening it until it imploded.[More on Some of the Plans We Critiqued in the Book→](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /11/wont-we-just-muddle-through-like-always

Won’t we just muddle through, like always? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/11)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t we just muddle through, like always?
#### The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.

See Chapter 10 and the [associated extended discussion](/10/a-closer-look-at-before-and-after) about the difference between Before and After.[Do you see alignment as all-or-nothing?→](/11/do-you-see-alignment-as-all-or-nothing)[Resources](/resources) › [Chapter 11](/11)[
### Won’t we just muddle through, like always?The world usually muddles through by trial and error. In this case, early errors wouldn’t leave survivors.1 min read](/11/wont-we-just-muddle-through-like-always)[
### Do you see alignment as all-or-nothing?No. But “partial alignment” is still likely to be catastrophic.2 min read](/11/do-you-see-alignment-as-all-or-nothing)[
### Won’t the situation get better once governments get more involved?It depends on how (and how soon) they get involved.2 min read](/11/wont-the-situation-get-better-once-governments-get-more-involved)[
### Won’t the most reckless companies naturally be the most incompetent, and thus not a threat?The more cautious companies today are still reckless.4 min read](/11/wont-the-most-reckless-companies-naturally-be-the-most-incompetent-and-thus-not-a-threat)[
### Isn’t it important to race ahead because of “hardware overhang”?That would be suicidal, because we’re too far from an alignment solution.3 min read](/11/isnt-it-important-to-race-ahead-because-of-hardware-overhang)[
### Isn’t it important to race ahead so we can do alignment research?We strongly recommend against this entire AI paradigm.3 min read](/11/isnt-it-important-to-race-ahead-so-we-can-do-alignment-research)[
### What if AI companies only deploy their AIs for non-dangerous actions?Actions that seem benign can still require dangerous capabilities.9 min read](/11/what-if-ai-companies-only-deploy-their-ais-for-non-dangerous-actions)[
### Why not just read the AI’s thoughts?Their thoughts are hard to read.6 min read](/11/why-not-just-read-the-ais-thoughts)[
### What if we made AIs debate, compete with, or oversee each other?If the AIs get smart enough to matter, they likely collude.3 min read](/11/what-if-we-made-ais-debate-compete-with-or-oversee-each-other)[
### What about various other AI alignment plans?We cover additional alignment proposals in the book.1 min read](/11/what-about-various-other-ai-alignment-plans)[
### Won’t there be early warnings researchers can use to identify problems?Warning signs don’t help if you don’t know what to do with them.8 min read](/11/wont-there-be-early-warnings-researchers-can-use-to-identify-problems)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Some of the Plans We Critiqued in the Book](/11/more-on-some-of-the-plans-we-critiqued-in-the-book)[
### We Know What It Looks Like When a Problem Is Being Treated with Respect, And This Isn’t It](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it)[
### Shutdown Buttons and Corrigibility](/11/shutdown-buttons-and-corrigibility)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
