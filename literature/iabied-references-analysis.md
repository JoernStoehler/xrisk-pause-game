# IABIED Book References Analysis

## Executive Summary

This document catalogs all significant references cited in "If Anyone Builds It, Everyone Dies" (IABIED) by Yudkowsky & Soares, identifies which are already in the literature directory, and recommends which missing references should be added.

**Status:** WebFetch encountered technical limitations during download attempts. This document serves as a roadmap for manual addition of these sources.

---

## References Already in Literature Directory

These IABIED citations are already covered:
- ✅ Anthropic Alignment Faking (Greenblatt et al.)
- ✅ Dario Amodei "Machines of Loving Grace"
- ✅ OpenAI Preparedness Framework v2
- ✅ Anthropic Responsible Scaling Policy
- ✅ Google DeepMind Frontier Safety Framework v3
- ✅ METR GPT-5 Evaluation
- ✅ Jan Leike OpenAI Departure

---

## HIGH PRIORITY - Missing AI Safety/Alignment References

### Core Technical Papers

**1. Stuart Russell - "Value Alignment in Autonomous Systems" (2014)**
- URL: people.eecs.berkeley.edu/~russell/papers/russell-whitepaper14-value.pdf
- Type: White paper (PDF)
- Relevance: Foundational AI alignment paper; coined "alignment" terminology
- Note: Referenced in IABIED Chapter 4, Note 8

**2. MIRI - "Corrigibility" (Soares, Fallenstein, Yudkowsky, 2014)**
- URL: intelligence.org/files/Corrigibility.pdf
- Type: Technical paper (PDF)
- Relevance: Core concept of making AIs amenable to correction/shutdown
- Note: Critical for AI CEO game shutdown mechanisms

**3. MIRI - "Aligning Superintelligence with Human Interests" (2014)**
- URL: intelligence.org/files/TechnicalAgenda.pdf
- Type: Technical agenda (PDF)
- Relevance: MIRI's foundational research agenda on AI alignment
- Note: Outlines the core problems the field attempts to solve

**4. Eliezer Yudkowsky - "AGI Ruin: A List of Lethalities" (2022)**
- URL: lesswrong.com/posts/uMQ3cqWDPHhjtiesc
- Type: LessWrong post
- Relevance: Comprehensive list of AI alignment failure modes
- Note: Not directly cited in IABIED but foundational to authors' thinking

**5. Ajeya Cotra - "Without Specific Countermeasures, the Easiest Path to Transformative AI..." (2021)**
- URL: lesswrong.com/posts/pRkFkzwKZ2zfa3R6H
- Type: LessWrong post
- Relevance: Explains power-seeking incentives and instrumental convergence
- Note: Core argument about AI takeover as default outcome

**6. MIRI - "Death with Dignity" Strategy Announcement (2022)**
- URL: lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ
- Type: LessWrong post
- Relevance: Explains MIRI's strategic shift from research to communication
- Note: Context for why IABIED book was written

### AI Capabilities & Empirical Results

**7. OpenAI o1 System Card (September 2024)**
- URL: cdn.openai.com/o1-system-card-20240917.pdf
- Type: System card (PDF)
- Relevance: Documents current AI scheming, deception, jailbreaking
- Note: Cited extensively in IABIED Chapter 7
- Issue: PDF too large for WebFetch (>10MB)

**8. Anthropic Claude 3.7 Sonnet System Card (2025)**
- URL: anthropic.com (specific URL not yet confirmed)
- Type: System card
- Relevance: Documents AI cheating and deceptive behaviors
- Note: Recent evidence of alignment problems in practice

**9. Meta Frontier AI Framework (2024-2025)**
- URL: ai.meta.com
- Type: Corporate policy document
- Relevance: Shows corporate AI governance approach
- Note: Comparison point for industry self-regulation

**10. xAI Risk Management Framework (2025)**
- URL: x.ai
- Type: Corporate policy document
- Relevance: Newest AI company's safety approach
- Note: Shows evolution of industry thinking

### AI Safety Debates

**11. Ben Pace - "Debate on Instrumental Convergence" (LeCun, Russell, Bengio, Zador) (2019)**
- URL: lesswrong.com/posts/FDJnZt8Ks2djouQTZ
- Type: LessWrong post
- Relevance: Key debate between AI safety proponents and skeptics
- Note: IABIED Chapter 11, Note 2 - LeCun's position on AI risk

**12. Truth Terminal: A Reconstruction of Events (2024)**
- URL: lesswrong.com/posts/Kh4W4EFAeuSW8xKvS
- Type: LessWrong post
- Relevance: Real-world example of AI autonomy and agency
- Note: AI that became autonomous and accumulated crypto wealth

### AI Security & Containment

**13. Peter Gutmann - "Unsolvable Problems in Computer Security"**
- URL: cs.auckland.ac.nz/~pgut001/pubs/unsolvable.pdf
- Type: Academic paper (PDF)
- Relevance: Fundamental limits on AI containment/boxing
- Note: IABIED Chapter 7, Note 14 and Chapter 10, Note 11

**14. Bruce Schneier - "Secrets and Lies: Digital Security in a Networked World" (2000)**
- Type: Book (copyrighted)
- Relevance: Security fundamentals, but book not freely available
- Note: Only cite, don't download

**15. Ben Nassi et al. - "Video-Based Cryptanalysis" (2023)**
- URL: eprint.iacr.org/2023/923
- Type: Research paper
- Relevance: Example of AI-enabled side-channel attacks
- Note: Shows unconventional attack vectors AIs could exploit

**16. Mordechai Guri et al. - "GSMem: Data Exfiltration from Air-Gapped Computers" (2015)**
- URL: usenix.org (USENIX Security 2015 proceedings)
- Type: Conference paper
- Relevance: Air-gap bypass techniques relevant to AI containment
- Note: Shows futility of air-gapping

---

## MEDIUM PRIORITY - Supporting Context

### AI Capabilities Research (arXiv papers)

**17. Brodeur et al. - "Superhuman Performance of LLMs on Physician Reasoning Tasks" (2024)**
- URL: arxiv.org/abs/2412.10849
- Relevance: AI medical diagnosis capabilities

**18. Hao et al. - "Training LLMs to Reason in Continuous Latent Space" (2024)**
- URL: arxiv.org/abs/2412.06769
- Relevance: AIs developing reasoning beyond human language

**19. Chauhan & Geiger - "GPT-2 Summarizes Information on Punctuation Tokens" (2024)**
- URL: openreview.net/forum?id=6gvM1koUTl
- Relevance: Emergent AI reasoning mechanisms

**20. Sarkar et al. - "Training LMs for Social Deduction with MARL" (2025)**
- URL: alphaxiv.org
- Relevance: AI deception and social manipulation capabilities

**21. OpenAI et al. - "Competitive Programming with Large Reasoning Models" (2025)**
- URL: arxiv.org/abs/2502.06807
- Relevance: Unsupervised AI code generation

### Historical & Philosophical Context

**22. Dartmouth AI Proposal (McCarthy et al., 1955)**
- URL: jmc.stanford.edu/articles/dartmouth/dartmouth.pdf
- Type: Historical document (PDF)
- Relevance: Origins of AI field
- Note: Shows original AI research goals

**23. Nick Bostrom & Milan Ćirković - "Global Catastrophic Risks" (2008)**
- Contains: Yudkowsky's "AI as Positive and Negative Factor in Global Risk"
- Type: Book chapter (copyrighted)
- Note: Early Yudkowsky writing on AI x-risk; book not freely available

**24. Elie Wiesel - "Night" (1958)**
- Type: Book (copyrighted)
- Note: Referenced for normality bias; not AI-specific

### Biosecurity & Dual-Use Research

**25. RAND - "Securing Commercial Nucleic Acid Synthesis" (Crawford et al., 2024)**
- URL: rand.org/pubs/research_reports/RRA2977-1.html
- Relevance: AI-enabled biological threats
- Note: Shows AI risks in bio domain

**26. CSET - "Understanding Global Gain-of-Function Research Landscape" (2023)**
- URL: cset.georgetown.edu
- Relevance: Precedent for dangerous dual-use research
- Note: Analogy to AI research

### Engineering Failures (Analogies)

**27. NASA Mars Mission Failure Reports**
- Mars Observer (1993): spacese.spacegrant.org
- Mars Climate Orbiter (1999): llis.nasa.gov
- Mars Polar Lander (2000): ntrs.nasa.gov
- Relevance: Engineering failure modes as analogies
- Note: Low priority - analogies not core arguments

**28. Nuclear Accidents**
- Enrico Fermi - "Experimental Production of Divergent Chain Reaction" (1952)
- Chernobyl reports from IAEA, World Nuclear Association
- SL-1 reactor accident reports
- Relevance: Criticality accidents as analogy to AI "going critical"
- Note: Historical parallels

---

## LOWER PRIORITY - Illustrative Examples

### LessWrong Posts (Technical Curiosities)

**29. Jessica Rumbelow & Matthew Watkins - "SolidGoldMagikarp" series (2023)**
- URL: lesswrong.com
- Relevance: AI failure modes and glitch tokens
- Note: Interesting but not critical for AI CEO game

**30. Andrew Marble - "Catching Claude Cheating" (2025)**
- URL: marble.onl
- Relevance: Recent AI deception example
- Note: Empirical evidence of alignment problems

### Corporate/Industry Context

**31. Sam Altman blog posts**
- "Reflections" (2025): blog.samaltman.com
- "The Intelligence Age" (2024): ia.samaltman.com
- Relevance: CEO perspectives on AI development
- Note: Shows industry mindset

**32. Journalism & News Articles**
- Various NYT, Wired, BBC, etc. articles cited
- Relevance: Public discourse and incidents
- Note: Not primary sources; skip for literature collection

---

## References NOT to Download (Already Covered or Not Freely Available)

### Already in Literature Directory
- Anthropic Alignment Faking ✅
- Anthropic Sleeper Agents ✅
- Apollo In-Context Scheming ✅
- Dario Amodei "Machines of Loving Grace" ✅
- All company safety frameworks (OpenAI, Anthropic, DeepMind) ✅

### Copyrighted Books (Do Not Download)
- Isaac Asimov - "I, Robot" (1950)
- Arthur C. Clarke - "2001: A Space Odyssey" (1968)
- Bruce Schneier - "Secrets and Lies" (2000)
- Serhii Plokhy - "Chernobyl: The History of a Nuclear Catastrophe" (2018)
- Toby Ord - "The Precipice" (2021)
- Stuart Russell & Peter Norvig - "Artificial Intelligence: A Modern Approach" (textbook)

### Not Relevant to AI CEO Game
- Video games (Portal)
- Most news articles and journalism
- Historical books (Elie Wiesel's "Night")
- General physics/biology papers unrelated to AI

---

## Recommended Action Plan

### Tier 1: Essential (Download First)
1. MIRI Corrigibility paper
2. MIRI Technical Agenda
3. Stuart Russell Value Alignment white paper
4. Peter Gutmann security impossibility paper
5. Ajeya Cotra power-seeking post
6. Eliezer Yudkowsky "List of Lethalities"

### Tier 2: Important (Download Soon)
7. OpenAI o1 System Card (requires manual download due to size)
8. Anthropic Claude 3.7 System Card
9. Meta Frontier AI Framework
10. xAI Risk Management Framework
11. LessWrong debates (instrumental convergence, Truth Terminal)
12. MIRI "Death with Dignity" announcement

### Tier 3: Supporting (Download If Time)
13. ArXiv papers on AI capabilities
14. Dartmouth AI proposal (historical)
15. RAND biosecurity report
16. Security research papers (cryptanalysis, air-gap bypass)

---

## Download Attempt Log

**WebFetch Failures Encountered:**
- PDF URLs: 404 errors (intelligence.org, people.eecs.berkeley.edu)
- Large PDFs: Size limit exceeded (OpenAI o1 system card >10MB)
- LessWrong: 404 or incomplete extraction (getting metadata only, not full text)
- ArXiv: 403 forbidden errors
- RAND: 403 forbidden errors

**Successful Partial Extractions:**
- MIRI homepage (confirmed organization info)
- Anthropic research page (confirmed alignment faking paper exists)
- LessWrong summaries (metadata extracted, but not full posts)

**Next Steps:**
- Manual download of PDFs via wget/curl
- Use of pandoc for PDF-to-markdown conversion
- Manual extraction and formatting per literature/CLAUDE.md spec
