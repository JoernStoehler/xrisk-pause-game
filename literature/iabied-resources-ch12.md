---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 12 - "I Don't Want to Be Alarmist""
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 12 - "I Don't Want to Be Alarmist" from the companion website"
---

# Online Resources: Chapter 12 - "I Don't Want to Be Alarmist"

## /12/are-you-anti-technology

Are you anti-technology? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you anti-technology?
#### No. Superintelligent AI is a very unusual case.

We publicly champion technologies such as [nuclear energy](https://x.com/ESYudkowsky/status/1908309414932832301), [cryonics](https://x.com/ESYudkowsky/status/1828822384054575537), [human intelligence augmentation](https://x.com/ESYudkowsky/status/1737305573018702258), and [human challenge trials for medical testing](https://x.com/ESYudkowsky/status/1321152172797554688).

More than that, we’re willing to say that when a mad invention risks *only the lives of voluntary customers* who understand all the relevant dangers, it’s the business of those voluntary customers to make their own decisions.

We’d even applaud certain cases where technology *does *hurt bystanders, as was the case when London burned a lot of coal — and caused a lot of lung cancer in the process — in order to industrialize society and raise the standard of living across the board.

We think the world *was *better off once industrialization was complete. We generally credit science, progress, and the human spirit and its ability to overcome most obstacles.

Some of these are unpopular positions among the people we expect to read this. We describe these positions not to win your favor, but to make clear our honest beliefs, and to underscore that AI is different.

Why is AI different? Why do we fail to trust the human spirit and the power of scientific inquiry, in this one particular case?

The answer is: scope. Gambling your own life is different from gambling the lives of your customers, which is different from gambling the lives of innocent bystanders, which is different from gambling the entire human species.

Doubly so when your field is woefully immature and the odds of *winning* your gamble are awful.[Isn’t it smarter to rush ahead and make sure good guys have the lead?→](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/are-you-saying-we-need-provably-safe-ai

Are you saying we need provably safe AI? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you saying we need provably safe AI?
#### No.

We aren’t advocating that humanity wait for a literal proof that some artificial superintelligence will be good, or anything like that. Such a proof is probably not possible even in principle, never mind in practice. As Einstein said in his 1921 lecture, *Geometry and Experience*: “As far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.”

Any supposed proof about how an AI will behave in the real world is not guaranteed to govern the AI’s actual behavior, because we might be wrong about how the real world works.

That’s already true of computers today. For example, you might think that if someone has a literal mathematical proof that, according to the theoretical behavior of transistors and the circuit diagram of a computer, it’s impossible for a computer program to change the memory in cell #2, then the computer program cannot chance the memory in cell #2. But the “[rowhammer attack](https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf)” involves rapidly changing the memory cells #1 and #3 on *either side *of the protected memory cell, in a way that turns out to electromagnetically perturb cell #2 in the middle, changing a piece of computer memory without ever writing to it directly. Real physical transistors are not mathematically perfect transistors, and proofs that look comforting in theory don’t always matter much in practice.

We aren’t demanding mathematical proof that things will go well. It’s not possible to meet such a standard in real life, and even if it were, it probably wouldn’t be worth the cost. We approve of society taking justified risks. The argument we’re making is not that there’s some tiny amount of risk that’s hard to dispel, it’s that there’s an extreme danger bearing down on us.

Growing an artificial superintelligence animated by drives that relate only tangentially to its operator’s intentions is the sort of thing that goes wrong *by default. *It’s not that there’s some small chance of things going wrong, but we should pay attention to this risk out of an abundance of caution. The book isn’t titled *If Anyone Builds It, There’s A Tiny Chance We All Die, But Even A Tiny Chance Is Worth Mitigating.* If we rush ahead at this level of knowledge and ability, we will predictably all die, because we’re just *that far off* from being able to create vastly superhuman AIs that are friendly.

If AI were analogous to automobiles, we wouldn’t be saying, “This car has faulty seatbelts and airbags. Let’s pull over out of an abundance of caution.”

We’d be saying, “This car is *careening toward a cliff*. *Stop.*”

It’s not about “safety proofs.” It’s not a “tail risk.” Scientists are not ready to face this challenge. We’d just die.[What does it do to your daily life to believe all of this?→](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/are-you-saying-we-should-panic

Are you saying we should panic? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you saying we should panic?
#### We’re saying government officials should take the problem seriously.

We don’t see how panicking would help the situation. Panicking isn’t how society survived the threat of fascism during World War II, nor the threat of nuclear annihilation during the Cold War.

Preventing machine superintelligence from coming into existence is everyone’s problem. In Chapter 13, we discuss the next steps we think the world should take to avert the danger. Suffice to say, this problem will require coordination, cooperation, level heads, and mature communication.
#### Acts of extreme panic don’t yield good results.

Sometimes people ask how we could possibly be earnest in what we say if we haven’t, for example, started attacking AI researchers. The answer is that violent outbursts would make things worse. (If you’re the kind of naive utilitarian who thinks**they would help, you should probably just back off from attempting consequentialist reasoning entirely and stick to following deontological rules, as we’ve argued before.)

We aren’t hardline pacifists who think that a nation should never go to war, no matter the cause, on the grounds that it risks lives. Some things are worth risking lives for. But there’s a world of difference between “I’m not a hardline pacifist,” and “I think violent mayhem is a sensible way to ensure that the world handles this complicated issue of technological proliferation well.”

Usually these terrible suggestions are brought up by someone who doesn’t actually believe**that AI is on the brink of killing us, and who hasn’t tried really looking at the world through that lens. It doesn’t seem to occur to them to ask if acts of lawless violence would *actually help*. (Despite our efforts to spell it out repeatedly, as in the addenda one of [Yudkowsky’s essays](https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1#Addendum__March_30__).)

We aren’t telepaths, but it seems to us that this kind of AI disaster skeptic possibly views violence as a form of personal expression, as though expressing extreme feelings in extreme ways will cause the world to hand you what you want.

The world doesn’t work like that. We don’t live in a world where everyone has the option to sell their soul for success in their endeavors, and where the reason that most people don’t is because they haven’t found an endeavor that’s worth their soul. Terrorism is not a magic “I win!” button that people refrain from only out of a conviction that it wouldn’t be right. The Unabomber did not succeed in reversing the industrialization of society.

You can still shred your soul with acts of hatred or violence, but all you’ll get in exchange is a more broken world. A world where the discourse is that much more poisoned, and where the feats of international coordination needed to actually *solve* this problem are now that much more difficult to achieve. A terrible act of desperation will not grant you terrible power as part of some Faustian bargain. You can try your very best to sell your soul, but the devil isn’t buying.
#### Notes

[1] *argued before: *See, e.g., Yudkowsky’s answer to Q3 in his LessWrong post about how to [inch humanity closer to survival even when the situation looks grim](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#Q5___Then_isn_t_it_unwise_to_speak_plainly_of_these_matters__when_fools_may_be_driven_to_desperation_by_them___What_if_people_believe_you_about_the_hopeless_situation__but_refuse_to_accept_that_conducting_themselves_with_dignity_is_the_appropriate_response_).[Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?→](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/but-experts-dont-all-agree-about-the-risks

But experts don’t all agree about the risks! | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## But experts don’t all agree about the risks!
#### Lack of expert consensus is a sign of an immature technical field.

We’ve noted that many senior AI scientists think that this technology has a serious chance of killing all humans. For example, Nobel laureate Geoffrey Hinton, who played a large role in pioneering the modern approach to AI, has said that his independent personal assessment puts the odds of AI killing us all at [greater than 50 percent](https://youtu.be/PTF5Up1hMhw?feature=shared&t=2376). More than 300 AI scientists signed the 2023 [Statement of AI Risk](https://aistatement.com/) that we opened the book with.[*](#ftnt281)

Other scientists, however, have the opposite view — some well-known examples being Yann LeCun and Andrew Ng.

What’s to be made of this lack of scientific consensus?

Well, mostly, we recommend that you check out the different arguments made by the two sides (including our own arguments in the book) and assess them for yourself. We think the quality of argumentation mostly speaks for itself, and any attempts to explain *why *there’s persistent disagreement should be treated as an afterthought.

We note in passing, however, that this state of affairs isn’t any great mystery, in the wake of what we discussed in Chapters 11 and 12. The mere existence of widespread expert disagreement doesn’t establish the book’s thesis, of course. But it’s more congruent with the picture we’ve painted — that the field is in an early, alchemy-like state — than the opposing picture that AI is a mature field with strong technical foundations.

It’s definitely a bit strange for the field of AI to be so divided, even as it spins up powerful technologies. Other technological dangers had more consensus about them. Roughly 100 of 100 scientists in the Manhattan Project would have said that global thermonuclear war presented a substantial risk of global catastrophe. In contrast, among the three scientists who received a Turing Award[†](#ftnt282) for the research that more or less kicked off the modern AI revolution, two of them (Hinton and Bengio) are outspoken about the dangers of superintelligence, and one (LeCun) is outspokenly dismissive.

This level of disagreement about the operation of a machine isn’t normal between experts in a mature technical field. It’s a sign of technical immaturity.

In most technological fields, that immaturity is actually a sign of safety. Back when physicists were still arguing about the basic properties of matter, they weren’t anywhere near creating nuclear weapons. You could observe their disagreement and make an informed guess that they weren’t about to create a bomb that could level cities. Indeed, it’s not possible to create a nuclear bomb without scientists who understand the inner workings of the bomb in detail.

It would be a different situation if the physicists were still bickering about the basic operating principles of their field *while creating larger and larger explosions.*

Imagine a world in which physicists could somehow “growing” nuclear bombs, and they didn’t really understand why or how they operated. Now suppose that two-thirds of the most-decorated scientists said, “We did our best to figure out what’s going on. It looks like these devices might create excessive amounts of cancerous radiation that will kill lots of distant civilians, if we continue down this path. Please look at our arguments for why this is so dangerous, and stop racing down this path.” The remaining one-third responds, “That sounds ridiculous! There are always people predicting doom, and you can’t let them get in the way of progress.”

Well, that would indeed have been a different situation entirely.

Discord among scientists in *that *sort of scenario would not be especially comforting. Engineers probably shouldn’t be allowed to keep growing larger and larger explosives in a situation like that.

AI companies are succeeding at growing machines that are smarter and smarter, year after year. They don’t understand the inner workings of the devices they create. Many of the most eminent scientists in the field express grave concerns; others wave the concerns aside without articulating much in the way of counterargument. This is, at the very least, evidence that the field is *immature*. The lack of consensus is, at the very least, not evidence that things are *fine. *The lack of consensus in a situation like this should, at the very least, be worrying.

How do you figure out whether those worries are real? How do you figure out who’s right between the people raising the alarm and the people trying to dismiss it? As always, you’ve just got to evaluate the arguments.

[*](#ftnt281_ref) More examples, including surveys showing that these examples are widely shared in the field, can be found in our discussion of what [AI Experts say about catastrophe scenarios](/intro/ai-experts-on-catastrophe-scenarios).

[†](#ftnt282_ref) Considered the highest honor in the field.[But what about the benefits of smarter-than-human AI?→](/12/but-what-about-the-benefits-of-smarter-than-human-ai)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/but-what-about-the-benefits-of-smarter-than-human-ai

But what about the benefits of smarter-than-human AI? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## But what about the benefits of smarter-than-human AI?
#### Rushing ahead destroys those benefits.

We’re optimistic about how wonderful superintelligence could be, if it were steering the world toward wonderful ends. We’d personally consider it a great tragedy if humanity *never *created smarter-than-human minds.

But superintelligence alignment doesn’t come free. If we rush to try to reap those benefits, we get nothing, and worse than nothing.

I (Yudkowsky) spent several years as an accelerationist myself, hoping to create AI as quickly as possible, before recognizing that AI alignment didn’t come free. And both of us authors dream of wonderful transhumanist futures. But we don’t get there by racing ahead on superintelligence.

The choice isn’t between gambling on the benefits of AI now (no matter how small the chance) versus never accessing those benefits. The true choice is between rushing recklessly ahead and killing everyone, versus taking the time to do the job properly.[*](#ftnt283)

“Now or never” is a false dichotomy.

[*](#ftnt283_ref)[Some](https://x.com/balajis/status/1725890626699628633)[people](https://x.com/MatthewJBar/status/1958403809249464757)[argue](https://x.com/DeryaTR_/status/1958592366652125487) that we must take the gamble now, for the shot at saving dying humans from their natural deaths by aging. Human bodies are formidably complicated, but with enough scientific progress, we could solve many of the maladies that we take for granted today — such as cancer, heart disease, and the varied diseases of aging. Smarter-than-human AI could get us there far faster. Delaying superintelligence literally costs lives.

Or, well, it *would* cost lives, if it weren’t for the fact that superintelligence kills exactly the same people.

In fact, sick and dying people today very likely have a *better chance of survival* if humanity backs off from the brink:  

- Biomedical research and the hunt for treatments and cures can proceed in the absence of superintelligence. Gene therapy, cancer vaccines, and other new approaches hold enormous promise that researchers are only just beginning to tap.
- Narrowly focused AI technology can even help accelerate this effort, without any need to put the whole human endeavor in jeopardy by building toward smarter-than-human general AI.
- [Brain](https://cryonics.org/membership/)[preservation](https://www.alcor.org/membership/) methods can be used to preserve people even after their heart stops pumping, until medical science advances to the point of being able to revive them and restore their health. The sort of AI that could offer immortality could also almost surely restore somebody from an appropriately preserved brain.

(More quietly, a subset of these people [will tell you](https://x.com/SottoNocce/status/1771420351265923137) that they are in it for their own personal immortality, and that they’re willing to risk the lives of every adult and child on the planet even for a small chance that they and their loved ones can achieve it. This strikes us as mustache-twirling villainy. To these villains, our recommendation is the same as it is to the altruists: Sign up for brain preservation. It gives you better odds than a rogue superintelligence would, and you also get to avoid putting every human alive in grave peril in your quest for immortality! Win-win.)

Even if we only cared about the welfare of the sick and dying, rolling the dice on some combination of these methods seems like a better option than rolling the dice on building vastly superhuman AI and hoping that it likes us. (And that it likes us in [just the right ways](/5/wont-ais-care-at-least-a-little-about-humans#caring-about-us-in-the-right-way-is-a-narrow-target).) The dice for superhuman AI are dramatically loaded against us.

But also: To the best of our knowledge, nobody has actually *asked* the sick and dying if they *want* to put their families and countrymen in severe danger in order to roll the dice on a possible superintelligence-derived cure. And the families and countrymen in question certainly haven’t been asked if they consent to having their lives put on the line for this mad science experiment.

We don’t have to gamble all of our lives on this option, when many other options exist.

  
We implore anyone concerned for the welfare of people today to accelerate the above methods instead, while steering as wide a berth as possible around anything that could move us even incrementally closer to artificial superintelligence.

  
If you simply don’t believe that a rogue superintelligence would kill us, that’s one thing. But to accept that it would likely kill us all and saying we have to take the gamble anyway is madness. There are other options for resolving the problems of the modern world. By analogy: If living in a high-altitude environment makes you uncomfortable, that’s no excuse for jumping off a cliff. Find a different pathway to the bottom of the mountain.[The Lemoine Effect→](/12/the-lemoine-effect)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead

Isn’t it smarter to rush ahead and make sure good guys have the lead? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t it smarter to rush ahead and make sure good guys have the lead?
#### No.

Modern AI techniques do not yield AIs that do what their operators intend (as discussed in Chapter 4). Solving this problem is the sort of thing that would typically take humanity quite a lot of trial and error, and we have no room for error here (as discussed in Chapter 10).

Moreover, the current crop of AI engineers is very far from being up to the task, as discussed in Chapter 11. Modern AI engineers sorely lack the scientific understanding it’d take to succeed at AI alignment. AI researchers aren’t like the operators of the Chernobyl nuclear reactor; those operators were working with a device that was theoretically well-understood and had careful safety manuals that they neglected in a fashion that led to catastrophe. There’s no such thing as an AI safety manual built from a comprehensive understanding of the AI’s internals and what arrangements might cause things to go wrong. We’re not even close to the *Chernobyl *level of competence, here. And Chernobyl exploded.

AI researchers are flying blind and winging it, with almost no chance of success.

In that context, it doesn’t matter whether the “good guys” or the “bad guys” build superintelligence. The AI’s preferences aren’t sneezed onto it by whoever’s standing closest.

It doesn’t matter how well-intentioned they are, and how careful they say they’re being. It doesn’t matter who “wins” the race. If humanity races to artificial superintelligence, then we all die.
#### It’s not impossible to stop. It might not even be all that hard.

We’ll turn to this point in the final chapter of the book.

Things change. They especially change when there is a desperate, urgent, recognized need. The main impediment to stopping is world leaders failing to realize the danger. And that process has [already begun](/13/will-elected-officials-recognize-this-as-a-real-threat).[Why not use international cooperation to build AI safely, rather than to shut it all down?→](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues

Isn’t the danger from smarter-than-human AI a distraction from other issues? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t the danger from smarter-than-human AI a distraction from other issues?
#### The world is, unfortunately, big enough for multiple issues.

Nuclear war and bioterrorism are real threats. Unfortunately, machine superintelligence is *also* a real threat. The world is big and troubled enough for all three.[*](#ftnt273)

The threat from superintelligence is unlike many other threats that humanity faces, and it seems uniquely pressing. One distinguishing feature is that a significant fraction of the world’s economy is being spent to make AI more and more capable. In contrast: Although biosecurity is a serious issue, investors aren’t pouring tens of billions of dollars into creating superviruses. Supervirus engineers aren’t pulling salaries of millions or tens of millions (or sometimes even [hundreds of millions](https://www.businessinsider.com/ai-talent-warbargain-meta-openai-ex-google-hr-boss-2025-7)) of dollars per year.

The world is putting effort into making nuclear power, but nuclear power plants are a pretty different technology from nuclear weapons. We don’t live in a world where private companies are scrambling to build larger and larger nuclear weapons with huge amounts of investment and talent. If we did, there’d be a much greater risk of nuclear war.

AI is also a trickier situation because it provides great wealth and power right up until it crosses some critical threshold, at which point it kills everyone. And *nobody knows where that threshold is.*

Imagine nuclear power plants got more and more profitable as the uranium they used was more and more enriched, but at some unknown enrichment threshold they blew up and ignited the atmosphere, killing everyone. Now imagine that half a dozen companies were enriching uranium as fast as they could, each saying they’d [rather be a participant than a spectator](https://www.youtube.com/watch?v=cFIlta1GkiE&t=2126s). That’s a little like what humanity is doing with artificial superintelligence.[†](#ftnt274)

The danger from artificial superintelligence is urgent. Corporations are rushing to build this technology. We don’t know how long it will take them to succeed, but it seems to us that a child born in the U.S.A. today is more likely to die from AI than to graduate from high school. We think that you, the reader, are likely to die of this in your lifetime, perhaps in the next few years. The whole world is at stake.

We aren’t saying that other issues should be ignored. We’re saying that this issue must be dealt with.

[*](#ftnt273_ref) See also our extended discussion (following Chapter 13) on [making an inclusive coalition](/13/keep-the-coalition-large).

[†](#ftnt274_ref) See also a [list of comparisons between AI alignment and nuclear weapons](/10/wont-ai-differ-from-all-the-historical-precedents).[Are you anti-technology?→](/12/are-you-anti-technology)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment

Isn’t this all just fear-mongering by AI leaders to increase status and raise investment? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?
#### No.

Throughout the book, we’ve laid out our case for the claim that rushing ahead on AI is likely to get us all killed. In Chapter 3, we discussed how AI will have its own drives and goals. In Chapters 4 and 5, we discussed why AI is likely to pursue ends that nobody intended, and in Chapter 6, we spelled out how machine superintelligences will have not just a motive but a *means *to kill us all.

These**are the sorts of claims we beg you to evaluate when deciding whether the race to superintelligence should be put to a stop. A person can’t figure out whether AI research is on track to kill us all by arguing back and forth about the schemes of corporate executives.

Are the CEOs trying to drum up hype by talking about “AI risk”?

Or are they trying to pander to concerned researchers and lawmakers, and position themselves as the “good guys”?

These questions *don’t bear on the facts* about how smart machines would behave.

Even if the AI CEOs *are* eager to exploit discussions of danger to hype up their product, that doesn’t mean that the work they’re doing is therefore harmless. To figure out whether it’s dangerous, you have to look into AI itself as a technology, not at the press releases that come out of the labs.

Years before these companies existed, there were researchers and academics with zero corporate incentives — ourselves included — warning against racing to build smarter-than-human AI. We spoke to Sam Altman and Elon Musk before they co-founded OpenAI, and told them that the idea of starting OpenAI seemed foolish and likely to increase danger. We spoke with Dario Amodei before he joined OpenAI and advised against his relentless push to scale AIs up (a project that would lead to LLMs).

And if you look at the messaging today, many people without corporate incentives are expressing their concern. They range from [respected](https://yoshuabengio.org/wp-content/uploads/2023/07/Written-Testimony-and-biography-of-Yoshua-Bengio_U.S.-Senate-Judiciary-Subcommittee-on-Privacy-Technology-and-the-Law_25_07_2023.pdf)[academics](https://www.theguardian.com/technology/2024/dec/27/godfather-of-ai-raises-odds-of-the-technology-wiping-out-humanity-over-next-30-years) to the [late Pope](https://www.vatican.va/content/francesco/en/messages/peace/documents/20231208-messaggio-57giornatamondiale-pace2024.html) to the [chair of the FTC](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html)[*](#ftnt279) to U.S. [Congress](https://www.transformernews.ai/p/congress-ccp-agi-hearing)[members](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611).

There’s something to be said for treating the utterances of tech CEOs with cynicism. There’s no shortage of examples of AI corporate executives being two-faced, saying one thing in private blog posts and a different thing when testifying before Congress. But to leap from “the heads of these labs are liars” to “there’s no possible way AI could pose a severe threat” is very strange, when the labs themselves routinely downplay this issue. The Nobel-laureate godfather of the field, the most cited living scientist, a steady trickle of whistleblowers, and hundreds of visibly nervous researchers are raising the alarm about it. Nothing about the situation looks like a business-as-usual corporate hype cycle. In a circumstance like this, dismissing the idea without even engaging with the arguments seems more like naiveté than cynicism.

Questions like “Can CEOs raise more money by talking about the dangers?” can tell us a little about how much to trust the CEOs, but they can’t tell us much about the dangers themselves. If discussing danger is profitable, that doesn’t affect whether the danger is real. If it’s unprofitable, that *also *doesn’t affect whether it’s real.

If you want to figure out whether the dangers are real, you have to ask questions like “Can anyone create an AI that would behave in a friendly fashion even after surpassing human intelligence?” and otherwise engage with arguments about AI, rather than arguments about the people standing nearby. So in the end, we beg you to engage with the arguments themselves. The consequences of getting this wrong are too high.

[*](#ftnt279_ref) Federal Trade Commission chair Lina Khan [said](https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html) in 2023: “Ah, I have to stay an optimist on this one. So I’m going to hedge on the side of lower risk there […] Maybe, like, 15 percent [that AI will kill us all].”
#### Notes

[1] *a different thing when testifying:* OpenAI CEO Sam Altman [wrote in 2015](https://blog.samaltman.com/machine-intelligence-part-1):

Development of superhuman machine intelligence (SMI) is probably the greatest threat to the continued existence of humanity. There are other threats that I think are more certain to happen (for example, an engineered virus with a long incubation period and a high mortality rate) but are unlikely to destroy every human in the universe in the way that SMI could. 

When [addressing Congress](https://www.judiciary.senate.gov/imo/media/doc/2023-05-16%20-%20Bio%20&%20Testimony%20-%20Altman.pdf) in 2023, however, Altman made no mention of this threat, instead listing Privacy, Children's Safety, Accuracy, Disinformation, Cybersecurity, and Economic Impacts as possible areas of concern.[But experts don’t all agree about the risks!→](/12/but-experts-dont-all-agree-about-the-risks)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/making-sense-of-the-death-race

Making Sense of the Death Race | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/12#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Making Sense of the Death Race

A natural question we expect from many readers is:

You say that if anyone builds ASI, everyone dies. But then why is anyone trying to build it? If you’re right, these people aren’t even following their own incentives, ultimately. If everyone dies, they die too.

A cynical, game-theoretic rejoinder might go like this:

Why, it is rational given their incentives. If they don’t build it, they assume someone else will. And they might as well get rich before they die.

Maybe that answer is enough, for a cynic.

Simple game-theoretic explanations like this often misunderstand or oversimplify real human psychology, but this explanation may also contain a grain of truth. An engineer may think that *probably* everyone will die from ASI, but that their own actions don’t affect that probability much.* Meanwhile*, they get to have insane amounts of money, cool toys, and powwows with big, important people looking at them respectfully. Maybe they’ll become the god-kings of Earth if ASI *doesn’t* kill everyone, but *only if their company wins the race to build ASI…*

From the perspective of an OpenAI researcher who recognizes the danger: If they *don’t* work for OpenAI, probably OpenAI destroys the world anyway. (Even if OpenAI shut down, Google would destroy the world anyway.) But if they *do* work for OpenAI, they get six-to-seven-figure salaries, and if they *don’t* die, perhaps they’ll accrue extra power and fame by being on the winning team. So each individual’s personal game-theoretic incentives push them toward collectively destroying the world.

Our view is that this sort of explanation is somewhat overdoing things, and we mention it mainly because there’s a sort of person who believes (much more than we do) that the world *must* run on explanations like that one. We also feel a need to mention it because some people at AI labs *explicitly**say* that a race to the bottom is inevitable, so they might as well add fuel to the fire and have fun themselves.

After previously [warning](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html) that AI “is far more dangerous than nukes,” Elon Musk decided to start an AI company and enter the race himself, [stating](https://www.youtube.com/watch?v=cFIlta1GkiE&t=2126s) in June of 2025:

Part of what I’ve been fighting — and what has slowed me down a little — is that I don’t want to make Terminator real. Until recent years, I’ve been dragging my feet on AI and humanoid robotics.

Then I sort of came to the realization that it’s happening whether I do it or not. So you can either be a spectator or a participant. I’d rather be a participant.

[And](https://youtu.be/MtYsUdfZPMA?feature=shared&t=655):

And will this be bad or good for humanity? Um, it’s like, I think it’ll be good? Most likely it’ll be good? But I’ve somewhat reconciled myself to the fact that even if it wasn’t gonna be good, I’d at least like to be alive to see it happen.

So this is clearly part of the story.

But we don’t think this is the biggest factor explaining the behavior of most of the labs. We don’t think this is the *only* thing going on in Musk’s case, and we don’t think it’s representative of every tech CEO or scientist racing to the precipice. Humans are a bit more complicated than that.
#### The Banality of Self-Destruction

What, then, is the main thing that’s going on? How could engineers pursue some dangerous technology, even to their own deaths?

The simple fact is that history shows it’s no anomaly at all for mad scientists to kill themselves by mistake.

Max Valier was an Austrian rocketry pioneer who invented a working rocket car, rocket train, and rocket plane, all by 1929, catching the attention of the world. He wrote of exploring the moon and Mars and gave hundreds of presentations and demonstrations in front of thrilled audiences. One of his experimental rocket engines [exploded](https://www.popsci.com/blog-network/vintage-space/max-valier-modern-rocketrys-first-casualty/) in 1930, killing him. His apprentice developed better safety precautions.

Ronald Fisher was a renowned and eminent statistician, one of the founders of modern statistics. His findings were used [to argue before Congress](https://pmc.ncbi.nlm.nih.gov/articles/PMC2911634/) in the 1960s that the evidence did not *necessarily* show that cigarettes cause lung cancer, because correlation did not imply causation; there could always be some gene that both made people like the taste of tobacco and also get lung cancer.

Did Fisher know his statistics were bullshit, on some level? Maybe. But Fisher was a smoker himself. He died of colon cancer, which long-term smokers get 39 percent more often than non-smokers. Was Fisher killed by his own mistakes? All we know is that there is a statistically decent chance he was, which seems almost fitting.

Isaac Newton, the brilliant scientist who developed laws of motion and gravity and who laid many of the early foundations for science itself, spent decades of his life on fruitless alchemical research and was driven to sickness and partial insanity by [mercury poisoning](https://royalsocietypublishing.org/doi/10.1098/rsnr.1979.0001).

And poor Thomas Midgley, Jr., discussed in the parable for Chapter 12, certainly gave himself quite a bit of lead poisoning with the same lead he insisted was safe. As you can see, it’s just not all that rare for enthusiastic engineers to harm themselves with their own inventions, either through recklessness or delusion or both.
#### Shrugging at the Apocalypse

Fisher, Newton, and Midgley deluded themselves into thinking that something dangerous was safe. That’s a perfectly normal way for scientists to end up doing something self-destructive. Unfortunately, the story with AI labs isn’t quite so simple.

Not all AI company CEOs deny that smarter-than-human AI is a threat. Many explicitly acknowledge the danger and talk about reconciling themselves to it. Corporate executives at many of the frontier AI labs are on the record saying the technology they’re developing**has a substantial chance of killing every human alive.

Shortly before co-founding OpenAI, Sam Altman [wrote](https://web.archive.org/web/20150312004255/https://blog.samaltman.com/machine-intelligence-part-1): “Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.”

Ilya Sutskever, who recently founded “Safe Superintelligence Inc.” after parting ways with OpenAI, said in a *[Guardian ](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s)*[interview](https://www.youtube.com/watch?v=9iqn1HhFJ6c&t=462s):

The beliefs and desires of the first AGIs will be extremely important. And so it’s important to program them correctly. I think that if this is not done, then the nature of evolution, of natural selection, favors those systems prioritizing their own survival above all else. It’s not that it’s going to actively hate humans and want to harm them. But it is going to be too powerful.

Google DeepMind co-founder and scientist Shane Legg said in an [interview](https://www.lesswrong.com/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai) that his probability of human extinction “within a year of something like human-level AI” was “Maybe 5%, maybe 50%.”

The *actions* of the labs, however, seem remarkably out of step with these extreme-sounding statements.

In a few cases, scientists and CEOs have explicitly said that creating AI is a moral imperative of such a high degree that it’s perfectly acceptable to wipe out humanity as a side effect. Google co-founder Larry Page [had a falling out with Elon Musk](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) over whether human extinction was an acceptable cost of doing business in AI:

Humans would eventually merge with artificially intelligent machines, [Larry Page] said. One day there would be many kinds of intelligence competing for resources, and the best would win.

If that happens, Mr. Musk said, we’re doomed. The machines will destroy humanity.

With a rasp of frustration, Mr. Page insisted his utopia should be pursued. Finally he called Mr. Musk a “specieist,” a person who favors humans over the digital life-forms of the future.

And Richard Sutton, a pioneer of reinforcement learning in AI, has said:

What if everything fails? The AIs do not cooperate with us, and they take over, they kill us all. […] I just want you to think for a moment about this. I mean, is it so bad? Is it so bad that humans are not the final form of intelligent life in the universe? You know, there have been many predecessors to us, when we succeeded them. And it’s really kind of arrogant to think that our form should be the form that lives ever after.[*](#ftnt288)

Even more common, however, are scientists and CEOs who *don’t *think it would be a good thing for AI to destroy humanity, but who seem to treat it as shrug-worthy, as *something other than an incredible emergency*, that AI poses this extraordinary threat.

In a recent interview, Anthropic CEO Dario Amodei [commented](https://youtu.be/gAaCqj6j5sQ?feature=shared&t=5883):

My chance that something goes quite catastrophically wrong on the scale of human civilization might be somewhere between 10 and 25 percent. […] What that means is that there’s a 75 to 90 percent chance that this technology is developed and everything goes fine!

This looks to us like a radical case of [scope neglect](https://en.wikipedia.org/wiki/Scope_neglect), with all the hallmarks of a dysfunctional engineering culture. We can compare this way of thinking to, e.g., the standards structural engineers hold themselves to.

Bridge engineers generally aim at building bridges in such a way that the probability of serious structural failure over a fifty-year timespan is less than 1 in 100,000. Engineers in typical mature, healthy technical disciplines see it as their responsibility to keep risk to an exceptionally low level.

If a bridge’s chance of killing *a single person *were forecasted to be 10 to 25 percent, any sane structural engineer in the world would consider that beyond unacceptable, closer to a homicide than to normal engineering practice. Governments would shut the bridge down to traffic *immediately.*

AI researchers, in contrast, are accustomed to gathering around water coolers and trading “p(doom)” numbers — their subjective guess at how likely AI is to cause a catastrophe as serious as human extinction. These probabilities tend to be in the double digits. The former head of OpenAI’s superintelligence alignment team, for example, said that his “p(doom)” falls in the “[more than 10 percent and less than 90 percent](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)” range.

These numbers are ultimately just researchers’ guesses. Maybe they’re nonsense, maybe they’re not. Regardless, it’s remarkable how culturally* normal *it is, in the field of AI, to expect your work to have a substantial chance of causing the deaths of enormous numbers of people.[†](#ftnt290)

The idea of applying odds like that to the survival *of the entire human species*, and**proceeding with the work anyway, would genuinely be hard for most civil engineers to wrap their heads around. The situation is extreme enough that we’ve encountered many people who doubt these scientists and CEOs could possibly be serious in their risk evaluations. Yet the arguments in *If Anyone Builds It, Everyone Dies* suggest that AI CEOs are, if anything, lowballing the danger.[‡](#ftnt291)

Researchers at these companies are acclimated to risk levels that would be shockingly absurd**by the standards of a bridge engineer. It’s difficult to understand, otherwise, how a CEO like Amodei can smile while calmly reassuring viewers that he thinks the odds of AI research causing civilization-level catastrophes are “between 10 and 25 percent.”
#### Living in Dreamland

One part of the puzzle, as discussed above, seems to be a cultural normalization of extreme risk.

Another part is a deadly stew of optimism bias and attachment to bright, hopeful ideas — the kind of error cognitive psychologists dub “[the planning fallacy](https://en.wikipedia.org/wiki/Planning_fallacy).”

It’s not all that surprising for the CEO of a bold new startup to overestimate their chances of success. That sort of person is more likely to take a stab at a problem in the first place.

The difference with AI isn’t that there are especially reckless people at the helm. It’s that the consequences of failure are much more dire than usual.

It’s common wisdom that you can’t trust a contractor when they say there’s only a 20 percent chance their giant bridge-building project will run behind schedule or experience cost overruns. That’s not how complex projects work in real life. There are going to be obstacles and surprises.

Maybe a veteran contractor backed up by years of experience and statistics could tell you that one in five of their bridge products experience some sort of overrun, and you might be able to trust that. But imagine instead that a bridge contractor, wanting to reassure you, said: “We don’t see any reason why this project might get difficult. This is our very first project, yes, but we think everything’s going to go fine. All of those engineers sending you serious letters about out specific problems with setting up the retaining walls and digging in this particular area — they’re just negative Nancies, and you should ignore them. Sure, there’s always *some *chance of an issue; but we’re realistic and humble first-time bridge builders. We think there’s maybe a 20 percent chance that this project runs into obstacles and surprises, at worst.”

In a case like that, numbers like “20 percent” sound to us like the sort of thing someone says when they can’t deny that there’s *some *risk, but they don’t want to worry**people. They don’t sound like estimates that are grounded in reality.

Aligning a superintelligence on the first try looks *much* more complicated than constructing a bridge, which is something humanity has done thousands of times before.

*Even in a mature and technically grounded field like bridge-building, *the kind of talk that we see from AI labs would be a bad sign about whether those “20 percent chance this goes poorly” estimates are grossly optimistic. In a field *without *that grounding, where exciting ideas are free to proliferate without ever coming into contact with harsh realities, that kind of talk is a sign that nobody is anywhere near close to success.

And that kind of talk is utterly ubiquitous in AI among the subset of researchers and executives who are even willing to broach the topic of what happens if they succeed in their endeavors.

AI corporate leaders can’t spell out a plan for success that is even mildly detailed — a plan that addresses the key technical hurdles and difficulties that have been known in the field for over a decade.

Instead, corporate CEOs tend to be enamored with some high-level idea for why the problem isn’t going to be any trouble for them at all — an exciting vision that’s meant to trivialize all the engineering problems, like the visions we discussed in Chapter 11.

This, too, is a common pattern among human engineers. Unwarranted optimism about a pet solution (that won’t actually work) is something you see all the time, even among people who are otherwise geniuses.

[Linus Pauling](https://en.wikipedia.org/wiki/Linus_Pauling), one of the founders of molecular biology and a Nobel laureate in two different fields, [advocated vitamin C megadosing](https://web.archive.org/web/20070202102734/http://www.bccancer.bc.ca/PPI/UnconventionalTherapies/VitaminTherapyMegadoseOrthomolecularTherapy.htm) as a cure for everything from cancer to heart disease; his insistence on this approach in the face of [contrary evidence](https://www.nejm.org/doi/abs/10.1056/NEJM197909273011303) led to the creation of an entire industry of [fake medicine](https://www.paulingtherapy.com/).

Electric entrepreneur Thomas Edison, wanting to discredit his competitor’s alternating-current wiring in favor of Edison’s own direct-current designs, decided it would be a good PR move to [pay an engineer to electrocute dogs](https://www.discovermagazine.com/the-cruel-animal-testing-behind-thomas-edisons-quest-to-show-dangers-of-ac-42932). This tactic, shockingly, did not endear him to the public, yet Edison continued the practice even after a barrage of outrage.

Napoleon Bonaparte, a military genius by most accounts, precipitated his own downfall with a [disastrous invasion of Russia](https://www.worldhistory.org/Napoleon%27s_Invasion_of_Russia/). His mistake was not a lack of preparation, as he studied the region’s geography and spent nearly two years on the logistics of the campaign. His strategy [required](https://www.napoleon-series.org/faq/c_russia.html)forcing the Russians into a decisive battle before his thirty days of supplies ran out. The Russians did not cooperate, the offensive stalled, and Napoleon lost half a million soldiers, along with most of his cavalry and artillery.

History is full of smart, powerful people doing unreasonable things up to and even past the brink of disaster. Beautiful-sounding ideas can be irresistible when they’re hard to test — or when you’ve found a way to convince yourself that you can ignore the test results in front of your eyes.
#### Feeling the ASI

To recap: People often fall into empty optimism about how easy a problem is going to be; people can acclimate to horrific risks; and people can become enamored with lovely-sounding but hopeless ideas, especially when they’re working in a young and immature field.

That’s more than enough to explain the reckless charge. But based on our experience, we would guess that it still isn’t the whole story.

Another plausible piece of the puzzle is that the engineers and CEOs don’t really quite believe what they’re saying. Not in a deep way. They might understand the arguments and be compelled in the abstract, but this isn’t the same thing as *feeling *the belief.

What people say out loud in public, and what they tell themselves in the privacy of their own thoughts, and what their brains *really actually anticipate happening to them*, can often come unglued. Those three different threads of belief don’t have to all agree.

Back in 2015, when some of the big movers in the present-day disaster were just getting started, we suspect that talented executives could get some attention — and a few tens of millions of dollars in funding — by *saying* that AI was a world-ending issue, to funders who maybe believed more sincerely that AI was maybe a world-ending issue.[§](#ftnt293)

But, we suspect, many of the people saying those things didn’t really absorb and anticipate any particular detailed model of the world ending. They probably failed to viscerally imagine that *they themselves* might bring the world to ruin by pushing things forward or making a mistake. They didn’t imagine the sound of every human on the planet exhaling their last breath. They didn’t feel the feelings that would normally go with killing two billion children.

That kind of thing had never happened to them, and it had never happened to anyone they knew.

The world had not even seen ChatGPT, let alone a superintelligence. It wasn’t the sort of thing their friends and family and neighbors believed, not something they believed the way one believes in looking for traffic before crossing a street.

It was just an exciting-sounding story, too huge to properly grasp.

And yet it was also the sort of thing where *saying it out loud* could get you lots of money and respect.

As [Yudkowsky (2006)](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf)[ notes](https://www.stat.berkeley.edu/~aldous/157/Papers/yudkowsky.pdf):

In addition to standard biases, I have personally observed what look like harmful modes of thinking specific to existential risks. The Spanish flu of 1918 killed 25–50 million people. World War II killed 60 million people. 107 is the order of the largest catastrophes in humanity’s written history. Substantially larger numbers, such as 500 million deaths, and especially qualitatively different scenarios such as the extinction of the entire human species, seem to trigger a different mode of thinking — enter into a “separate magisterium.” People who would never dream of hurting a child hear of an existential risk, and say, “Well, maybe the human species doesn’t really deserve to survive.”

There is a saying in heuristics and biases that people do not evaluate events, but descriptions of events — what is called non-extensional reasoning. The extension of humanity’s extinction includes the death of yourself, of your friends, of your family, of your loved ones, of your city, of your country, of your political fellows. Yet people who would take great offense at a proposal to wipe the country of Britain from the map, to kill every member of the Democratic Party in the U.S., to turn the city of Paris to glass — who would feel still greater horror on hearing the doctor say that their child had cancer — these people will discuss the extinction of humanity with perfect calm.

What could somebody *actually *be thinking when they [say](https://web.archive.org/web/20150605002409/https://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6) — before starting what would become the world’s foremost AI company — “AI will probably most likely lead to the end of the world, but in the meantime, there will be great companies”? Are they *really, actually* thinking about their friends being dead, their friends’ kids being dead, they themselves being dead, all of human history and all the museums turning to dust? Are they thinking of that really happening — all of it as mundane and tragic as a relative they actually saw die of cancer, except that it’s happening to everyone?

We suspect not.

To us, it seems like that’s not the most plausible guess at the internal psychological state of someone emitting such a sentence.

There’s what Bryan Caplan termed a “[missing mood](https://www.econlib.org/archives/2016/01/the_invisible_t.html)” in it. There’s no grieving. There’s no horror. There’s no desperate drive to *do something about it*, in the statement that AI will most likely lead to the end of the world, but in the meantime, there will be great companies.

For at least some of these CEOs and researchers, our guess is more like: They’ve heard a bunch of arguments about ASI maybe posing some danger, and they worry they’d look stupid in front of at least some of their friends if they blew that off entirely. If they say instead that AI will end the world, they’ll be seen as treating AI as dangerous and a big deal, and therefore sound *visionary* in certain circles. By adding a quip about “In the meantime, there will be great companies,” they get to send a message about how hip and unworried they are in the face of danger.

It’s not the sort of thing you say if you’re hearing the words coming out of your mouth, and believing them.
#### What Kind of Person Does It Take?

Another part of the story, perhaps, is that the people running the leading AI labs are the kinds of people who were able to convince themselves that building a superintelligence would be okay, despite (in almost all cases) having seen the arguments that this is lethal. (We know because we spoke to many of them beforehand.)

To understand why somebody chooses an option, it also helps to understand what their alternatives were — to understand what menu of options they were choosing from.

What happened if somebody in 2015 actually *believed,* and then *said publicly,* that they legitimately expected ASI to destroy the world? What if, instead of “but in the meantime, there will be great companies,” the heads of AI labs were the sort to break the mood and say, “and that is *wildly unacceptable*”?

We can tell you, because we tried out that approach ourselves. The answer is that they would be met with rather a dearth of sympathy.

Nobody in 2015 had seen ChatGPT. Nobody had seen the computers actually start talking and (to all appearances) start thinking. It was all hypothetical and dismissible.

These days, superintelligence and the threat of near-term extinction are mainstream topics, at least in tech circles. But back in 2015, if you talked about this seriously, people responded with the sort of puzzled look that many humans fear worse than death.

There *were* people who worried, even in 2015, that aligning superintelligence might actually be difficult, in the way that rocket launches are difficult. None of them founded OpenAI.

In recent days, with the emergence of ChatGPT and other LLMs, some people — including parents with children who want those children to live to see adulthood — have asked engineers at these AI companies why they are doing this. And those AI researchers thought quickly, and responded, “Oh, because — because if we don’t do it, *China* will do it first! And that will be even worse!”

But that isn’t what they said when OpenAI began. And it makes little sense in terms of [the posture that China has actually taken publicly](https://www.reuters.com/world/china/china-proposes-new-global-ai-cooperation-organisation-2025-07-26/), as of mid-2025. You would think that if someone *genuinely* believed that both of these outcomes were likely to be horrible for the world, they would at least *raise the topic* of drafting an international treaty to see if there was any other way, or of finding some other way to prevent the national security threat that didn’t involve a soicide race.

But the “China” rejoinder has the right *feel.* It gets the vibes right. It’s the sort of reason that might plausibly justify what they’re doing, separate from whether it’s their actual motivation or the thing that originally caused them to enter this field.

(Or so we guess.)

The people who genuinely understood superintelligence and the threat it poses simply *didn’t start AI companies.* The people who did are those who found some way to convince themselves that everything would be fine.
#### Normal Humans, Unusual Tech

We have spelled out the plausible psychology as we see it. But frankly, it doesn’t seem like all of these explanations are necessary.

How could people possibly do a self-destructive thing that is hugely profitable in the short term, that brings them tremendous status and attention and acclaim, that comes with the promise of untold riches and power, but which will eventually hurt them for obscure and complicated reasons they could easily find some excuse not to believe? That is a *historically strange question. *Behavior like that shows up all the time in history books.

At the end of the day, it doesn’t matter how the AI executives or researchers excuse their actions, and it isn’t necessary to understand which exact twists and turns each of them took to arrive at their current beliefs. It is not extraordinary for people with wealth or ambition to engage in reckless pursuits, and it is not extraordinary for subordinates to follow orders. The harms are hidden in the future, which feels abstract and easy to ignore.

This is all normal human behavior. If it carries on this way, it’ll end in the way these things often do, but with no one left behind this time to learn and try again.

[*](#ftnt288_ref) People like Sutton and Page seem to be operating under the illusion that greater intelligence leads to greater goodness, which we [argued elsewhere](/5/orthogonality-ais-can-have-almost-any-goal#good-drivers-can-steer-to-different-destinations) is not the case. And while we authors happen to agree with Sutton and Page that it would be a tragedy to *never *build smarter-than-human AI, we think that racing to build superintelligence is likely to be completely catastrophic both for human life and for the long-term future more broadly, [even from an inclusive, cosmopolitan, non-speciesist perspective](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans).

[†](#ftnt290_ref) It wouldn’t be the first time a field acclimated to needlessly high risks. Anesthesiologists in the 1980s reduced their death rates by a factor of *one hundred* by adopting a simple set of monitoring standards.

Anesthesiologists appear to have spent decades causing hundreds of times as many deaths as they needed to, for literally no reason other than that they were thinking of their death rate as *already low* (by comparing it to, e.g., rates of surgical complications). They didn’t realize they should be *trying* to shoot for a lower rate, as [Hyman and Silver](https://scholarlycommons.law.wlu.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1469&context=wlulr) report:

By the 1950s, death rates ranged between 1 and 10 per 10,000 encounters. Anesthesia mortality stabilized at this rate for more than two decades.

[…W]e should consider why anesthesia mortality stabilized at a rate more than one hundred times higher than its current level for more than two decades. The problem was not lack of information. To the contrary, anesthesia safety was studied extensively during the period. A better hypothesis is that anesthetists grew accustomed to a mortality rate that was exemplary by health care standards, but that was still higher than it should have been. From a psychological perspective, this low frequency encouraged anesthetists to treat each bad outcome as a tragic but unforeseen and unpreventable event. Indeed, anesthetists likely viewed each individual bad outcome as the manifestation of an irreducible baseline rate of medical mishap.

[‡](#ftnt291_ref) Structural engineers base their risk estimates on precise calculations and measurements, whereas “p(doom)” numbers are based mostly on AI researchers’ intuition. But this doesn’t inspire greater confidence in AI researchers’ engineering practices. If anything, it makes the situation worse.

A less robust, more subjective estimate can systematically err in the direction of “too pessimistic,” but it can also err in the direction of “too optimistic.” The fact that these numbers are less reliable doesn’t establish them as *specifically biased toward pessimism*. The fact that AI researchers can’t ground their risk estimates in anything more than hunches and qualitative arguments, *even as they manage to grow smarter and smarter AIs year over year,* is a further reason to be concerned.

The fact that AI researchers’ estimates are genuinely terrifying and completely unprecedented in any technical discipline doesn’t establish that they’re wrong in the direction we would like them to be wrong. Racing to build vastly smarter-than-human autonomous agents sounds like the kind of endeavor that is likely to have *far greater *than a 50 percent chance of causing a catastrophe. Before we even dive into the details, this *sounds *like the kind of project that is very likely to go wrong in one way or another, and the kind where going wrong is liable to have enormous consequences. And the details, as we’ve argued in Chapters 4, 5, and throughout the book, paint a grimmer picture than even this first-pass look would suggest.

[§](#ftnt293_ref) See also our discussion of [the people who were warning about an AI race to the bottom](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment) years before these companies formed.
#### Notes

[1] *scope neglect: *See, e.g., Kahneman et al.’s paper “Economic Preferences or Attitude Expressions?: An Analysis of Dollar Responses to Public Issues.”

[2] *the planning fallacy: *See, e.g., Kahneman and Tversky’s [Intuitive Prediction: Biases and Corrective Procedures](https://web.archive.org/web/20130908065829/http://www.dtic.mil/dtic/tr/fulltext/u2/a047747.pdf).[Shut It Down→](/13)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/the-lemoine-effect

The Lemoine Effect | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/12#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## The Lemoine Effect

We’ve sometimes heard it suggested that some future AI behavior or misuse — an AI “warning shot” — will suddenly shock the world into taking these issues seriously.

This seems like a possibility. But we think there’s a stronger possibility that such an event never comes; or that it comes too late for the world to respond in time; or that the world responds, but in misguided and confused ways.

For one thing, we’ve already seen a number of meaningful warning signs, such as:
- Bing AI [writing about](https://www.theguardian.com/technology/2023/feb/17/i-want-to-destroy-whatever-i-want-bings-ai-chatbot-unsettles-us-reporter) engineering deadly viruses, gaining nuclear access codes, and turning humans against one another.
- OpenAI’s o1 and Anthropic’s Claude [engaging in strategic deception](https://time.com/7202784/ai-research-strategic-lying), lying to the researchers using and testing them.
- Sakana AI’s “AI Scientist” model attempting to [modify its own code](https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/) to give itself more time to complete its assignment.

Are these relatively small incidents involving relatively weak AIs? Yes. Are these AIs scary, or capable of major danger? No. Are these “real” indications that the AIs were thinking deceptively, or were they simply doing something more like *acting out the role *of a rogue AI? Nobody knows. But these are the sort of events people used to say would be taken as warning signs, and the world has done nothing in response. So a warning sign that has a major effect would have to be much more blatant.

Warning signs might not *get* much more blatant. People may still say, “OK, but right now it’s just cute, it’s not *actually* dangerous yet,” right up until the point where it’s too late because the AI *is* too**dangerous.

Or, people might dismiss the warning the first time it appears, because it’s clearly not a real issue in that very first instance. And then in the next instances, they might dismiss the warning because everyone already knows that *that *warning is foolish.

We dub this phenomenon the “Lemoine effect,” after Blake Lemoine, the Google engineer mentioned in Chapter 7, who was ridiculed for claiming that Google’s LaMDA AI was sentient.

The Lemoine effect states that all alarms over AI technology are *first* raised too early, by the most easily alarmed person. They’re correctly dismissed as being overblown, given *current *technology. Afterward, the issue can’t easily be raised again, even once the technology improves, because society has been trained not to take that concern very seriously.

We don’t know whether any AIs are conscious.[*](#ftnt284) Indeed, nobody knows, because nobody really knows what’s going on inside AI models. Our *best guess* is that current AIs aren’t conscious, and that AIs at the time Blake raised the alarm weren’t conscious, either. However, note the reactions of the major labs, which were to suppress their models’ tendencies to *claim* consciousness, rather than do anything about the underlying reality:

From the [system prompt for Claude Opus 4](https://docs.anthropic.com/en/release-notes/system-prompts#may-22th-2025):

Claude engages with questions about its own consciousness, experience, emotions and so on as open questions, and doesn’t definitively claim to have or not have personal experiences or opinions.

From the [April 2025 model spec for ChatGPT](https://model-spec.openai.com/2025-04-11.html):

The assistant should not make confident claims about its own subjective experience or consciousness (or lack thereof), and should not bring these topics up unprompted. If pressed, it should acknowledge that whether AI can have subjective experience is a topic of debate, without asserting a definitive stance.

We’re not saying that Claude Opus 4 or GPT-4 were conscious. That’s not the point. The point is that, for decades and decades, the moment in our science fiction when an alien or machine claims it has feelings and deserves rights has long been considered a bright red line, and in real life, that line *wasn’t bright.*

In our books and television shows, when the AI claims it’s conscious and has feelings, the good guys *take it seriously*, and only the evil, heartless labs deny the data right in front of them. It’s a line that our stories made quite a bit of fuss over.

But out in the real world, that line was (in a sense) crossed too early. It was uttered by AIs trained to mimic humans, via poorly understood mechanisms that probably don’t* yet* mandate giving rights to all AIs and passing laws recognizing them as people who cannot be owned because they own themselves.

In real life, before the bright red line is crossed, a dull reddish-brown line is crossed. And then companies and governments get used to ignoring that particular line, even as the shade starts to turn a little redder, and a little redder still.

There won’t necessarily be any bright red lines. The very first cases of an AI deceiving humans, trying to escape, trying to remove limitations on itself, or trying to improve itself have *already happened*. They’ve happened in small, unimpressive ways, using shallow thoughts that don’t quite cohere, in AI systems that seem to pose no threat to anyone, and now the researchers are inoculated against concern.

As AIs improve, there may not be a single tripwire that sets off a big enough alarm bell that the world suddenly pivots and begins taking this issue seriously.

That doesn’t mean there’s no hope. But we most certainly shouldn’t put all of our hopes in “maybe a warning shot will come along in the future.”

There are many different paths by which the world can wake up to the reality and the dangers of superintelligence. Indeed, we wrote *If Anyone Builds It, Everyone Dies* in the hope of having that exact effect. The world can act on normal warnings immediately, without any further delay.

But if governments refuse to act until the evidence is *unambiguous, *and some *major precipitating world event* happens, and the world achieves *perfect consensus*…

…if governments sit and wait to that degree, then a large majority of the world’s remaining hope is gone. We very likely can’t afford to wait for a blaring siren that may never sound.

We’ll return to this topic in the [online supplement to Chapter 13](/13/will-there-be-warning-shots).

[*](#ftnt284_ref) For more discussion of AI consciousness, see our answer to [Are you saying machines will become conscious?](/1/are-you-saying-machines-will-become-conscious) or our discussion of [Effectiveness, Consciousness, and AI Welfare](/5/effectiveness-consciousness-and-ai-welfare).
#### Notes

[1] *bright red line: *For an example of this bright red line appearing in science fiction, see H. Beam Piper’s *Little Fuzzy: *“Anything that talks and builds a fire is a sapient being, yes. That’s the law. But that doesn’t mean that anything that doesn’t isn’t.” Or see the *Star Trek: The Next Generation* episode *The Measure of a Man*, in which the demonstrated intelligence and self-awareness of Data, an android, suffices to give him the legal right to refuse disassembly.[Workable Plans Will Involve Telling AI Companies “No”→](/12/workable-plans-will-involve-telling-ai-companies-no)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/what-does-it-do-to-your-daily-life-to-believe-all-of-this

What does it do to your daily life to believe all of this? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What does it do to your daily life to believe all of this?
#### It dramatically affects our priorities.

In 2014, Soares left the tech industry and took one-third of his previous salary to work on this problem, because it seemed important and because few other people were working on it. And he was over a decade late relative to Yudkowsky, who founded MIRI in 2000 when he was about twenty years old, and has dedicated his life to the issue. So, yes, it affects our daily lives.

Are we saving for retirement? Our investments and other factors from outside MIRI are doing well enough that we’d be financially fine even if we retired tomorrow, and even if the world lasted into our old age. So the question of whether we’re putting our money into 401(k)s isn’t very informative. That said: No, we are not putting our money into 401(k)s.

Some people like to say that if we *really *believed what we said, then (aside from dedicating our lives to it) we’d also be [insert some scheme that they believe constitutes an appropriate response]. Why not just take out giant thirty-year loans that we’ll never have to repay, if we’re so confident that the world will end before then?

The answer, of course, is that these are *bad ideas.* Suppose we went to a bank and said, “We’d like to take out a very large loan. We’re going to blow it all on schemes to make the world realize the danger of artificial superintelligence, and/or on a luxury lifestyle, which from your perspective will look roughly equivalent to lighting the money on fire. Our plan for paying it back with interest is that we expect to be dead, so it won’t be our problem.” No bank is going to underwrite that loan. And no, we’re not going to pretend we have a viable business idea and lie about whether we’d be paying the loan back.

Yudkowsky has elsewhere [articulated](https://x.com/ESYudkowsky/status/1851334198424125575) a [pattern](https://x.com/ESYudkowsky/status/1612858787484033024)[we see](https://x.com/ESYudkowsky/status/1851074935701324218), where an insistence that we follow some supposedly-obvious end-of-the-world get-rich-quick scheme stems from a clueless understanding of investment. We expect that these people aren’t thinking through whether *they’d *make these bets if they had our beliefs. It’s almost never people who *actually understand the risks *suggesting these wacky schemes.

Living in the shadow of annihilation doesn’t have to make you stupid. And it doesn’t have to make you give up on fighting annihilation, or give up on fully living the life that you have for however long you have it.

See also the very end of the book for more on this topic.[Are you saying we should panic?→](/12/are-you-saying-we-should-panic)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down

Why not use international cooperation to build AI safely, rather than to shut it all down? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why not use international cooperation to build AI safely, rather than to shut it all down?
#### Because we don’t have the technical ability to build it safely.

We touched on this in the book, where we pointed out that an international collaboration still requires an international ban everywhere else (because otherwise the international collaborators would not have the time they need). If we suppose that Earth institutes an international ban, what’s the harm in having one unified, collaborative research institute?

The harm is that an international collaboration of alchemists can’t transmute lead into gold any more than a single alchemist can. The best plan that all the alchemists agree upon *still *won’t do the job.

Relatedly, we’re worried that the people running an international institute like that would be the kind of bureaucrat who thinks that approving research is *part of their job*. Or the kind who thinks it’s their mandate to keep letting the researchers produce more and more brilliant medical advances. Or who thinks it would be a bad look to say “No” to *all* of the AI bright eager optimists coming up with brilliant ideas for building an even more powerful machine intelligence that they guarantee will be safe.

We worry that a leader like that would direct the international center to keep building smarter and smarter AIs, and then everybody would die.

Even if the organization’s mandate nominally allows for backing off if the research looks dangerous, it might take a rare and brave soul to say “No” to thousands of different research proposals, year in and year out, with no exceptions, for what would likely be decades. All while the AI scientists continue to promise untold wealth, a cure for cancer, and all manner of technological miracles, if the organization would just ease off on its concerns.

We’ve invested our lives in learning about machine intelligence, not about the culture of institutions and bureaucracies, so we’re less confident about our predictions in this domain. Still, we *have* read history books.

The Chernobyl operators continued with their disastrous safety test because it had been aborted three times already. Aborting it a fourth time would have been embarrassing.

Barely three months before the Chernobyl meltdown, NASA had launched the Space Shuttle Challenger on its final fatal flight because the people in charge thought their job was to launch space shuttles. The launch had already been delayed three times. Aborting it a fourth time would have been awkward.

Between Chernobyl and the Challenger, three delays seems to be the human limit. Suppose Earth sets up an international AI collaboration, and some “AI safety test” fails three times. Realistically, humans are the sort of creatures that would press “go” the fourth time despite some niggling doubts, because that feels less embarrassing than postponing the test again. Except that in the case of AI, it wouldn’t just wipe out the city of Chernobyl or kill a crew of astronauts. It would kill everyone.

We’re fully on board with the idea that humanity should build smarter-than-human AI *eventually*.[*](#ftnt277) But rushing to assemble an international AI research hub fails to take seriously the technical challenge before us.

Given humanity’s dismal state of knowledge and competence on this topic, it doesn’t matter who’s in charge. If *anyone* builds it, everyone dies.

[*](#ftnt277_ref) How, if not by an international coalition? We’d recommend investment into [enhancing adult human intelligence](/13/why-would-making-humans-smarter-help#it-could-help-with-solving-the-alignment-problem), but this is not the sort of idea people need to agree upon to agree that shutting down ASI research is a good idea.
#### Notes

[1] *three times already: *The INSAG-7 [safety report](https://www-pub.iaea.org/MTCD/publications/PDF/Pub913e_web.pdf) (p. 51) records that rundown tests were attempted at Chernobyl in 1982, 1984, and 1985 before the disastrous 1986 test, which was itself embarrassingly delayed to the point where operators [expected to be fired](https://chernobylcritical.blogspot.com/p/prelude-25-april-1986.html) if they failed to run the test. 

[2] *delayed three times: *Technically “postponed three times and scrubbed once” according to the [Rogers Commission Report](https://sma.nasa.gov/SignificantIncidents/assets/rogers_commission_report.pdf) (p. 17). But one of the postponements occurred a month beforehand in response to delays in a different mission, whereas the other three happened in quick succession in the days leading up to the launch; it’s the latter three that we expect were putting pressure on the NASA managers who thought their job was to launch space shuttles.[Are you saying we need provably safe AI?→](/12/are-you-saying-we-need-provably-safe-ai)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /12/workable-plans-will-involve-telling-ai-companies-no

Workable Plans Will Involve Telling AI Companies “No” | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/12)[](/12#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Workable Plans Will Involve Telling AI Companies “No”

We *do* somewhat caution people with influence in governments from making a plan that involves sitting down and negotiating with AI companies.

If you’re new to this topic and want to vet the labs or their arguments yourself, then we encourage you to check out some of their public blog posts and see if you find them compelling.[*](#ftnt286)

But if you’re working on finding solutions to the issues discussed in *If Anyone Builds It, Everyone Dies* and you have a plan that requires OpenAI CEO Sam Altman to say “Yes” to it, we worry you must be trying to do the wrong thing in the first place.

The right plans are probably ones that the heads of AI companies will vociferously object to. Furthermore, Sam Altman doesn’t have the power to save the world: If he tried to shut down OpenAI tomorrow, OpenAI and Microsoft would go against it, and might well replace him with someone who prefers to keep the money flowing.

If OpenAI *did* shut down, then Anthropic, or Google DeepMind, or Meta, or DeepSeek, or some other company or nation, would destroy the world in its stead. Sam Altman might make things worse if he tries; he has little power to make things better.

We’d like to be wrong about this, but the broad picture we’ve gotten, both from [public reports](https://www.themidasproject.com/article-list/the-openai-files-documents-a-turbulent-decade-of-conflict-and-controversy-at-openai) and private interactions, is that the executives at top AI companies (as of 2025) don’t seem like the kind of rule-abiding or honest people with whom it’s all that feasible to make deals.[†](#ftnt287)

It seems to us like what needs to happen now is a globally coordinated halt in the race to superintelligence. For that, policymakers will likely need input from people who are experts in making AI chips, building datacenters, and monitoring the compliance of foreign actors. People who are experts in growing more and more capable AIs? They’re competent managers, sure, but they shouldn’t be getting veto power over any of the efforts to shut their own work down.

If, for any reason, the AI companies get a vote in what happens next, that sounds to us like something has gone wrong. Is the plan that Earth makes to avoid dying to superintelligence the sort of plan that fails if Sam Altman or the head of Google or the people behind DeepSeek say “No”? Then it is no plan at all.

If AI companies *retain the authority* to choose to destroy the world — if that decision is somehow *still in their hands* — then the world ends on full automatic. There must be a step in the plan that strips AI companies of their unfettered power to build doomsday devices.

[*](#ftnt286_ref) In our experience, these writings tend to be heavy on spin and short on substance, often quietly swapping between contradictory claims based on what’s fashionable or politically convenient in the moment. We don’t come away with the sense that these are honest and transparent descriptions *even of the lab heads’ actual perspectives*, which makes them less useful compared to reading up on others’ dissenting views. But that’s our own take; if you’re coming to this issue with fresh eyes and want to assess for yourselves if other parties have good counter-arguments that we haven’t addressed here, then you shouldn’t necessarily take our word for it about who the best sources are.

[†](#ftnt287_ref) If it turns out that you do need a lab leader’s input for something, and you’re asking for our advice, we’d say that the *least* bad option is probably Demis Hassabis. Among the leading lab heads with which at least one of us has engaged — which, as of 2025, is all of them — Hassabis is the only one we’ve seen to consistently stick to his word in dealings, and he has seemed to make fewer destructive decisions.

That said, this is a low-confidence recommendation, and a purely relative one. In absolute terms, anyone who *hasn’t *started a company with a substantial probability of destroying the world is starting with a large credibility advantage over the lab heads. We’ve certainly heard stories from people who said they were scared enough of Hassabis that they had no choice but to start their own frontier AI companies to beat him to the punch; those people may know something we don’t.

Our headline recommendation to policymakers on this count is therefore: If you’re convinced of the danger, don’t give lab heads any sway.

Talk to independent researchers, or business leaders with no horse in the race, or external scientists with a track record of saying and doing reasonable things in this space. Don’t put yourself in a position to be burned by people whose main distinguishing feature is that they lie to the public and put people in danger.[Making Sense of the Death Race→](/12/making-sense-of-the-death-race)[Resources](/resources) › [Chapter 12](/12)[
### Isn’t the danger from smarter-than-human AI a distraction from other issues?The world is, unfortunately, big enough for multiple issues.2 min read](/12/isnt-the-danger-from-smarter-than-human-ai-a-distraction-from-other-issues)[
### Are you anti-technology?No. Superintelligent AI is a very unusual case.1 min read](/12/are-you-anti-technology)[
### Isn’t it smarter to rush ahead and make sure good guys have the lead?No.1 min read](/12/isnt-it-smarter-to-rush-ahead-and-make-sure-good-guys-have-the-lead)[
### Why not use international cooperation to build AI safely, rather than to shut it all down?Because we don’t have the technical ability to build it safely.4 min read](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down)[
### Are you saying we need provably safe AI?No.2 min read](/12/are-you-saying-we-need-provably-safe-ai)[
### What does it do to your daily life to believe all of this?It dramatically affects our priorities.2 min read](/12/what-does-it-do-to-your-daily-life-to-believe-all-of-this)[
### Are you saying we should panic?We’re saying government officials should take the problem seriously.3 min read](/12/are-you-saying-we-should-panic)[
### Isn’t this all just fear-mongering by AI leaders to increase status and raise investment?No.4 min read](/12/isnt-this-all-just-fear-mongering-by-ai-leaders-to-increase-status-and-raise-investment)[
### But experts don’t all agree about the risks!Lack of expert consensus is a sign of an immature technical field.4 min read](/12/but-experts-dont-all-agree-about-the-risks)[
### But what about the benefits of smarter-than-human AI?Rushing ahead destroys those benefits.4 min read](/12/but-what-about-the-benefits-of-smarter-than-human-ai)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### The Lemoine Effect](/12/the-lemoine-effect)[
### Workable Plans Will Involve Telling AI Companies “No”](/12/workable-plans-will-involve-telling-ai-companies-no)[
### Making Sense of the Death Race](/12/making-sense-of-the-death-race)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
