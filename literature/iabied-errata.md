---
title: "If Anyone Builds It, Everyone Dies - Errata"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/errata"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Corrections and updates to the book and online resources"
---

Resources for Errata: | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/errata)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
# Errata

Errata for the book:
- Error (in the first print, U.S. and U.K. editions): On page 137, we said that @Truth_Terminal came out in 2023. It came out in 2024.
- Error (in the first print, U.S. and U.K. editions): On page 213, we said that “[t]he entire technological revolution that led to ChatGPT and other popular LLMs was kicked off by a 2018 paper introducing a clever new arrangement of arithmetic inside a GPU, the “transformer” algorithm, […]” The paper was actually a 2017 paper, titled “[Attention Is All You Need](https://arxiv.org/abs/1706.03762),” which led to the creation of the first GPT (GPT-1) in 2018.
- Error (in the first print, U.S. and U.K. editions): On page 213, we said that “[p]retty much every year, scientists come out with a newer, cleverer, more efficient set of AI algorithms that lets them more cheaply train a new AI model as powerful as last year’s most powerful model — often using literally 10 percent or 1 percent as much computing power.” The word “often” should be “sometimes.” In recent years, algorithmic progress has [tended](https://arxiv.org/pdf/2403.05812) towards it taking 33 percent as much computing power to train a model as powerful as one year before. The 10 percent and 1 percent cases are cases where new algorithms allow AIs to behave in qualitatively new ways (like the dawn of LLMs), which are harder to measure, and do not tend to come yearly.
- Mischaracterization: On pages 199–200 (hardcover, U.S. and U.K. editions), we relay an account of passengers on the *Titanic *jesting at those who boarded the lifeboats. Some historians [have argued](https://99percentinvisible.org/episode/632-the-titanics-best-lifeboat/) that the choice to stay aboard the *Titanic *actually had merit: Lifeboats at that time were often more dangerous than a damaged vessel.

Errata for the online resources:
- Error (in the online resources, as of 2025-9-17): Our online list of some ways that AIs seem to have "[inhuman psychologies](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient#ais-appear-to-be-psychologically-alien)" was subpar — 1a3orn made the apt [point](https://x.com/1a3orn/status/1968091242392367375) on Twitter that a fair number of the examples we gave have parallels in human psychology. As of 2025-9-17, we've deleted this list. The section also mistakenly said that the AI behind [@Truth_Terminal](https://www.lesswrong.com/posts/buiTYy75KJDhckDgq/truth-terminal-a-reconstruction-of-events) was a fine-tuned version of Claude Opus, when it's actually a fine-tuned version of a Llama 3.1 70B model, created by Andy Ayrey.[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
