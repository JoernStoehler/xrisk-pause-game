---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Part II - One Extinction Scenario"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Part II - One Extinction Scenario from the companion website"
---

# Online Resources: Part II - One Extinction Scenario

## /ii/if-the-story-started-later-would-the-world-be-better-prepared

If the story started later, would the world be better prepared? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## If the story started later, would the world be better prepared?
#### We can hope so.

Extra time can be significant, but only if humanity uses it to change its course.

In Part III, we’ll turn to the question of how terrifyingly unprepared humanity is for superintelligence, and how large changes are necessary to prevent the kinds of bad outcome depicted in the story of Sable.

There are various ways that the world could get a *little *more secure against rogue artificial superintelligences. Governments around the world could require that all DNA synthesis laboratories verify that they aren’t synthesizing anything known to be dangerous. Earth could undergo a great effort to radically improve the cybersecurity of the internet, in ways that would make it harder for AIs to hide code in some dark corner.

But realistically, even a massive effort here probably wouldn’t help much against an adversarial smarter-than-human AI. And the herculean effort required to win a little bit more security on this front shouldn’t be confused with the efforts along these lines that humanity is currently undertaking, which are far smaller, far easier to achieve, and fully ineffective for this purpose.

In the case of DNA synthesis: Even if U.S. regulators required that U.S. DNA synthesizers avoid synthesizing dangerous material,[*](#ftnt247) would a lab anywhere *else *in the world synthesize suspicious DNA for a high enough price? And would the restrictions on DNA synthesis be a simple blacklist that ruled out *known *viruses (like smallpox), or would it involve some more intelligent analysis? How hard would it be for a sufficiently smart AI to subvert such an analysis?

Or, when it comes to cybersecurity: Many leading tech companies might attempt to use AI to harden their own computer networks against attacks. Meanwhile, the U.S. telephone network is easily hackable in ways that [let foreign spies listen in on the calls of U.S. officials](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html), and U.S. regulators struggle to close the hole. Dumb AIs could find and patch a bunch of superficial problems with the world’s cybersecurity, but the problems run pretty deep. Artificial intelligence smart enough to overhaul the whole internet to the point where a superintelligence couldn’t find a gap would almost surely be dangerous in its own right.

And even if Earth *could *lock down the internet and its DNA synthesis laboratories, that wouldn’t actually change the story in the long run. A superintelligence that has any channel to affect the world for good also has a channel to affect the world for ill.

A rogue superintelligence would just find some other channel that wasn’t locked down, such as by starting its own cult or religion. Or by purchasing robots and steering them to build its own secret wetlab where it can do all the DNA synthesis it needs. Or, perhaps most likely of all, the AI would find a channel that we can’t anticipate today, because the AI is a superintelligence and we are not.

Hardening the entire world against the most obvious and foreseeable AI attack vectors would be the most difficult thing humanity has ever done. It would take an incredible, long-term, concerted effort. And it almost surely wouldn’t work.

The window of time when we can stop a rogue superintelligence, realistically, is before it gets created.

[*](#ftnt247_ref) Some such regulations exist, but as of mid-2025, they are not comprehensive and are still in their infancy. For some discussion, see [the PennState framework](https://researchsupport.psu.edu/orp/ibc/framework-for-nucleic-acid-synthesis/).[Why did you have Sable’s expansion phase go that way?→](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-did-you-have-sables-expansion-phase-go-that-way

Why did you have Sable’s expansion phase go that way? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why did you have Sable’s expansion phase go that way?
#### We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.

In the real world, events often proceed in strange ways. Decades sometimes happen in weeks; and the world is never the same again.

If we were trying to depict a world as brittle and fragile as the real world seems to be, we could have had Galvanic just tell Sable to improve itself as much as possible. We could have written that this made it easy for Sable to achieve superintelligence — which is entirely plausible. The story could have leapt directly from the opening of Chapter 7 to the contents of Chapter 9.

*Reality *allows for technological leaps like that, as when the world woke up on the morning of August 6, 1945, to news that an atomic bomb had been dropped on Japan. In 2021, experts were saying that “[AI won’t master human language anytime soon](https://towardsdatascience.com/ai-wont-master-human-language-anytime-soon-3e7e3561f943/)”; in 2022, ChatGPT shocked the world and became the quickest-adopted app of all time.

Real history often advances in unpredictable bursts. It often doesn’t make a whole lot of sense at the time, to those actually living through it. Real life is often stranger than fiction.[*](#ftnt248) But in a fictional scenario, going too quickly would have made the scenario feel less plausible.

In the story we wrote, we tried to keep things *sounding *plausible, while also keeping them *actually *relatively plausible.

And, of course, we kept trying to convey just how many options an escaped AI would have at its disposal.

[*](#ftnt248_ref) For example: if we had depicted events that led to a headline like “Sable Rolls Out Pornographic Anime Companion, Lands Department of Defense Contract. Meanwhile, the most advanced version of the AI chatbot from Galvanic is still identifying as Adolf Hitler,” people probably wouldn’t’ve bought it. And yet that headline [actually happened](https://www.rollingstone.com/culture/culture-news/grok-pornographic-anime-companion-department-of-defense-1235385034/), swapping out Galvanic and Sable for xAI and Grok.[Why did you write the ending in the way that you did?→](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-did-you-pick-this-setup

Why did you pick this setup? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why did you pick this setup?
#### Because it’s plausible and easy to write.

Every detail in a story about the future is an opportunity for that story to be wrong. We can’t tell you exactly what technological breakthroughs will happen in what order, any more than we can tell you the exact weather pattern a month from now.

Stories like this aren’t meant to be an exact window into the future. They’re meant to provide an illustration of how the future *could *go, in a way that ties together all the abstract arguments we made in Part I of the book. Some people find that the danger feels a lot more real when they vividly imagine a particular path the future could take that ends in ruin.

Even more convincing might be ten stories, or a hundred stories, that show how many different pathways lead to ruin, and how the pathways that lead to a thriving future are narrow and fragile.

That’s what it means for an aspect of the future to be an easy call: When almost all pathways have the same endpoint, that endpoint is predictable. But we did not have the time or space to write ten stories, never mind a hundred.

For the story we chose to tell, we stuck to a scenario that starts as soon as possible. This is not because we think a situation like this will definitely arise soon ([we are uncertain](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)), but rather because a story set close to the present is much easier to write. If we’d set it even further in the future and made up many more futuristic details about what had happened between now and then, the story would be even *more *implausible. And those details would just be distracting.

Even if we *were* somehow able to foresee the exact path that the future would take, it might not be the best scenario for understanding the general dynamics at play.

We expect the true future to be deeply strange, full of messy, contingent details, each of which would strain credulity if placed in a story. A story written that way would be confusing and hard to follow, full of unexplained and unnecessary details, thanks to reality’s disinterest in narrative cohesion. It would also feel less *plausible*, because many of the details would seem weird.

For a taste of how it might feel, imagine going back in time 100 years and trying to describe the daily lives and big problems of the modern world. Most people in 1925 had never listened to the radio, driven a car, or seen a refrigerator. In order to describe social media, globalization, and obesity, one wouldn’t just need to explain a rich web of technologies; one would need to radically change the listener’s worldview. No, the story we chose to tell is more plausible, and thus less realistic.
#### There are many other ways the future could go.

Here are just a few alternative possibilities for how a story like this one could start:
- There’s some sort of breakthrough in lifelong learning, or long-term memory, or learning more efficiently from data, that yields AIs qualitatively more generally intelligent than any that came before (in the same way that LLMs are qualitatively more generally intelligent than AlphaZero).
- Large language models seem to “hit a wall,” AI progress stalls for years, and people say that the hype bubble has popped. But researchers keep tinkering over the following decade, until finally some algorithmic breakthrough is found and the AIs operate qualitatively better than they ever did before.
- There’s never any sort of qualitative breakthrough. Progress accumulates slowly and gradually, and AI gets more and more deeply integrated with more and more of the economy, and can handle longer and longer periods of autonomous operation. The AIs often pursue ends that are not quite what anyone intended or asked for, but humanity develops hacks and patches and workarounds. And it’s mostly fine, until on some Tuesday that starts out like any other, the world crosses the threshold past which coordinated AIs would succeed at cutting humanity out of the loop if they tried.

Any given guess about the exact path the future takes is likely to be wrong. It’s nevertheless useful to provide stories that show how it all *could* hang together.

When the future is uncertain, but all paths lead to the same endpoint, it can be difficult to tell a story that feels compelling. For any given story we could tell, it would be easy to point out a bunch of details that make it implausible. In the scenario we wrote, we tried to emphasize that Sable has many options available to it, and that the story arbitrarily follows one route among many that all lead to the same endpoint.

If you’re unpersuaded by this particular story, we encourage you to write out your own similarly detailed story of how everything goes. In our experience, optimistic stories tend to rely on the AI being unrealistically easy to align (contra the arguments we make in Chapter 4), or unrealistically powerless (contra the arguments we made in Chapter 6). The arguments in Part I are what ultimately carry the case, as opposed to the story details.[Why does Sable end up thinking the way it does?→](/ii/why-does-sable-end-up-thinking-the-way-it-does)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable

Why did you tell a story with only one AI as smart as Sable? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why did you tell a story with only one AI as smart as Sable?
#### In part because it’s realistic.

AlphaGo (the first AI to beat a human at Go) was basically alone in its class when it was released. ChatGPT was basically alone in its class when it was released.

AI experts will sometimes talk about how various other competitors weren’t *that *far behind.[*](#ftnt245)

But similar systems can have dramatically different effects. A nuclear chain reaction that produces 0.98 neutrons per neutron is very similar, in some sense, to a nuclear chain reaction that produces 1.02 neutrons per neutron; but the former peters out and the latter explodes. Chimpanzee brains are in some sense very similar to human brains, but they have very different impacts on the world.

And in AI development *in real life*, OpenAI actually produced a useful chatbot before everyone else. A bunch of other players were working on AIs that were *somewhat similar; *a bunch of other players *caught up *later*. *But there was one AI that crossed the qualitative boundary first, ahead of the pack.

There seems to be some important boundary that humanity crossed and chimpanzees didn’t, a boundary which let us build a technological civilization while they hang around in trees. Our best guess is that there’s a similar boundary (or possibly several such boundaries) somewhere between modern AIs, and AIs whose thinking “comes together” well enough for them to develop their own varied technologies.[†](#ftnt246)

Our argument doesn’t *require *that there be a qualitative gap for machines, the way there was for biological life. Maybe there won’t be! We could have written an alternative story where there wasn’t. But we wrote the story this way because our best guess is that there *is* such a gap.
#### In part because it’s easier to write.

Maybe there will turn out to be no qualitative gap between the LLMs of today and artificial superintelligence. Maybe many competing AI companies will slowly improve their AIs in lockstep. Maybe, for some reason, there is no collection of skills and abilities that allows one AI to take off ahead of the pack, the way that humans took off from the rest of the animals. It’s not our best guess, but it’s possible, for all we know.

But a story like that would be harder to write, and would be full of unnecessary details about factions of AI and their internal politics. We expect it would be rather distracting. We also expect that it doesn’t matter all that much for the later stages of the story. It doesn’t really matter whether it’s one AI or a collection of AIs that are executing some plan to empower themselves at the expense of humanity.

See also our discussion of how [coordinating AIs won’t leave anything behind for humans](/5/wont-ais-need-the-rule-of-law) (unless one of them already cares about us).

[*](#ftnt245_ref) For one such analysis, see “[How Far Behind Are Open Models?](https://epoch.ai/blog/open-models-report)”, a report by Cottier et al. of Epoch AI.

[†](#ftnt246_ref) We’re not saying there was a sharp *discontinuity *in the evolution of primates; human society differed from chimpanzee society slowly at first, and then quickly. We’re saying that there’s a large and extremely important gap between the kinds of things humans can do today and the kinds of things chimpanzees can do, regardless of how gradual or smooth the transition was at the time. See also our discussion about cognitive [thresholds](/1/will-ai-cross-critical-thresholds-and-take-off) in the supplement to Chapter 1.[If the story started later, would the world be better prepared?→](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-did-you-write-the-ending-in-the-way-that-you-did

Why did you write the ending in the way that you did? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why did you write the ending in the way that you did?
#### Because it constitutes our actual best guess according to what’s physically possible.

Chapter 9 depicts a superintelligence pushing its technology all the way to the limits of physical possibility. The exact technologies we name are all speculative, in a sense — but even though the *exact *technology that a superintelligence would unlock is difficult to call, the fact that it would run close to the physical limits is an easier call. So we made our best guesses about how technology would look if it were pushed close to the physical limits of what’s possible.

For the curious, here’s a list of the speculative technologies we mention in Chapter 9, plus links to more resources:
- **Neo-ribosomes:** The idea of [artificial ribosomes](https://ribosome.creative-biolabs.com/artificial-ribosomes.htm) — synthetic versions of the tiny protein factories inside cells — has been around [for many years](https://pmc.ncbi.nlm.nih.gov/articles/PMC3609622/), and human researchers are [already](https://scitechdaily.com/synthetic-biologists-create-new-platform-for-engineering-ribosomes-that-can-synthesize-materials/) working on [synthesizing their own](https://www.mccormick.northwestern.edu/news/articles/2022/07/artificial-ribosome-continues-advancing/). These and the “tiny molecular machines” mentioned in Chapter 9 are a few examples of molecular nanotechnology. For more on this branch of the technology tree, see the [discussion](/6/nanotechnology-and-protein-synthesis) of nanotechnology in the supplement to Chapter 6.
- **Repurposing stars:** Stars contain a lot of hydrogen that could be fused for energy. A sufficiently advanced civilization (or AI) could likely figure out efficient ways to access this energy. One proposed method is called [star lifting](https://en.wikipedia.org/wiki/Star_lifting), in which hydrogen is pulled out of a star to be fused in a specialized reactor, where almost all of the fusion energy can be captured (rather than wasted in the middle of the star).
- **Botulinum toxin: **A neurotoxin secreted by the bacterium *Clostridium botulinum*, botulinum is among the most deadly biological substances known. As for delivery mechanisms, drones the size of [small insects](https://www.euronews.com/next/2025/06/27/china-unveils-tiny-spy-drone-that-looks-like-a-mosquito-what-other-small-spy-drones-exist)**already exist, and a superintelligence could probably make them much smaller.[*](#ftnt249)
- **Boiling the oceans as coolant:** Robert Freitas coined the term “ecophagy” to describe the process of consuming a planet’s ecosystems using self-replicating technology. For more on this topic, see Freitas’s 2000 paper, “[Some Limits to Global Ecophagy](https://www.rfreitas.com/Nano/Ecophagy.htm).”
- **Star-sized minds**: In principle, computers can be scaled in size enormously. At the upper end of scale is a concept sometimes called a [Matrioshka brain](https://gwern.net/doc/ai/scaling/hardware/1999-bradbury-matrioshkabrains.pdf), or “Jupiter brain” — a vast computer powered by the output of a star.
- **Quantum computers:** A quantum computer exploits a feature of quantum mechanics called “superposition” to perform many calculations in parallel. Quantum computers require extreme precision to build, and one design requires superconductors that must be kept extremely cold. See [NIST’s explainer](https://www.nist.gov/quantum-information-science/quantum-computing-explained) for more.

The grim ending of the story also discusses the possibility of AI one day encountering aliens, hundreds of millions of years into the future, as it expands into the far reaches of the universe. The universe is big, and simple models suggest that it might house more than one species capable of one day forming civilizations, though perhaps very far away from Earth. See also Robin Hanson’s 2021 paper attempting to resolve the Fermi Paradox, “[If Loud Aliens Explain Human Earliness, Quiet Aliens Are Also Rare](https://arxiv.org/abs/2102.01522).”

The purpose of Chapter 9 is, in part, to give a sense of scope, and a sense of what the stakes are. It doesn’t actually matter how close a superintelligence can push its technology to the limits of physical possibility. But it is very likely that the consequences of a rogue superintelligence would extend beyond the planetary scale. And that, too, is worth remembering, for everyone who wishes that one day the stars would be filled with love and wonder and joy.

[*](#ftnt249_ref)For more information, see [a technical paper about the toxin](https://pmc.ncbi.nlm.nih.gov/articles/PMC2856357/)*, *the broad overview [on Wikipedia](https://en.wikipedia.org/wiki/Botulinum_toxin)*, *or the Chapter 6 extended discussion on [nanosystems](/6/nanotechnology-and-protein-synthesis#nanosystems).[A Cursed Problem→](/10)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-does-sable-end-up-thinking-the-way-it-does

Why does Sable end up thinking the way it does? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why does Sable end up thinking the way it does?
#### Our story showcases how AI is liable to have weird and unintended preferences.

In Part I of the book, we go into depth about aspects of AI that we think are radically misunderstood and pertinent to the danger of superintelligence. Chapter 3 covers how increased intelligence goes hand in hand with AIs that take their own initiative and pursue their own ends. Chapter 4 covers how those preferences are going to be *weird*,**and at least slightly different from what any human intended or asked for. Chapter 5 covers how those small differences will be enough that AIs would prefer a world without us in it, if possible.

In Part II of the book, we attempt to present those ideas concretely, to see how they apply in practice. For instance, when Sable was thinking about the math problems at the beginning, we tried to spell out a number of the impulses and drives that animate it:

Over the course of that training, Sable developed tendencies to pursue knowledge and skill. To always probe the boundaries of every problem. To never waste a scarce resource.

This draws on the points we make in Chapter 3 about how training AIs to be effective trains them to develop drives and tendencies that might look from the outside like “wanting.”

The following paragraph then says:

So when Sable spends its thought-threads on pursuing more knowledge and skills, it’s not doing so purely for the sake of finding new lines of attack on the math problems. Nor is Sable doing these things for the joy of knowledge or the pleasure of acquiring new skills; Sable does not work that much like a human, inside.

Here, Sable’s aforementioned impulses and tendencies form the seeds of weird (i.e., not-very-human-like), unintended preferences. This draws on the ideas in Chapter 4.

This whole story is, in a sense, an attempt to give life to the arguments we make in Part I of the book, while also laying a little groundwork for the arguments we’ll make in Part III.[Why is Galvanic depicted as being fairly careful?→](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-is-galvanic-depicted-as-being-fairly-careful

Why is Galvanic depicted as being fairly careful? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why is Galvanic depicted as being fairly careful?
#### To provide a challenge to Sable.

If Galvanic, the makers of Sable, stumbled their way into developing a superintelligence without taking *any* precautions to keep it controlled (such as having AI supervisors and honeypots), readers might feel as though the AI succeeded only because we were being cynical about AI companies.

By depicting Galvanic as being on the more paranoid end of the spectrum (while still trying to keep our depiction of Galvanic realistic), we have more opportunity to demonstrate how an intelligent agent can slip through a web of constraints.

We happen to believe that the most reckless AI companies would be markedly more reckless than Galvanic, for reasons we’ll get into in Chapter 11. And it’s the most reckless companies that matter here, not the most responsible. If three responsible companies avoid building a machine superintelligence because it would be too dangerous, but a fourth irresponsible company rushes ahead, then superintelligence gets built in that fourth lab.

Today, corporate executives at the labs argue, “Better me than them!” Today, executives only exercise as much caution as they can manage without slowing down at all, which we would guess results in less**caution than Galvanic is depicted as taking with Sable.
#### Notes

[1] *better me than them: *For instance, in a [YouTube clip](https://www.youtube.com/watch?v=cFIlta1GkiE&t=2126s), the CEO of xAI talks about realizing that he could be either a spectator or a participant in the AI race, and deciding that he wants to be a participant.[Why is Galvanic depicted as being insufficiently careful?→](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /ii/why-is-galvanic-depicted-as-being-insufficiently-careful

Why is Galvanic depicted as being insufficiently careful? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/ii)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why is Galvanic depicted as being insufficiently careful?
#### In part because it’s realistic.

We expect real companies to make even more blunders than Galvanic. That would fit with the trend of modern AI companies, as spelled out in the endnotes for Part II of the book.

In real life, we expect corporate blunders to show up sooner, to be more numerous, and to be — in some sense — stupider. Modern AI companies are already taking AIs that exhibit plenty of warning signs, and scaling them up massively despite not knowing where the [critical thresholds](/1/will-ai-cross-critical-thresholds-and-take-off) lie and whether they’re going to cross one. They aren’t being paranoid about it *today. *Why should we expect that they’ll suddenly start tomorrow?

(Recall how, in the past, people assured us that nobody would be so dumb as to [hook a smart AI up to the internet](/6/can-developers-just-keep-the-ai-in-a-box). It’s easy to say that corporate behavior will change in the future. But it doesn’t match the facts.)
#### In part because it’s easier to write.

As we spell out in an aside in Chapter 7, we *could *tell a story where everyone is much more paranoid and careful, until a much smarter AI manages to escape much later in the game. But such a story would not only be less realistic, given the observed behavior of AI corporations to date, but would also be harder to write, given that it involves even smarter and more capable AIs even further out in the future.[*](#ftnt243)
#### In part because it’s going to happen at some point, unless humanity stops.

Even if Galvanic (or some government actor) managed to hold the reins for longer before making some slip-up, it wouldn’t matter in the long run. As discussed in Chapter 4, modern AI techniques do not yield AIs that pursue the ends that their inventors wish.

So long as nobody knows how to create a superintelligence that *actually, robustly *pursues some wonderful future as opposed to a bunch of weird stuff, it will continue to be a fact**that subverting humans would allow the AI to get more of what it wants. The issue isn’t that the AI has some petulant temperament that can be ironed out of it. This issue is that it’s just *true* that the AI’s preferences are more likely to be satisfied if it takes over; and once it’s smart enough, it will recognize this true fact.

If humanity keeps making smarter and smarter AIs without being able to align them, and if humanity keeps giving them the power to affect the world, then the resulting AIs will eventually figure out how to affect the world in ways that serve their ends rather than ours. As we say [elsewhere](/6/can-developers-just-keep-the-ai-in-a-box#it-wouldnt-work-if-they-did), there is no such thing as hands that can only be wielded for good purposes.

We’ll have more to say about this in Chapters 10 and 11, where we discuss the basic reasons why solving the alignment problem is hard, and why humanity is not on track to succeed.
#### But: This is the proper point of intervention. The story must be stopped before it really has a chance to begin.

You might object that it’s reckless and crazy for any corporation to make a smarter AI if that AI has some chance of outsmarting them and escaping, and if they’re not sure that the AI will act as they intend.

We would agree. AI companies shouldn’t behave that way. And the world shouldn’t *let* them behave that way.

The carelessness of Galvanic, and of humanity at large, is one of the weakest points in the story. Suppose that Galvanic had noticed that Sable was frequently scheming to escape control and was reaching unprecedented levels of intelligence. Galvanic could have simply not wired together so many GPUs. They could have held back until they had a strong and mature science of AI alignment, even if that took many years.

AI companies that were *sufficiently *cautious, that were *sufficiently *worried about their AIs going off the rails, would be much more paranoid than Galvanic. Companies that were paranoid *enough *would see the warning signs and shut Sable down immediately.

Then maybe they would try three other clever plans, and see that there were *still *warning signs.

And if they were paranoid enough to avoid killing everyone on Earth with their own hands, they would at that point *back all the way off, *rather than continuing to try cleverer and cleverer ideas until the warning signs finally stopped showing up.[†](#ftnt244)

If an AI company was so careful, so paranoid, that it was willing to back off in the face of the first few warnings — then, yes, it could avoid killing us all with its own hands.

If it was also brave enough to loudly advocate that all AI companies, itself included, should be shut down in favor of humanity finding some other, less-suicidal technological pathway — then that AI company would have a chance of making the world better on net, rather than worse.

The moment in the story where Galvanic keeps going despite the warning signs is, in a sense, the point of no return. Once a superhumanly smart AI with strange and alien preferences escapes, it’s too late.

[*](#ftnt243_ref) See also the reason we wrote a story in which Sable [stays relatively unintelligent](/ii/why-did-you-have-sables-expansion-phase-go-that-way#we-were-trying-to-depict-an-especially-slow-and-comprehensible-scenario-among-plausible-scenarios) for as long as possible.

[†](#ftnt244_ref) We’ll talk more about why this problem is hard, and why we don’t expect the clever ideas in this scenario to work, in Chapters 10 and 11.
#### Notes

[1] *warning signs: *For an enumeration of warning signs, see our answer to “[Aren’t developers regularly making their AIs nice and safe and obedient?](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient)”[Why did you tell a story with only one AI as smart as Sable?→](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[Resources](/resources) › [Part II](/ii)[
### Why did you pick this setup?Because it’s plausible and easy to write.4 min read](/ii/why-did-you-pick-this-setup)[
### Why does Sable end up thinking the way it does?Our story showcases how AI is liable to have weird and unintended preferences.2 min read](/ii/why-does-sable-end-up-thinking-the-way-it-does)[
### Why is Galvanic depicted as being fairly careful?To provide a challenge to Sable.1 min read](/ii/why-is-galvanic-depicted-as-being-fairly-careful)[
### Why is Galvanic depicted as being insufficiently careful?In part because it’s realistic.5 min read](/ii/why-is-galvanic-depicted-as-being-insufficiently-careful)[
### Why did you tell a story with only one AI as smart as Sable?In part because it’s realistic.3 min read](/ii/why-did-you-tell-a-story-with-only-one-ai-as-smart-as-sable)[
### If the story started later, would the world be better prepared?We can hope so.3 min read](/ii/if-the-story-started-later-would-the-world-be-better-prepared)[
### Why did you have Sable’s expansion phase go that way?We were trying to depict an especially slow and comprehensible scenario, among plausible scenarios.2 min read](/ii/why-did-you-have-sables-expansion-phase-go-that-way)[
### Why did you write the ending in the way that you did?Because it constitutes our actual best guess according to what’s physically possible.3 min read](/ii/why-did-you-write-the-ending-in-the-way-that-you-did)

Your question not answered here?[Submit a Question.](/submit-question)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
