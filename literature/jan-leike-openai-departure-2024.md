---
title: "Jan Leike's Departure from OpenAI and Safety Concerns"
author: "Jan Leike and various sources"
year: 2024
source_url: "https://x.com/janleike/status/1791498174659715494"
source_format: html
downloaded: 2026-02-10
encrypted: false
notes: "Jan Leike's May 2024 departure from OpenAI and public statement about safety concerns. Key whistleblower moment highlighting tensions between AI safety and product development at frontier labs."
---

# Jan Leike's Departure from OpenAI and Safety Concerns

## Overview

On May 17, 2024, Jan Leike, OpenAI's head of alignment and superalignment team leader, resigned and published a series of posts on X (formerly Twitter) detailing his concerns about OpenAI's approach to AI safety. His departure, coming just days after co-founder Ilya Sutskever's resignation, marked a significant moment in AI safety discourse and raised questions about prioritization of safety at leading AI labs.

## Background: Who is Jan Leike?

**Professional Background**:
- Prominent AI safety researcher
- Joined OpenAI in 2021
- Co-led the Superalignment team (announced July 2023)
- Head of Alignment at OpenAI
- Executive at OpenAI

**Superalignment Team Mission**:
The Superalignment team was created with a bold goal: solve the core technical challenges of superintelligence alignment in four years. OpenAI committed 20% of its compute resources to this effort, making it one of the most ambitious safety research programs in the industry.

## The Departure: Timeline

### May 14, 2024
Ilya Sutskever, OpenAI co-founder and Leike's co-lead on the Superalignment team, announces his resignation from OpenAI.

### May 16, 2024
Jan Leike's last day at OpenAI as head of alignment, superalignment lead, and executive.

### May 17, 2024
Leike publishes a thread on X detailing his reasons for leaving and concerns about OpenAI's direction.

Shortly after his departure announcement, OpenAI dissolves the Superalignment team, distributing its members to other research groups within the company.

## Leike's Public Statement

### The Announcement

> "Yesterday was my last day as head of alignment, superalignment lead, and executive @OpenAI."

### Core Disagreements

> "I joined because I thought OpenAI would be the best place in the world to do this research. However, I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point."

### Safety Culture Concerns

> "Over the past years, safety culture and processes have taken a backseat to shiny products."

This statement became one of the most widely quoted critiques of AI safety prioritization at frontier labs.

### Resource Constraints

> "Over the past few months my team has been sailing against the wind. Sometimes we were struggling for compute and it was getting harder and harder to get this crucial research done."

Despite OpenAI's public commitment to dedicate 20% of compute to superalignment work, Leike indicated the team faced resource constraints in practice.

### Not on the Right Trajectory

> "I believe much more of our bandwidth should be spent getting ready for the next generations of models, on security, monitoring, preparedness, safety, adversarial robustness, (super)alignment, confidentiality, societal impact, and related topics. These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there."

### The Stakes

> "Building smarter-than-human machines is an inherently dangerous endeavor. OpenAI is shouldering an enormous responsibility on behalf of all of humanity."

Leike emphasized the gravity of what OpenAI is attempting and the corresponding responsibility that comes with it.

### Hope for the Future

Despite his concerns, Leike ended on a somewhat optimistic note:

> "OpenAI must become a safety-first AGI company. I am hopeful they will... But I'm concerned things are getting harder not easier. The fact that we are racing multiple companies makes this a lot more difficult."

## Context: The Broader Situation

### Ilya Sutskever's Departure

Ilya Sutskever's resignation announcement came days before Leike's and struck a very different tone. Sutskever's statement was brief and positive, expressing confidence in OpenAI's path to AGI that is "safe and beneficial."

The contrast between Sutskever's optimistic departure and Leike's critical one raised questions about internal dynamics and disagreements within the safety team.

### The Board Crisis (November 2023)

Leike's departure occurred roughly six months after OpenAI's November 2023 board crisis, in which:
- The board fired CEO Sam Altman, citing concerns about candor
- Ilya Sutskever was reportedly involved in the decision
- Massive employee backlash led to Altman's reinstatement
- Board was reconstituted
- Sutskever left the board

While Leike didn't explicitly reference this crisis, it formed important context for understanding tensions around safety and governance at OpenAI.

### Dissolution of the Superalignment Team

Following Leike's and Sutskever's departures, OpenAI dissolved the Superalignment team entirely. The company stated that members would be integrated into other research teams across the organization, but this move was widely seen as a significant downgrade in priority for long-term safety research.

## Broader Whistleblower Activities

### Open Letter from OpenAI Employees (June 2024)

Less than a month after Leike's departure, current and former OpenAI employees published an open letter raising broader concerns about the AI industry:

**Key Points**:
1. **Lack of Oversight**: "AI companies possess substantial non-public information about their systems' capabilities and limitations, the adequacy of their protective measures, and the risk levels of different kinds of harm. However, they currently have only weak obligations to share some of this information with governments, and none with civil society."

2. **Absence of Whistleblower Protections**: The letter described significant obstacles preventing employees from raising concerns, including:
   - Broad confidentiality agreements blocking employees from voicing concerns
   - Restrictions on warning regulators about AI risks
   - Fear of retaliation for speaking out
   - Non-disparagement agreements limiting public criticism

3. **Right to Warn**: The letter called for AI companies to commit to:
   - Not enforcing non-disparagement agreements that restrict criticism
   - Not retaliating against employees who raise risk-related concerns
   - Establishing anonymous channels for employees to raise concerns to company leadership and regulators
   - Supporting a culture of open criticism

**Signatories**: The letter was signed by several current and former OpenAI employees, as well as employees from other AI companies. The mix of current and former employees highlighted that these concerns extended beyond a few dissatisfied ex-employees.

### Accusations of Restricting Communication with Regulators

Whistleblowers accused OpenAI of preventing employees from warning regulators about possible AI risks. This raised concerns about regulatory capture and whether external oversight could be effective if employees were restricted from communicating with oversight bodies.

## Industry Reactions

### Other AI Safety Researchers

Many AI safety researchers expressed support for Leike's decision to speak out publicly, even while acknowledging the difficulty of such decisions.

Some viewed his departure as evidence of systemic issues in balancing commercial pressures with safety research at leading labs.

### Company Responses

**OpenAI Response**:
- CEO Sam Altman posted on X: "I'm very sad Jan is leaving. I really appreciate and respect his contributions... I also am a little hurt that he chose to leave this way... I should have been more communicative about my various worries. But I sure wish Jan & Ilya had given me more notice..."

- OpenAI leadership reaffirmed commitment to safety but didn't directly address specific resource allocation concerns

**Industry Peers**:
- Several other AI companies emphasized their own safety commitments
- Some researchers pointed to the incident as evidence for need for independent AI safety research outside of corporate structures

### Policy Community

Policymakers and AI governance experts cited Leike's departure as evidence that:
- Self-regulation by AI companies may be insufficient
- External oversight and regulation are necessary
- Whistleblower protections for AI safety concerns are needed
- Competition dynamics may undermine safety priorities

## Leike's Next Steps

### Joining Anthropic

On May 28, 2024, Leike announced he was joining Anthropic to continue alignment research:

> "I'm excited to join @AnthropicAI to continue my work on aligning smarter-than-human AI. The AGI race is heating up, and we need more people working on this critical challenge."

**Why Anthropic?**:
Anthropic, founded by former OpenAI researchers (including Dario and Daniela Amodei), has positioned itself as particularly focused on AI safety and alignment. The company's Responsible Scaling Policy and emphasis on constitutional AI aligned with Leike's concerns.

His move to Anthropic was seen by many as a vote of confidence in Anthropic's approach and a statement about comparative safety cultures at frontier labs.

## Broader Implications

### For OpenAI

Leike's departure and public critique:
- Damaged OpenAI's reputation as safety-focused
- Raised questions about resource allocation to safety research
- Highlighted tensions between product development and safety
- Added to perception of turmoil following board crisis
- Increased scrutiny of safety commitments vs. reality

### For AI Safety Field

The incident demonstrated:
- Safety research can be deprioritized even at ostensibly safety-conscious companies
- Competitive pressures create obstacles to adequate safety investment
- Need for independent safety research outside corporate control
- Importance of whistleblower protections
- Difficulty of maintaining safety focus during rapid scaling

### For AI Governance

Leike's statement contributed to:
- Growing calls for external AI oversight
- Increased interest in mandatory safety testing
- Push for whistleblower protections in AI
- Questions about industry self-regulation
- International coordination on AI safety standards

## Key Quotes and Their Significance

### "Safety culture and processes have taken a backseat to shiny products"
This phrase became emblematic of concerns about prioritization at AI labs. It suggested that despite public safety commitments, commercial pressures and product launches took priority.

### "We were struggling for compute"
Despite OpenAI's commitment of 20% of compute to superalignment, the team apparently faced practical difficulties accessing resources. This raised questions about the gap between public commitments and internal reality.

### "We aren't on a trajectory to get there"
Leike's assessment wasn't just that current safety measures were inadequate, but that the trajectory wasn't improving fast enough relative to capability growth.

### "The fact that we are racing multiple companies makes this a lot more difficult"
Acknowledgment that competitive dynamics between frontier labs create pressure that makes safety work harder. This supported arguments for coordination or regulation.

## Comparisons to Other Whistleblower Moments

### Frances Haugen (Facebook/Meta)
Like Haugen's revelations about Facebook, Leike's statement:
- Came from senior insider with direct knowledge
- Included specific concerns about prioritization
- Suggested gap between public statements and internal reality
- Led to increased regulatory scrutiny

**Key Difference**: Leike focused on future risks rather than current harms.

### Timnit Gebru (Google)
Gebru's 2020 departure from Google after conflicts over an AI ethics paper showed similar patterns:
- Conflicts over research autonomy
- Questions about company commitment to stated values
- Public airing of internal tensions
- Broader implications for tech ethics

**Key Difference**: Leike's concerns centered on existential risk rather than current fairness/bias issues.

### Yoshua Bengio's Shift
AI pioneer Yoshua Bengio's increasing focus on AI risks provided external validation:
- One of the field's founders expressing serious concerns
- Call for slowing down or pausing development
- Emphasis on research before deployment
- Leike's departure gave concrete example of Bengio's abstract concerns

## Open Questions

### Was This Inevitable?

**Arguments for Inevitability**:
- Commercial pressures inherent to corporate AI development
- Competition dynamics create race conditions
- Tension between capability and safety research timelines
- Difficulty of maintaining culture during rapid scaling

**Arguments Against**:
- Other companies (particularly Anthropic) seem to better balance priorities
- Specific organizational choices matter
- Board crisis aftermath created unique circumstances
- Better communication and management could have prevented this

### Impact on OpenAI

**Short-term**:
- Reputation damage in safety community
- Increased regulatory scrutiny
- Loss of key safety researchers
- Questions from stakeholders

**Long-term**:
- Will OpenAI reform safety practices?
- Can they rebuild trust?
- Impact on ability to recruit safety-focused researchers
- Relationship with regulators and policymakers

### Broader Industry Impact

- Will other companies improve safety prioritization to avoid similar incidents?
- Does this accelerate regulatory intervention?
- Will competition dynamics worsen or improve?
- Can coordination mechanisms prevent race-to-bottom on safety?

## Lessons for AI Development

### Resource Allocation
Public commitments to safety must be matched by actual resource allocation, including:
- Compute access
- Personnel
- Executive attention
- Decision-making authority

### Organizational Culture
Maintaining safety culture during rapid growth and competitive pressure requires:
- Clear prioritization from leadership
- Empowered safety teams
- Mechanisms for raising concerns
- Balance between capabilities and safety work

### Competitive Dynamics
The "AI race" creates pressure that can undermine safety. Potential responses:
- Industry coordination agreements
- Regulatory requirements as level-setting
- Independent oversight
- Whistleblower protections

### Transparency and Accountability
Gap between public commitments and internal reality damages trust. Companies should:
- Be honest about challenges and limitations
- Report on safety work with specificity
- Enable external verification
- Create channels for internal concerns

## Conclusion

Jan Leike's departure from OpenAI and subsequent public statement marked a significant moment in AI safety discourse. As a respected researcher and senior leader at one of the world's leading AI labs, his willingness to publicly criticize safety prioritization carried substantial weight.

His concerns—that safety was taking a backseat to product development, that resources weren't matching commitments, and that the trajectory wasn't adequate—resonated widely in the AI safety community and beyond. The dissolution of the Superalignment team shortly after his departure reinforced these concerns.

The incident highlighted fundamental tensions in AI development: between moving quickly and being careful, between competitive pressure and adequate safety measures, between public commitments and internal reality. These tensions aren't unique to OpenAI but reflect broader challenges in developing increasingly powerful AI systems responsibly.

Whether the incident will catalyze meaningful changes—at OpenAI, across the industry, or in governance approaches—remains to be seen. But it established a notable data point that even well-resourced, ostensibly safety-conscious companies face serious challenges in adequately prioritizing safety work alongside capability development.

For those concerned about AI existential risk, Leike's statement underscored that these aren't abstract future concerns but present-day organizational and prioritization challenges at the labs developing the most capable systems.

## Sources

- Jan Leike's X thread: https://x.com/janleike/status/1791498174659715494
- CNN: "More OpenAI drama: Exec quits over concerns about focus on profit over safety": https://www.cnn.com/2024/05/17/tech/openai-exec-exits-safety-concerns
- Fortune: "Top OpenAI researcher resigns, saying company prioritized 'shiny products' over AI safety": https://fortune.com/2024/05/17/openai-researcher-resigns-safety/
- Axios: "OpenAI's long-term safety team has disbanded": https://www.axios.com/2024/05/17/openai-superalignment-risk-ilya-sutskever
- CNBC: "Current and former OpenAI employees warn of AI's 'serious risks' and lack of oversight": https://www.cnbc.com/2024/06/04/openai-open-ai-risks-lack-of-oversight.html
- CIO: "Ex-OpenAI researcher Jan Leike joins Anthropic amid AI safety concerns": https://www.cio.com/article/2130038/ex-open-ai-researcher-jan-leike-joins-anthropic-amid-ai-safety-concerns.html
