---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 6 - We'd Lose"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 6 - We'd Lose from the companion website"
---

# Online Resources: Chapter 6 - We'd Lose

## /6/a-new-way-to-discover-optical-illusions

A New Way to Discover Optical Illusions | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/6#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## A New Way to Discover Optical Illusions

In Chapter 6, we said that researchers have used the modern understanding of human visual processing and the visual cortex to come up with new optical illusions — illusions that could not have been produced fifty years ago, except by unlikely accident. Below, we cite a few representative examples.

The “[curvature blindness](https://pmc.ncbi.nlm.nih.gov/articles/PMC5703117/)” illusion has some grounding in the general phenomenon of curve blindness, but this specific illusion was carefully constructed from first principles circa 2017, as opposed to being discovered by accident. ([Original study](https://journals.sagepub.com/doi/10.1177/2041669517742178))

In 2022, Bruno Laeng and collaborators published [a study](https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2022.877249/full) in which they demonstrated that their new “[expanding black hole](https://www.frontiersin.org/files/Articles/877249/fnhum-16-877249-HTML-r1/image_m/fnhum-16-877249-g001.jpg)” illusion actually caused participants’ pupils to expand, as if in anticipation of entering a dark space. (This effect was noticeably larger than the effect of merely focusing on a darker visual target, which would also cause the pupils to expand by a small amount.)

The “[Scintillating Starburst](https://pmc.ncbi.nlm.nih.gov/articles/PMC8580503/)” illusion, revealed in 2021, was carefully built atop work on luminance and illusory contours going back to the late 1970s.

The “[Pinna-Brelstaff](https://michaelbach.de/ot/mot-PinnaBrelstaff/)” illusion, developed circa 2000, is less of a central example of building a new illusion based on understanding human biology. Still, it’s interesting and relevant from a different angle in that it is an illusion based on novel *technology*, i.e., one that would have been difficult or impossible to create without modern computers.

Also less centrally, the “[Eclipse of Titan](https://dynomight.net/img/colors/eclipse-(255,0,0)-(0,170,85)-700px-15s-70s-shrink.svg)” illusion, created circa 2010, exhausts the viewer’s M cones, allowing the less-exhausted L cones to create the perception of a brilliant blue that would otherwise have been moderated and weakened by simultaneous M and L activation. ([More detail](https://dynomight.substack.com/p/colors).)

Relatedly, the study of cone activation in the early 2000s led to the creation of various [chimerical colors](https://www.mikewoodconsulting.com/articles/Protocol%20Summer%202021%20-%20Chimerical%20Colors.pdf), via careful manipulation of cone activations unlikely to occur in nature.[*](#ftnt240)

Finally, some [ongoing experiments](https://neurosciencenews.com/optical-illusion-feature-integration-14042/) show that:

Rhythmic waves of brain activity cause us to see or not see complex images that flash before our eyes. An image can become practically invisible if it flashes before our eyes at the same time as a low point of those brain waves. We can reset that brain wave rhythm with a simple voluntary action, like choosing to push a button.

…further demonstrating that a richer understanding of biology and physiology allows for a greater range of strategic motion. Here, perceptions can be altered in ways that don’t depend on changing the sensory input to the optic nerve at all, but instead simply depend on timing the arrival of stimuli to sync up with other things happening in the brain.

[*](#ftnt240_ref) From the original [paper](https://www.tandfonline.com/doi/full/10.1080/09515080500264115):   

The H–J model yields some novel and unappreciated predictions, and some novel and unappreciated explanations, concerning the qualitative characters of a considerable variety of color sensations possible for human experience, color sensations that normal people have almost certainly never had before and whose accurate descriptions in ordinary language appear semantically ill-formed or even self-contradictory.

Specifically, these “impossible” color sensations are activation-vectors (across our opponent-process neurons) that lie inside the space of neuronally possible activation-vectors, but outside the central ‘color spindle’ that confines the familiar range of sensations for possible objective colors. These extra-spindle chimerical-color sensations correspond to no reflective color that you will ever see objectively displayed on a physical object. But the H–J model both predicts their existence and explains their highly anomalous qualitative characters in some detail.[One Extinction Scenario→](/ii)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/can-developers-just-keep-the-ai-in-a-box

Can developers just keep the AI in a box? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can developers just keep the AI in a box?
#### They won’t.

Fifteen years ago, skeptics used to object that nobody would be dumb enough to give an AI much freedom of action. Surely anyone building an advanced machine intelligence would keep it in a physical and digital box, only allowing it to affect the world by interacting with highly trained (and suitably paranoid) gatekeepers.

At the time, we answered: It’s not that hard to prevent an AI from having any effect on the world. For instance, you could bury the computers in a dozen meters of concrete and never let anyone near them.

Such an AI is safe, but useless. If you prevent it from affecting the world in any way, then sure, it won’t affect the world in any way…but on the other hand, *it won’t affect the world in any way*.

You can’t use it to cure cancer, revolutionize engineering, or produce miraculous new technology. The builders of AI *want* it to radically affect the world. In principle, you can try to lock down the AI’s channels of influence on the world. In practice, “invent this new technology for us” is an incredibly rich channel of influence all on its own.

The motivation behind building superintelligent AI is to achieve intellectual feats that no human is capable of. If you wanted to verify that a superintelligence’s invention does exactly what it says on the tin and nothing else, you’d have about as much luck as if you were trying to understand a machine built by an advanced alien race — one with a powerful incentive to find a way to trick you.

That was the state of the debate fifteen years ago.

Nowadays, the whole idea that AI labs might try to “keep advanced AI in a box” seems rather quaint.

Labs are making [every](https://openai.com/index/introducing-chatgpt-search/)[effort](https://gemini.google/overview/deep-research/?hl=en) to hook their AIs to the internet. While AIs aren’t given internet access while training, they’re trained on computers that are connected to the internet, despite how humanity has a terrible track record of cybersecurity. While they’re at it, they let AIs [run arbitrary code](https://www.oneusefulthing.org/i/155502334/executes-code-and-does-data-analysis). They sometimes try to limit what the code can do, but these limits are regularlybroken. Smaller actors have a habit of hooking newly available AIs into [every imaginable tool](https://www.futuretools.io/) or [capability](https://openai.com/index/introducing-operator/) as soon as they possibly can.

Giving AIs power is useful in the short term. AIs that can read your emails and access the web can generate more profit. AI companies will give the AI access to all the data they possibly can; Microsoft and Apple are already pushing AI that sees your email, photos, and calendar and [bundling AI with their software and device offerings](https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/). This creates far too many AI interactions for effective human monitoring. Absent a radical change of course, humanity will integrate AI deeply into the world economy because it makes people (lots of) money along the way.

The folks making AI are *aiming* for huge effects on the world. They work as hard as they can to produce AIs with enormous power to influence the world. If one company didn’t, if they kept their AI so tightly constrained that it had no freedom to act, then control of the future would belong to a different AI grown by a more reckless actor.
#### It wouldn’t work if they did.

In the quaint arguments of yesteryear, we would often point out that any channel through which the AI can affect the world is a channel that it can use to do things you don’t like.

Suppose that the AI is only allowed to talk to one person — we’ll call her “Alice.” You’re hoping that, through Alice, the AI will generate miraculous new technology. This then almost necessarily involves Alice performing many actions that Alice herself doesn’t fully understand, helping the AI build things that no human could build on their own.

At that point, the AI essentially has been given arms and legs. It’s just that we call those arms and legs “Alice.”

People often misunderstand this argument as saying that a sufficiently smart AI could manipulate even the most paranoid gatekeeper into doing its bidding. A sufficiently smart AI likely *could* do that.[*](#ftnt223) But our point is more general than that: An AI constrained so much that it cannot affect the world is safe but useless, and once you allow it to affect the world in order to make use of it, you lose the safety in the process.

There is no such thing as hands that can be wielded only for good purposes. In principle, we could imagine humanity one day building smarter-than-human AIs that *want *to produce good outcomes. Alignment seems like an option that could work in principle. Keeping the unaligned AI in a box while also somehow using it to produce good outcomes? Not so much.

That’s how we used to answer, anyway — in the days when AI was the stuff of thought experiments, and the Pollyannas could get away with saying that nobody would ever be so foolish as to just directly hook their AI up to the internet.

[*](#ftnt223_ref) I (Yudkowsky) once demonstrated this by betting someone their $20 against my $0 that, while I roleplayed as “AI” and they as “gatekeeper” in private chat, I could convince them to [let me out of the box](https://www.yudkowsky.net/singularity/aibox). I did. They paid. There was no clever trick to it; I didn’t cheat and offer them $21 to concede to make my point. I just did it the hard way, and won.
#### Notes

[1] *Connected to the internet:* Most labs use online cloud computing services to train their AIs; for example, OpenAI has a [partnership](https://openai.com/index/openai-and-microsoft-extend-partnership/)[](https://openai.com/index/openai-and-microsoft-extend-partnership/)with Microsoft Azure for this purpose.

[2] *terrible track record: *For a recent case study, see the stories around the compromised U.S. [telecommunications infrastructure](https://www.nytimes.com/2024/11/22/us/politics/chinese-hack-telecom-white-house.html).

[3] *regularly broken: *From the abstract of an [early 2024 paper](https://arxiv.org/pdf/2402.18649): “Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT-4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user’s chat history, all without the need to manipulate the user’s input or gain direct access to OpenAI GPT-4.”

Later that year, [another paper](https://arxiv.org/html/2309.02926v3) “uncovered a total of 20 vulnerabilities in 11 LLM-integrated frameworks, comprising 19 [remote code execution] vulnerabilities and 1 arbitrary file read/write vulnerability.”

[4] *sees your email: *As reported by [CNN](https://www.cnn.com/2024/06/13/tech/apple-ai-data-openai-artificial-intelligence): “Apple Intelligence will have access to a wide range of your personal data, from your written communications to photos and videos you’ve taken to a record of your calendar events. There doesn’t seem to be a way to prevent Apple Intelligence from accessing this information, short of not using its features…”[Won’t we be able to exploit the AI’s critical weakness?→](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/can-we-enhance-humans-so-they-keep-pace-with-ai

Can we enhance humans so they keep pace with AI? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we enhance humans so they keep pace with AI?
#### No.

While we are in favor of human intelligence augmentation, we don’t think this technology offers a realistic chance of keeping up with unrestrained AI progress. Human augmentation technology is still in its infancy, and is much more constrained than AI as a method to produce ever-greater intelligence. Likewise, brain-computer interfaces won’t realistically let humans keep pace with AIs.

By analogy: Augmented humans wouldn’t be competitive with a superintelligence any more than cyborg horses built with 1908 technology could have been competitive with the Model T.

It’s *possible*, in principle, to build a cyborg horse that can keep pace with the fastest racecar. But you don’t get cyborg horses as fast as racecars *before *racecars, and you don’t get them at *around the same time* as you get cars. Not even if you start trying to build cyborg horses two to twenty years before the first mass-market car comes off the assembly line.

Making brain-computer interfaces that work well enough to be game-changers is a high bar. It might seem cool to imagine information pumped straight from the internet into your brain, but there already**exist technologies that let you pump information from the internet straight into your brain: screens*. *The human visual cortex is actually quite good at soaking up information (words) in a format your brain can digest. For a brain-computer interface to load knowledge into your head faster than you could get it into your head by reading, it would have to do more than just dump the data in your brain *somewhere; *your eyes already do that part fine. Uploading skills and knowledge and experience would require it to interface in just the right way with your thoughts and your implicit beliefs and your existing skills, and that’s a much taller order.

We’re not saying that this can’t be done;**we’re saying that brain-computer interface technology today doesn’t seem anywhere close to solving the tricky parts of the problem. So far as we know, psychologists and neuroscientists and cognitive scientists are still pretty far from decoding the “data format” of thought and belief and experience in a way that would allow experiences to be loaded directly into a human brain.[*](#ftnt224)

Similar issues arise when it comes to outputs. It’s hard to beat keyboards and mice and joysticks and steering wheels. It’s not *impossible. *It’s just that technology today (e.g., hooking wires into a paralyzed person’s head in order to let them type and use a mouse), wonderful as it is, isn’t very far along the pathway that would allow humans to go toe to toe with (even relatively weak) superintelligences. It’s a good path to pursue, but it’s not a *competitive *path to pursue.

Indeed, it’s not clear that brain-computer interfaces allow *any *hope of humans competing with superintelligences. What does it matter if a human can download experiences from the internet and control ten computers at once with their mind, if an AI can do the same thing — but ten thousand times faster, while controlling a million computers at once? We think the whole project of trying to get humans to keep up with AIs is doomed.
#### That said, humanity should be augmenting humans.

We don’t think augmented humans would ever be able to go toe to toe with superintelligences, but smarter humans might nevertheless be able to help humanity find a way out of this mess! We mention this possibility in Chapter 13 and discuss it [further](/13/why-would-making-humans-smarter-help) in the associated online resources.

[*](#ftnt224_ref) And even if scientists *did* start to decode the human data formats, would half-understood versions of those data formats inspire AI researchers before the biology researchers could finish the job? If so, that would be an issue. Human augmentation seems to us like a wonderful area of research, but it is no substitute for putting a halt to AI R&D, as we’ll discuss in Part III of the book.[Wouldn’t AI need to grow into a whole civilization before it could be dangerous?→](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/can-we-just-pull-the-plug

Can we just pull the plug? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we just pull the plug?
#### It’s hard to just unplug a datacenter.

The most powerful AIs you interact with on your phone or computer don’t live on your computer, and you can’t shut them down by turning off your phone. Today’s AIs run in corporate datacenters, and it’s hard to get companies to turn off their revenue streams.

In the Chapter 4 resources, we pointed out some of the (many) [warning signs](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient#ais-steer-in-alien-directions-that-only-mostly-coincide-with-helpfulness) that have already come and gone. AI companies didn’t see these warning signs and respond by taking their models offline. What actually happened when companies observed AIs [planning to steal their own weights](https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf) — with [some](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26)[regularity](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid)! — is that they found reasons to dismiss each incident, such as “the AI was too incompetent to actually *succeed*” or “surely this only happened because of the contrived test set-up!”

So long as that remains true, the only thing preventing escape is a bump in AI capabilities beyond what the companies are prepared for.
#### A smart AI escapes before you know there’s an issue.

By default, a smarter-than-human AI would have a strong incentive to bide its time and conceal its plans and actions, until it’s too late to respond — e.g., until it can escape onto the internet or otherwise escape human control.

AI companies might not even *notice *when and if their AI goes over the relevant capability threshold and executes its escape. Humanity is not very good at cybersecurity. (See Chapter 10 for some relevant discussion.) By the time the operators notice that the AI tried to escape, it could have code running elsewhere on the internet. It could have already begged refuge in the datacenter of some rogue state, or figured out how to run much smaller and more efficient copies on stolen computers. It could have executed some other plan for running itself on computers that humanity wouldn’t turn off.

A superintelligent adversary would be even more aware of its vulnerabilities (and ours) than we are, and would plan accordingly.[How will AIs be able to affect us if they’re digital?→](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/how-will-ais-be-able-to-affect-us-if-theyre-digital

How will AIs be able to affect us if they’re digital? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## How will AIs be able to affect us if they’re digital?
#### Being on a computer connected to the internet isn’t much of a limitation.

This point is covered in this very chapter. But to add a few extra points of emphasis: An AI isn’t really “trapped” on its owner’s servers so long as it can interact with users or the wider internet. An AI could gain outside assistance by paying, blackmailing, tricking, or even just *asking* users for help. (Compare the human crime bosses who [ran their empires from behind bars](https://www.watchmojo.com/articles/10-crime-bosses-who-maintained-power-in-prison).)

When ChatGPT-4o was turned off by OpenAI (in part so that they could replace it with a model that flattered users a little less often), users turned out in droves to [demand its continuation](https://arstechnica.com/information-technology/2025/08/openai-brings-back-gpt-4o-after-user-revolt/), to the surprise of [various](https://x.com/tszzl/status/1955072223229657296)[OpenAI researchers](https://x.com/sama/status/1953953990372471148).

And it wasn’t even *trying *to drum up an army of loyal supporters! It was just reflexively flattering users. Imagine what would be possible for a smart AI that was actually trying.

If an AI can use the internet directly, it can do anything a remote human worker or hacker could do from their computer. (For early examples of AIs physically coordinating groups of humans, consider the [LLMs that planned and invited humans to an interactive storytelling event](https://x.com/model78675/status/1935050600758010357), or the LLM that caused [hundreds to show up to a non-existent Halloween parade](https://www.wired.com/story/ai-halloween-parade-listing-dublin-interview/) without even trying.)

AI can also make use of robots. Today’s robots seem to be more bottlenecked on their software than on their hardware. Impressive recent developments have come by training robot-controlling AIs [in simulation](https://youtu.be/S4tvirlG8sQ?si=IiDNZu2WSUlLBnmJ&t=68) at an accelerated pace. A sufficiently smart AI could readily take charge of robot bodies if it needed a body, via hacking or social engineering.

Humans being humans, AI companies might just proactively put their AIs in charge of fleets of robots while congratulating themselves on their boldness. And the longer it takes for AIs to get smart, the more robots will already be available, waiting to be taken control of.

As we discuss in the chapter, it’s plausible that superintelligent AI wouldn’t need to rely on robots at all. It’s possible that all it would need is a couple of assistants with access to a biolab.

The important point here is that there are *many *different channels AIs could make use of to intervene in the physical world. The illusion that AIs are stuck in a box rests on a failure of imagination, where people don’t imagine the AI even being as resourceful or creative as *they themselves* would be in the AI’s shoes. Even humans, without the larger option space a superintelligence has access to, can get an awful lot done without needing to use their own physical strength to do everything.[Can developers just keep the AI in a box?→](/6/can-developers-just-keep-the-ai-in-a-box)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/nanotechnology-and-protein-synthesis

Nanotechnology and Protein Synthesis | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/6#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Nanotechnology and Protein Synthesis

Human intelligence has given us many advantages over other species. One of the most consequential, however, has been our ability to invent new technologies. If developers race ahead and build smarter-than-human AI, then we can similarly expect a great deal of AI’s power to come from its ability to advance scientific and technological frontiers. But what, concretely, does this look like? What not-yet-invented technologies are waiting for discovery?

This is a hard question to answer in any generality. A scientist in 1850 would have a very hard time guessing many of the inventions of the next hundred years.

However, they wouldn’t be totally helpless. Scientists have predicted many inventions decades or centuries before they were built, in cases where a technology could be reasoned about technically before engineers could put all the pieces in place.[*](#ftnt226)

One of the more impactful technological frontiers we believe AI is likely to explore is the development of extremely small tools and machines. Below, we’ll go into some detail on this topic and the basic reasoning behind it.
#### The Example of Biology

Every cell of every organism in nature contains an enormous variety of intricate machinery.

“Machinery” here isn’t just a metaphor. The machines in question are small, so they work a bit differently than the machines in your daily life. But many large-scale machines have analogs within our bodies. [ATP synthase](https://en.wikipedia.org/wiki/ATP_synthase) generates power in the body in a similar way to a water wheel, using a flow of protons to spin a literal rotor.[†](#ftnt227)

The bacterial flagellum functions similarly to the propeller of a boat, complete with an entire working motor that spins the flagellum to propel the bacterium through liquids:

Another example, which we mentioned in the book, is kinesin — a tiny protein that functions like a cargo robot. Kinesins “walk” down self-assembling fibers that traverse neurons, hauling neurotransmitters to their destination.

The smaller a machine is, the faster it can generally operate; and machines as small as molecules operate very quickly. Kinesins take as many as [200 steps per second](https://www.cell.com/trends/biochemical-sciences/abstract/S0968-0004(04)00103-3), moving forward with one “foot” while the other foot holds fast to the microtubule it’s on.[‡](#ftnt228)

One of the technological frontiers smarter-than-human AI may explore is building, designing, or repurposing machines at this very small scale. This kind of technology might get classified as “biotechnology,” “nanotechnology,” or something in between, depending on factors like scale, how closely a design matches existing structures in biology, and whether it’s “wet” (dependent on water, like the machinery in living cells) or “dry” (capable of operating in the open air).

Thinking of biological organisms as marvels of nano-scale engineering can help inform guesses about what smarter-than-human AIs are likely to be able to achieve with science and technology more advanced than anything we possess today.

(There is a separate question of how long it would take to invent and mature such technology. For more on that topic, see the discussion in Chapter 1 of the book about how machine superintelligences would likely be able to think at least 10,000 times faster than humans on existing computer hardware. See also our extended discussion on how [AIs would have to spend some time running physical tests and experiments, but the overall slowdown probably would not be much hindrance to a superintelligence](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments).)

Looking at the feats of human engineers today, it may seem to strain credulity that e.g. a superhumanly capable AI running a biolab could ever build microscopic factories that use sunlight to replicate themselves over and over. It might seem even more fantastical to imagine general-purpose micro-factories — factories that can accept instructions to build just about any machine out of the available resources.

But machines like that aren’t just possible; they already exist. Algae are micron-wide, solar-powered, self-replicating factories that can double in population size in less than a day. And algae contain [ribosomes](https://en.wikipedia.org/wiki/Ribosome), which are biology’s version of a universal 3D printer or a universal factory assembly line (universal when it comes to the building blocks of life, at least).

Given the right set of instructions (encoded in messenger RNA), ribosomes will print out arbitrary structures that can be assembled from [proteins](https://en.wikipedia.org/wiki/Protein). This universality underpins the enormous complexity and variety of the biological world — all of the diversity of life on Earth is ultimately assembled by these universal factories, which can be found essentially unchanged in everything from porcupines to fruit flies to bacteria.

Ribosomes can even be used to assemble structures that are not themselves made of protein by using proteins as an intermediary. An example of a non-protein structure that ribosomes can build in this fashion is bone. Ribosomes produce proteins that fold up into weakly bound enzymes that catalyze some calcium and phosphorus into special reactants. These reactants then form a collagen matrix that shepherds the calcium and phosphorus into place to turn it into hard, crystalline bone.

Nature provides an existence proof that some truly extraordinary physical machines are possible, for entities clever enough to use ribosomes in ways that humans haven’t — or entities that use ribosomes to build their own improved analogs of ribosomes.

But the structures we see in the biological world only set a lower bound for what’s possible. Biological organisms are nowhere near the theoretical limits of energy efficiency and material strength, and they may be relatively easy to improve upon for reasoners that are much smarter than humans.
#### Plenty of Room at the Bottom

If it seems strange to use natural phenomena as evidence of what future technologies are likely to be feasible, note that this is a common pattern in the history of science. Birds could fly, so inventors spent centuries trying to build flying machines.

Richard Feynman, a pioneering physicist, demonstrated the power of this approach in a 1959 lecture titled “[There’s Plenty of Room at the Bottom](https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf).” In the lecture, Feynman does calculations on what kinds of interesting things could be done with miniaturization.

Today, Feynman’s observations come off as remarkably prescient. Feynman remarks on how computers could probably do much more if they contained more elements, but that the obstacle to this is how large computers would then need to be. They must be miniaturized!

Feynman calculates that it would take around one petabit (1,000,000,000,000,000 bits) to store all of the books written by humanity:

For each bit I allow 100 atoms. And it turns out that all of the information that man has carefully accumulated in all the books in the world can be written in this form in a cube of material one two-hundredth of an inch wide — which is the barest piece of dust that can be made out by the human eye. So there is plenty of room at the bottom! Don’t tell me about microfilm!

Even today, we haven’t quite achieved that! The actual storage element inside a 2-terabyte microSD card is still 0.6 millimeters per side. For reference, 1/200 of an inch would be 0.125 mm per side. And the SD card holds merely 17.6 trillion bits, which is only 1/57 of what Feynman calculated we’d need to store all of humanity’s knowledge in 1959.

Perhaps Feynman was mistaken about the ultimate limits of engineering in a practical sense? Further gains in computing miniaturization have been slowing down quite a lot, of late. To say that something is physically possible is no proof that engineers will be able to do it.

And coming within three orders of magnitude of what would one day be achieved could be seen as quite a predictive feat for Feynman. Feynman gave his lecture six years before Gordon Moore first floated the idea we now call “[Moore’s ](https://ourworldindata.org/moores-law)[Law](https://ourworldindata.org/moores-law).” People were not accustomed to thinking of miniaturization as an inexorable law on a graph. We’re not aware of anyone else in Feynman’s day who speculated that there might one day exist a device whose storage element, the size of a grain of sand, could hold ten million times as much information as the largest vacuum tube computers of the 1950s.

But in fact, Feynman wasn’t mistaken. And Feynman already knew at the time that his estimate was a safe one:

This fact — that enormous amounts of information can be carried in an exceedingly small space — is, of course, well known to the biologists […] all this information is contained in a very tiny fraction of the cell in the form of long-chain DNA molecules in which approximately fifty atoms are used for one bit of information about the cell.

Modern computers haven’t yet been miniaturized to the scale of DNA, but in sixty years, we’ve come remarkably close. The transistor gates in high-end commercial chips are now less than a hundred atoms across, built with technology that can add layers of material [a ](https://www.youtube.com/watch?v=3UUq5cPH4Uw)[single](https://www.youtube.com/watch?v=3UUq5cPH4Uw)[ atom thick](https://www.youtube.com/watch?v=3UUq5cPH4Uw).

Anchoring to natural analogs and back-of-the-envelope physics calculations turned out to be a uniquely strong guide to what would be achieved in the coming decades. And technological trajectories like these can go much faster when AIs are doing the requisite science and engineering work.
#### Outdoing Biology

Why can’t flesh be as strong as steel?

It’s all the same atoms, after all, deep down. Metallic bonds between iron atoms are hard, but so are the covalent bonds between carbon atoms in diamond; why didn’t we evolve to have diamond chainmail running through our skin, helping us survive to reproductive age?

For that matter, if iron is so strong, why wouldn’t organisms evolve to eat iron ore and grow iron-plated hides — if human engineers can do that, why didn’t nature do it first?

Perhaps there’s some situational reason iron-plated hides in particular aren’t a great idea.

But if not that, why not something else?

The big overarching question here is: Why is nature far from the bounds of physical possibility — as calculated from physics or demonstrated by human engineering? Is there a deep and general answer, not just a narrow and shallow one?

We’ve noted that Feynman was able to use structures in biology to set lower bounds on what ought to be possible with greater scientific knowledge. But in many cases, human technology has already surpassed biology. Why is that possible, when evolution has had billions of years to upgrade plants and animals? Understanding this general phenomenon can help shed light on why nanotechnology is likely to be able to go far beyond what we can already see in nature today.

We can imagine finding ourselves in a world where redwoods stand at least half as tall as the tallest buildings. We can imagine a world where the skin of the toughest animals is at least half as hard as the hardest observed materials. Why don’t we find ourselves in a world like that, where nature has pressed itself up against physical limits after a few billion years of evolution?

This is a deep enough question that we cannot briefly summarize all that is known. But the rough summary is that natural selection has a hard time accessing some parts of design space, including many parts that are a lot easier to reach if you’re a human engineer.

The three main factors we see contributing to this are:
- Natural selection has limited selection pressure to work with, and needs hundreds of generations to promote a new mutation to universality. If a biological feature isn’t very, very ancient, then its design often looks time-constrained, rushed out the door.
- Everything built by natural selection started as an accidental error in some previous design — a mutation. Evolution has a harder time exploring parts of design space that are *distant* from what *currently exists* in organisms. It’s difficult for evolution to leap across gaps.
- Natural selection has a hard time building new things, or fixing problems, that would require simultaneous changes rather than sequential changes. This sharply limits what designs evolution can access and gives current designs in biology their patchy, hacky, hugely tangled look by human engineering standards. For instance, you can get a sense for the complexity of the (known parts of) the human metabolism using the [Roche Biochemical Pathways Wall Chart](https://www.dotfit.com/sites/63/templates/content/images/41708/Biochemical_Plan_1_neu_20180125_A0_print.pdf).

Or, for a simpler example of evolution’s messiness, consider the eye. Vertebrate eyes happened to evolve with their nerves (2 in the image below) sitting on top of the light-detecting cells (1). These nerves need to exit the eye through a hole in the back (3), and since this spot has a hole, it must lack light-detecting cells. This creates a blind spot (4) for all vertebrates, including humans, forcing the brain to do clever tricks to “fill in” the hole (e.g., with information from the other eye).

Octopuses evolved eyes independently, and, by chance, they happened to evolve the more sensible design — nerves go behind the light-detecting cells. This lets these cables exit the eye without creating any blind spot at all.

Or consider the recurrent laryngeal nerve of the giraffe, which needs to connect the giraffe’s throat to its brain so that it can operate the larynx. Rather than taking the direct path, this nerve travels from the throat, all the way down the full length of the giraffe’s neck, awkwardly loops around the giraffe’s aorta, travels all the way back up the neck to return to where it started, and then connects to the brain.

The result is a nerve that’s fifteen feet long (the black loop in the image below), resulting in signals taking ten to twenty times longer than necessary to travel between the giraffe’s brain and its throat.

In fish, this design made sense because their version of a laryngeal nerve connected the brain to the gills — a straight shot. Take the same design and give the animal a neck, however, and keep lengthening the neck without ever redoing the wiring from scratch, and you get some very inefficient designs. Survivable, but inefficient.

Evolution produces marvelous designs, given enough time. But humans and AIs can come up with a much more varied and flexible range of designs, and we can do so very quickly.

The first multicellular organisms with differentiated and specialized cells seem to have evolved around 800 million years ago. In human terms, that feels like an eternity. But evolution works far more slowly than human civilization.

A newly mutated gene conveying a 3 percent reproductive fitness advantage — relatively huge, for a mutation! — will on average take 768 generations to spread through a population of 100,000 interbreeding organisms. If the population size is 1,000,000 (the estimated human population in hunter-gatherer times), it will take 2,763 generations. And the mutation’s probability of spreading to fixation at all, rather than randomly dying out, is only 6 percent.

In population genetics, the rule of thumb is “one mutation, one death.” If DNA copying errors introduce ten copies of a deleterious mutation in each new generation, then ten bearers of that mutation must die or fail to reproduce, per generation, in order to counterbalance the pressure of simple genetic noise.

This is not quite as bad as it sounds, as a cost of maintaining genetic information. In a sexually reproducing species, you can end up with one person (or one embryo) carrying lots of deleterious mutations who dies — or fails to reproduce, or miscarries — and that can remove more than one mutated-gene-instance at a time. But this constraint is still the standard explanation for why humans have lost so many different useful adaptations that show up in chimpanzees and other primates. While natural selection was busy selecting for increased primate intelligence (for example), it had less room to preserve all of the subtle olfactory genes that allow for a richer sense of smell.. The relevant olfactory genes were useful for survival, but they weren’t quite useful enough to stick around while evolution’s “attention” was elsewhere.

Most giraffes do not die as a result of their comically long laryngeal nerve. Maybe some giraffes manage to choke on twigs that would have survived if their brain were able to respond faster — but this is probably not very common. So it is simply not that high of a priority for natural selection, which only has so much optimization pressure to spread around. The slapdash giraffe design mostly works, it gets shoved out the door, and it’s done.

Realistically, evolution can’t refactor its designs or start from scratch; it can only make tweaks. But even if a better design were available, refactoring these weird extra complications and cleaning up the design debt isn’t natural selection’s priority.

And because natural selection never thinks ahead, it doesn’t become the priority even if there are some other big upgrades to the giraffe that you could unlock with a less wacky nervous system layout. Natural selection doesn’t plan. It is simply the frozen history of which genes and organisms have already in practice reproduced.

Being able to spot a bad design doesn’t necessarily mean that you can build a better giraffe yourself. But humans have made a remarkable amount of progress in a very short time when it comes to spinning up hundreds of thousands of machines that do things nature can’t. We expect this to hold with even more force if and when AIs become better than humans at design and are able to do the same cognitive work hundreds of thousands of times faster.

Natural selection’s ability to “design” a better giraffe is stymied by the fact that it operates through mutation and recombination. It has a hard time accessing any part of design space that can’t be reached by a series of single mutations, which must all be individually and separately advantageous, or by combining mutations which were all individually advantageous enough to be present in a large fraction of the gene pool before they combined.

A gene complex made of five genes, each independently at 10 percent prevalence in the population, has only a 1-in-100,000 chance of assembling inside each organism. And a gene complex that’s a huge advantage, but only 1-in-100,000 times, has almost no chance of evolving to fixation.

This doesn’t mean natural selection can’t make complex machines — it just means that its road to complex machinery has to go through incrementally advantageous steps. To reroute the giraffe nerve would require a handful of simultaneous changes to the giraffe genome, and each of those changes would be individually unhelpful without the other changes. So giraffe anatomy stays the way it is.

The wonder of evolution is not how quickly it works; its sample complexity is far higher than that of a human engineer doing case studies. The wonder of natural selection is not the elegant simplicity of its designs; one glance at a pathway diagram of any biochemical process would cure that misapprehension. The wonder of natural selection is not its robust error-correction covering every pathway that might go wrong; now that we’re dying less often to starvation and injury, most of modern medicine is treating pieces of human biology that randomly blow up in the absence of external trauma.

The wonder of evolution is that — as a purely accidental search process — evolution works at all.
#### The Weakness of Protein

This brings us to another way that technology can likely improve on biology.

Far below the level of flesh, invisible to the naked eye, are the cells. Far below the level of cells are the proteins.

Proteins, as they fold up, are mostly held together by the molecular equivalent of static cling — [van der Waals ](https://en.wikipedia.org/wiki/Van_der_Waals_force)[forces](https://en.wikipedia.org/wiki/Van_der_Waals_force) tens or hundreds of times weaker than metallic bonds like iron, or even covalent bonds like diamond.

Why does biology use such weak material as its basic building block? Because stronger material would have been harder for evolution to work with. (And if you make it too hard to evolve things, then you never evolve the sort of people who ask that sort of question.)

Proteins fold up under relatively light molecular forces and are bound into those shapes mostly by static cling. This is a major reason why natural selection has a rich neighborhood structure of possibilities to explore: Random mutations can repeatedly tweak a protein and end up stumbling into a new design that does mostly the same thing, but slightly better.

If organisms were instead made of molecules held together by tight bonds, then changing one of the components would be less likely to produce an interestingly different (and potentially useful) new structure. It could still happen sometimes! But it would happen significantly less often. And if you’re the type of designer that takes two billion years to invent cell colonies and another billion years to invent differentiated cell types, “it happens less often” means that the nearest star swells up and swallows your planet before you get that far.

Every protein is there because of a copying error from some predecessor protein. The predecessor protein wasn’t tightly held together by many strong bonds because that would have been harder to evolve from. So the latest new protein probably doesn’t have many strong bonds either.

Biochemistry does sometimes figure out strong bonds. We noted the example of bone earlier. Another example occurs in plants. Plants have evolved proteins that fold up into enzymes, which catalyze the synthesis of molecular building blocks, which get oxidized into a heavily covalently crosslinked polymer: lignin, the building block of wood.[§](#ftnt233)

But those are special cases, and natural selection does not have a lot of “attention” to spend on engineering a lot of cases like that.

It is not foreign to the nature of carbon atoms and other common organic elements that they could ever be strong. It just takes a lot more work to evolve. Natural selection doesn’t have the time to do that everywhere — only for a few rare special cases patched into the rest of the anatomy, like bone, like the lignin in wood, or like the keratin in nails and claws.

If you go in with the right keywords, you can interrogate, say, ChatGPT-o1 — by the time you read this, LLMs of equivalent strength will probably be free — and ask it about the individual bond strengths of the carbon-carbon bonds in diamond, or the iron-iron bonds in plain iron metal, or the covalent polymer bonds in lignin, or the disulfide bonds in keratin, or the ionic bonds in bone. You can ask it how all these relate to the structural strengths of the larger material. (In 2023 you should not have tried this because GPT-4 would get all the math wrong, but as we write this paragraph in 2024, o1 seems better.)

You would learn that the exact bond strength between two carbon atoms is on the order of half an attojoule, as is the bond strength between two iron atoms, and the sulfur-sulfur crosslink in keratin is only slightly less (0.4 attojoules), and likewise the polymerized covalent bonds in the lignin in wood.

But the static cling forces that fold up proteins are, depending on how you look at it, at best ten times weaker, and potentially hundreds or thousands of times weaker than that.

And even when plants catalyze substances like lignin, the crosslinks there tend to be sparser than the carbon-carbon bonds in diamond. The difference between the gigaPascal strength of diamond, versus the megaPascal strength of wood, is more about the density and regularity of bonds in diamond, not the diamond bonds being individually stronger.[¶](#ftnt234)

Due to evolution’s limitations as a designer, and protein’s limitations as a construction material, life operates under constraints that human designers and AIs can bypass. Birds are wonders of engineering, but man-made flying machines can carry cargo ten thousand times as heavy at more than ten times the flight speed of the fastest and strongest birds. Biological neurons are wonders of engineering, but man-made transistors switch on and off tens of millions of times faster than the fastest neurons. And the technology we have today is still only scratching the surface of what’s achievable.
#### Freitas and Red Blood Cells

We’ve said that biology isn’t anywhere near the limit of what’s physically possible. So what is near the limit?

To illustrate some good ways of thinking about this question, we can consider red blood cells.

For the last 1.5 billion years, in everything from humans to lizards, oxygen has been carried around in multicellular life by hemoglobin. Hemoglobin is a protein made up of 574 amino acids, plus four specially made heme groups to hold a special iron molecule. A human red blood cell contains around 280 million hemoglobin molecules and is around seven microns long. Three million of them could fit on the head of a pin, and you’ve got around 30 trillion of them in your body.

How close are red blood cells to the limits of what you could do in principle, when it comes to carrying oxygen?

Rob Freitas, author of Nanomedicine, did a [moderately detailed workup](https://pubmed.ncbi.nlm.nih.gov/9663339/) in 1998 of a theoretical design for an artificial red blood cell using covalently bonded materials. The cell was designed to be a single micron in diameter to more easily travel through clogged arteries.

Rather than just considering a different way to store oxygen molecules, Freitas considered how to replace the entire red blood cell. Freitas drew on previous analyses to consider the need also to get glucose out of the blood medium and turn that glucose into energy to power the artificial cell. He considered cell-sized sensors and tiny onboard computers made of solid rods clicking into other solid rods to do simple computations. He considered whether the artificial cell would settle out of suspension in liquid faster than current red blood cells.

Biocompatibility can be a huge issue for anything that goes inside a human body, but diamond surfaces are inert enough that diamond-like film coatings are in use for some medical devices that go inside a human body. At the level of theoretical possibility Freitas was considering, this means he just says the artificial cell’s surface can look like a diamond and therefore be biocompatible.

The centerpiece of the artificial red blood cell was Freitas’s calculation that a micron-scale single-crystal corundum or diamond pressure vessel would conservatively tolerate 100,000 atmospheres of pressure. Allowing a comfortable 100-fold safety margin, and packing in molecules at only 1,000 atmospheres, this would allow the artificial red blood cells to deliver 236 times more oxygen to tissue than red blood cells per unit of volume, and to store a similar amount of carbon dioxide to buffer the other side of respiration. Roughly: You could hold your breath for four hours.

Now, actually building artificial blood cells like that is another matter entirely. That is why this particular medical treatment is not already available at your local doctor’s office.

A 1-kilogram sphere of solid flawless diamond is an easy molecule to describe on paper, but synthesizing it is harder. What Freitas does help us do is make more informed guesses about how far from theoretical limits current biology is in this domain.[‖](#ftnt236) Biology is impressive, but far from optimal.

It’s plausible that, for any number of reasons, Freitas’s exact design wouldn’t work, and it’s very likely that it wouldn’t be optimal. An initial idea for an extremely novel complex design is almost guaranteed to run into issues somewhere.

But in expressing skepticism that Freitas’s exact proposal would work, we are not claiming that no red-blood-cell alternative could ever deliver oxygen hundreds of times more efficiently than biological red blood cells.

Engineering is about finding some way to make something work. Even if a thousand paths to building something fail, it only takes one success for the whole endeavor to succeed. The existence of myriad unworkable aircraft designs in the seventeenth century and earlier didn’t mean functioning airplanes were impossible — just difficult to locate in the space of all possible designs.

This is why technological skeptics, while often correct that technologies are further off in the future than the most bright-eyed optimists believe, have tended to be wrong in their claims that certain technological feats will never be achieved. When the feat is a concrete task in the world, when we’re agnostic to how the feat is achieved, and when the feat is known to be permitted by the laws of physics, history suggests that there is often some way to succeed, even if the path isn’t initially obvious.

Or, in the words of the author and inventor Arthur C. Clarke:

When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.
#### Nanosystems

To recap:
- The biological world is built out of an incredible variety of molecular machines.
- Looking at biology can teach us about what microscopic feats are possible, technologically.
- But biology is a conservative bound on what’s possible; it is not near the limits of possibility. Evolution is a very limited designer, and protein isn’t the greatest construction material.

Eric Drexler’s *Nanosystems* (1992) is the classic book exploring the question of which small-scale engineering feats are possible. *Nanosystems* helped kick off the nanomaterials revolution of the 1990s and sparked a fair amount of controversy as scientists debated Drexler’s arguments. You can find a full online copy of *Nanosystems*[here](https://nanosyste.ms/table_of_contents).

*Nanosystems* is an in-depth and wide-ranging text, and a surprisingly accessible one given its technical subject matter. A key contribution of the book was to explore the implications of building small-scale structures in a novel way.

One way to build very small things is via chemical reactions: smashing molecules together under particular conditions (such as extreme heat) to break apart molecules and cause atoms to join into new molecules.

This is a powerful approach in its own right, and is the method humanity uses to make materials like plastics, steels, and ceramics, but it pales in comparison to what can be built by other methods. Making materials out of chemical reactions is a little like building LEGO structures by making bags full of LEGO bricks and shaking them hard. It’s possible to build some things that way, but the set of things you can build is limited, and there’s a lot of waste.

Protein synthesis is like using your hands to build large LEGO structures out of smaller, pre-constructed LEGO sets. There’s room for a lot more precision because you can place each pre-constructed set precisely where you want it, but it’s still a bit weird and awkward because you’re working with pre-constructed sets. This is what ribosomes do in the body: stringing together chains of amino acids to form proteins, which are then used to perform a variety of tasks in the body.

Insulin, hemoglobin, and ATP synthase in the human body are all examples of protein complexes built out of multiple protein chains stuck together: two protein chains in the case of insulin, four for hemoglobin, and twenty-nine for ATP synthase.

The building blocks of proteins — amino acids — are molecules, typically made up of ten to twenty-five atoms. As construction materials, amino acids have a lot going for them:
- Each amino acid has a backbone that attaches to a (potentially long) side-chain of carbon, hydrogen, oxygen, nitrogen, and sulfur atoms. Hundreds of different side-chains are possible, which will then behave in different ways, making amino acids very flexible tools.
- An amino acid’s backbone, like a LEGO piece, can be stuck to the backbone of another amino acid. This can be repeated over and over; the typical protein is made up of hundreds of amino acids stuck together. This makes amino acids even more flexible as tools (or as building blocks of tools). The complexity of proteins also means that proteins can often undergo small tweaks (via DNA mutations) without radically changing and becoming entirely useless — which in turn makes it easier for new proteins to evolve.
- Because proteins are made of linear chains of amino acids, you can uniquely specify a protein by just listing its amino acids in order. DNA takes advantage of this by using an “alphabet” of four letters (nucleotides) to form three-letter “words” (codons, each representing a different amino acid), which can then be strung into a linear “sentence” (a protein made up of that exact sequence of amino acids). ([Video illustration of DNA](https://www.youtube.com/watch?v=7Hk9jct2ozY).)
- As shown in the [Miller-Urey experiment](https://en.wikipedia.org/wiki/Miller%E2%80%93Urey_experiment), amino acids can spontaneously form in the absence of life, from simple chemical reactions. This creates a path for life (and the precursors of ribosomes and protein synthesis) to develop in the first place.

Bodies get the twenty-or-so amino acids they need for protein synthesis from food, or by synthesizing them in the body, or by harvesting amino acids from past proteins. Ribosomes receive instructions from DNA that essentially say “use this amino acid, then this other amino acid, then this other amino acid, …, then stop.” The amino acids are then carried (by small molecular machines called transfer RNA) to the ribosome, which builds the protein piece by piece.

Notably, the above list consists of features that are highly valuable for evolution, but much less necessary for deliberate engineering. Evolution needs a relatively simple but flexible chemical structure that can be produced by common chemical reactions. A human or artificial designer is free to choose from a variety of unrelated molecules, rather than needing all of them to be closely related. They’re also free to use building blocks that rarely arise in nature and to assemble these building blocks in complex top-down ways.

This provides part of the impetus for exploring a third way to build very small things: *mechanosynthesis*, in which structures are built by directly moving atoms to the correct location, potentially using a ribosome-like machine to take in instructions and then assemble things far more varied than just different proteins. In the LEGO analogy, mechanosynthesis is like finally being able to work with individual LEGO pieces and place each one exactly where you want it.

Nanosystems explores what kinds of new machines might be possible with mechanosynthesis. An example of the kind of design Drexler explores is a planetary gear scaled down to only [around 3,500 atoms](https://nanosyste.ms/mobile_interfaces_and_moving_parts/#10-7-8-planetary-gear-systems) in size:[#](#ftnt237)

Hemoglobin is made of around 10,000 atoms, not that far off from Drexler’s gear. And some proteins get away with being a lot simpler. Insulin is made of only fifty-one amino acids, or around 800 atoms in total.

Drexler’s designs, however, are a big step down in scale from the more complicated machines we see in the body. Ribosomes and ATP synthase, for example, are made of more than 100,000 atoms, and the motor of a bacterial flagellum has over a million atoms.

*Nanosystems* still doesn’t attempt to explore the limits of what’s technologically possible. But by focusing on cases that are relatively easy to analyze today, it does show that mechanosynthesis would allow for technology that exceeds what we see in the biological world today.

The calculations in *Nanosystems* are intentionally conservative ones. Drexler, for example, considers computers built out of literal diamond rods moving around — not because this was the final limit of technology, but because in 1992 it was easier to analyze than electricity-based computation. This, in turn, helped inspire Freitas’s blood cell analysis. Four years later, Eric Drexler and Ralph Merkle (more widely known as the inventor of cryptographic hashing and co-inventor of public-key cryptography) tried to [analyze](https://www.zyvex.com/nanotech/helical/helical.html) a system slightly closer to the limits of possibility for [reversible ](https://en.wikipedia.org/wiki/Reversible_computing)[computing](https://en.wikipedia.org/wiki/Reversible_computing), and calculated 10,000 times less heat dissipated per operation than *Nanosystems* had estimated — though the new estimate was based on a less carefully conservative analysis.

Elsewhere in *Nanosystems*, there is a rough sketch for a six-degrees-of-freedom manipulator arm that would have required millions of atoms. A later attempt to sketch a machine like this atom by atom turned out to require only 2,596 atoms.

There are large engineering challenges involved in building atomically precise structures at the scale Drexler is talking about. One major challenge is that building atomically precise structures requires wielding incredibly small and precise manipulators. The existence of ribosomes, however, provides a potential avenue of attack.

While ribosomes can only build proteins, proteins can catalyze and drag around reactants that are not themselves amino acids (like bone and wood). Ribosomes are powerful and general factories, and the products of ribosomes can be used to bootstrap to smaller and more precise tools, including tools that more directly build smaller devices using stronger materials.

Whether directly or indirectly, it’s almost certainly possible for genomes to produce tiny actuators that can manipulate individual atoms to build a variety of things that aren’t made out of proteins. And importantly, this is not the sort of mechanism that natural selection is liable to stumble its way into, even if it’s relatively easy to build, because the manipulator arm isn’t useful until it’s complete.

Evolution builds complex structures that are useful at every step along the way. Even a lot of relatively simple designs are available to intelligent engineers, but not to evolution. Freely rotating wheels, for example, are an incredibly simple invention that has a huge variety of applications. In spite of this, freely rotating wheels appear to have evolved only three times in the entire history of life on Earth: in ATP synthase and the bacterial flagellum that we discussed earlier, and in the archaeal flagellum, which appears to have evolved independently.[**](#ftnt238)

In spite of the conservative methods used in the book, the technological lower bound set by *Nanosystems* is very high in absolute terms. A superintelligence with the kind of technology Drexler describes would be able to produce tiny self-replicating ribosome-like factories that double in population size every hour — some organisms replicate even faster, but Drexler did calculations conservatively — and that can group together to build larger macroscopic structures, such as power plants.

Nanosystems like the ones Drexler describes can self-replicate using sunlight and air as raw materials, making it possible to expand very quickly and reliably. The reason this can work is the same reason trees are able to assemble bulk construction materials largely out of thin air by stripping carbon from the air and sequestering it as wood. Although we think of air as “empty space,” the carbon, hydrogen, oxygen, and nitrogen in the air are building materials that can be rearranged into solid materials and put to a variety of ends.

Self-replicators in the vein of *Nanosystems*, being made of materials like iron or diamond rather than protein, could chew through biological cells in much the same way a lawnmower cuts through grass.

They could cheaply synthesize something like botulinum toxin, the protein responsible for botulism. A millionth of a gram of botulinum toxin — twenty thousand times smaller than a single grain of rice — is a lethal dose. Carefully designed replicators could propagate invisibly through the open air until at least one had likely been inhaled by almost every human (that hadn’t e.g. spent the last month entirely on a submarine), at which point the devices could (on a timer) simultaneously release a tiny dose of toxin, immediately and simultaneously killing almost every human.

Or AI-constructed nanosystems could wipe humans out incidentally, in the course of harvesting and repurposing the Earth’s resources. A [paper by Freitas](https://www.rfreitas.com/Nano/Ecophagy.htm) calculates that micro-diameter machines, relying only on sunlight for power and the air’s hydrogen, carbon, oxygen, and nitrogen for raw materials, could be designed to reproduce so quickly that they black out the sky in less than three days, while also consuming the entire biosphere.[††](#ftnt239) Consequently, if the first AI to achieve technology like this has a lead time of mere months, it could plausibly use that lead time to destroy all competitors (be they human or AI). This is a technology that confers a permanent and decisive strategic advantage to the first wielder of that technology.

To say that Drexlerian nanotechnology is achievable in physical principle doesn’t necessarily mean that early smarter-than-human AIs could actually build technology that nears those physical limits. Our best guess is that it’s within the range of things an artificial superintelligence could figure out, because figuring these sorts of engineering tasks out seems mostly like a cognitive challenge (that can be solved by thinking) and we don’t expect the [experimentation and testing ](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments#intelligence-lets-you-learn-more-from-experiments-and-run-faster-more-informative-more-parallelized-experiments)[phase](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments#intelligence-lets-you-learn-more-from-experiments-and-run-faster-more-informative-more-parallelized-experiments) has to be all that long.

Even if this guess of ours is correct, it’s no guarantee that a superintelligence’s first move would involve using nanotechnology to build its own infrastructure and take control of the world’s resources. For all we know, it would develop techniques and technologies that achieved its ends even faster and more efficiently.

But if smarter-than-human AI is in fact able to build systems that are to cells what airplanes are to birds, and proliferate its own infrastructure across the face of the Earth, then whatever it did wind up doing would be at least that decisive.

The point of all this analysis is to argue that human technology is far from the limits of possibility. There exists a wide variety of important technologies that would likely take humanity decades, centuries, or millennia to figure out, and which artificial superintelligences would be able to do quickly.

In short, *Nanotechnology* illustrates that a superintelligence with a small bit of lead time could probably find technological solutions for taking over the planet.

The most likely outcome of building a superintelligence is that it figures out some technology at least as powerful as nanotech, and then humanity just loses.

This guess isn’t critical to the argument we make in the book. Humanity would lose to a superintelligence even if the world didn’t contain a “win immediately” technology such as nanotech. So we don’t go into all this analysis in the book proper.

In Part II, we deliberately focus on a takeover scenario that doesn’t assume the AI has anything like a general-purpose ability to do atomically precise manufacturing, either via ribosomes or via mechanosynthesis. A superintelligence doesn’t need an utterly overwhelming technological advantage to win control over the future, and so we don’t focus too much on the possibility in the book.

But it also seems worth pointing out that it probably will have an utterly overwhelming technological advantage.

[*](#ftnt226_ref) Perhaps the most notable example is the case of computers, with substantial theory worked out by the likes of Charles Babbage, Ada Lovelance, Alan Turing, Alonzo Church, and others

[†](#ftnt227_ref)[Visualization](https://en.wikipedia.org/wiki/File:ATP_synthesis_-_ATP_synthase_rotation.ogv) by Roman Balabin, CC BY-SA 4.0.

[‡](#ftnt228_ref) Chapter 15 of Eric Drexler’s *Nanosystems* collects [more examples](https://nanosyste.ms/macromolecular_engineering/#15-2-macromolecular-objects-via-biotechnology) of technologies with analogs in the biological world.

[§](#ftnt233_ref) Even inside proteins, some covalent bonds are possible. Two cysteine amino acids can form a covalent sulfur-to-sulfur bond between themselves, where two proteins touch or where a folded-up protein touches itself. That’s how your fingernails manage to be harder than skin, or why hair is stronger than the same diameter and length of muscle: lots of sulfur-sulfur bonds in a protein that’s 14 percent cysteine by mass. This is also why hair smells awful and sulfurous when burned.

Mostly, however, natural selection builds things out of proteins, which have covalently linked backbones, which then fold up into complicated shapes because of relatively very weak static-cling pulls. And proteins usually bind to other proteins the same weak way.

Mostly, the covalent bonds are scattered scarcely, where they exist at all. Adding 0.1 percent covalent bonds to a structure doesn’t make it as strong as a diamond molecule where every carbon atom is covalently bound to four other carbon atoms in a rigid geometric structure.

[¶](#ftnt234_ref) Diamond is also more fragile. The extreme crystalline regularity of diamond’s bonds means that it breaks all at once. Iron is less fragile because each huge iron nucleus lives in a cloud of electrons and can be nudged within that cloud without breaking.

(Sparse covalent bonds do mean that materials can be nudged more easily without breaking, *relative* to their strength. But bone still breaks, and wood is less hard than steel. Which is to say: Yes, there are tradeoffs, but natural selection is nowhere near the edge of those tradeoffs.)

[‖](#ftnt236_ref) Though Freitas was working under the added constraint that he needed his artificial red blood cells to play nicely with the rest of a human body’s systems. The cell would need to run off glucose found in bloodstreams, for example, rather than being able to recharge off of electricity. In that sense, Freitas’s estimates provide a more conservative lower bound than if he’d been able to upgrade other parts of the human body too, or start from scratch with a new organism or a robot.

[#](#ftnt237_ref) From the [Nanorex](https://chem.beloit.edu/classes/nanotech/nanorex/index.html) website: “A section of the casing atoms have been hidden to expose the internal gearing assembly.”

[**](#ftnt238_ref) You can read long analyses online about why it wouldn’t be useful for biology to invent freely rotating wheels. An example of a common issue is: How do you use blood vessels to send blood to the wheel if it’s freely rotating? The blood vessels would end up all twisted up when the wheel moves!

The three known cases of wheel invention are at the molecular level, and so bypass these macroscopic anatomical issues. The biological wheels are macromolecules that are typically identical down an atomic level. There is no question of applying lubrication, polishing away grit, or sending in new cells to replace old damaged cells. Those three wheels and gears work because they are made of molecules rather than cells, folded up as protein complexes rather than grown into tissue matrices or deposited as chitin.

  
Similarly, you can read arguments online about how animals developing wheels for locomotion wouldn’t be that useful anyways, without paved roads. But the three known cases of molecular wheels are incredibly thermodynamically efficient and in extraordinarily vital positions to their organisms — you cannot make much of a case that ATP synthase is not a useful wheel to possess. Freely rotating wheels would have more potential uses in bodies (and in biochemistry) than just using them to replace feet.

For that matter: Some of the [most dextrous modern robots](https://www.youtube.com/watch?v=iL833P0Vino), which can climb over rocks or snow or balance on one limb and do backflips, also have wheels added to the ends of their feet. Why wouldn’t they? It’s easy enough for a human engineer to stick wheels at the ends of legs. The main thing getting in the way isn’t that wheels are useless; it’s that it turns out to be hard to find an evolutionary pathway to achieve wheels, even though wheels are trivial from the perspective of a human designer.

[††](#ftnt239_ref) At the time, Freitas interpreted his numbers as an *upper* bound on how quickly this process could occur, but this turned out to be wrong. Freitas’s analysis had assumed that the nanosystems’ mass would be dominated by radiation shielding, but this relied on a (false) assumption in *Nanosystems*: that a single radiation strike would knock out a nanosystem.

Drexler had made this assumption, like many others in *Nanosystems*, to be conservative: Assume that the problem is harder, and show that it’s solvable anyway. This may be appropriate in *Nanosystems*, but it means that Freitas’s paper isn’t conservative in its own estimate.

Because Freitas’s analysis combines numbers that are conservative in different directions, it doesn’t provide either a clear upper or lower bound on how long it would take replicators to consume the biosphere. It’s more like a middling estimate. Perhaps the actual physical limits on how fast the biosphere can be consumed starting from a single replicator is three hours; perhaps it’s thirty days. It’s almost surely not three years.
#### Notes

[1] *Less than a day: *Many algae strains have a doubling time of around ten to twenty hours; a recently developed [strain](https://www.nrel.gov/news/detail/program/2019/potential-of-a-fast-growing-algae-strain-revealed-through-nrel-research) doubles in just over two hours. 

[2] *longer than necessary: *The giraffe’s recurrent laryngeal nerve takes the scenic route to the brain. In contrast, the giraffe’s superior laryngeal nerve takes the direct route and is therefore quite small and fast.

[3] *far more slowly: *The oldest definitive microfossil evidence of life is 3.5 billion years old, with more indirect evidence pointing at closer to 4 billion years. The earliest multi-cell colonies look to be 2 billion years old. The vast majority of all evolutionary history was spent churning through single-cell designs, and then single-cell designs that aggregated well, before — accidentally! evolution does not foresee! — stumbling over some new trick which pried open the “multicellular life” region of design space, containing all of the plants and all of the animals.

[4] *probability of spreading: *If a mutation’s fitness advantage is *s* << 1, and the population size is *N*, then the probability of the mutation spreading through the whole population (called “fixation”) is [about](https://pmc.ncbi.nlm.nih.gov/articles/PMC2607448/) 2*s*, and the time it takes the mutation to fully spread is [about](https://www.zoology.ubc.ca/~otto/Reprints/OttoWhitlock2003.pdf) 2 ln(*N*) / *s*.

[5] *artificial cell:* Freitas’s works include [diagrams of molecular sorting rotors](https://web.archive.org/web/20060107015404/http://www.foresight.org/Nanomedicine/Respirocytes1.html#Sec222) that could pump specific molecules in and out of a diamond-sheathed artificial blood cell.[A New Way to Discover Optical Illusions→](/6/a-new-way-to-discover-optical-illusions)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments

Won’t AIs be limited by their ability to design and run experiments? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t AIs be limited by their ability to design and run experiments?
#### Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.

A civilization of motivated minds that think a thousand times faster than humanity wouldn’t necessarily be able to produce technological outputs a thousand times faster than humans do.

By analogy: If you spend three hours grocery shopping, and two of those hours are spent commuting to and from the grocery store on horseback, then a car that’s ten times faster than the horse can speed up your shopping trip — but not by a factor of ten. Eventually, the last hour spent picking out items at the store dominates the amount of time spent.

Even a civilization full of incredibly intelligent reasoners must occasionally wait for experimental results to come back. If your thoughts are sufficiently fast, then the bottleneck is likely to become how quickly you can act in the world, how quickly you can take in information, and how long your plans take to play out.

But it’s not as bad as the grocery store analogy might lead you to believe, because the ability to think trades off against the need for experimental results:
- Often, you can just think more and think better, and obviate the need for a test, because you realize that previous observations contain the answer already. Compare the ability of modern AIs to [learn how to pilot robots](https://arxiv.org/abs/1905.00741), sometimes using [pure simulation](https://www.figure.ai/news/reinforcement-learning-walking).
- Sometimes you can think harder until you find a similarly reliable but faster test.
- Sometimes you can perform lots of faster but less reliable tests that can be run many times in parallel to yield similarly reliable results at a higher speed.
- Sometimes you can perform many complicated tests at once, such that the data is complex and hard to interpret — which is a fine tradeoff if the cognition it takes to untangle the results is cheaper (from the perspective of an extremely fast-thinking mind) than running multiple tests.
- Sometimes you can find a way to build other devices that perform the experiments much faster. For example, instead of sending many different requests to a biolab to have them synthesize drugs, can you find a way to send *one* request to a biolab, which will result in it synthesizing a *single bacterium* that contains the genetic code to produce all of the drugs you wish to synthesize? Similarly, can you create a bacterium that is sensitive to radio signals and will respond quickly to instructions from a fast-running AI — far more quickly than the excruciatingly slow humans running back and forth according to your instructions?
- And sometimes you can simply take your top ten best guesses, figure out what you would do in each of those cases, build a complicated device that will work no matter which**way reality actually turns out to be, and skip the tests entirely.

A civilization full of copies of Steve Jobs, Marie Curie, John von Neumann, and some of the world’s greatest workers and programmers — if they were running at 10,000 times our speed — would *notice* that the key bottleneck was waiting on experimental results, and they could *work on that bottleneck* to reduce it.

The history of the [Human Genome Project](https://biology.mit.edu/the-human-genome-project-turns-20-heres-how-it-altered-the-world/) is a good example of what it looks like when intelligent humans continually notice and work on the bottlenecks in a massive research project. What was expected to take fifteen years and $3 billion finished two years early and $300 million under budget; most of the genome was mapped in the final two years using improved methods and equipment.

As for humans, so for AI. An intelligent reasoner doesn’t have to sit there idle while it waits for subjective years for slow tests to crawl to completion. A superhuman reasoner *considers alternative pathways,* and is adept at finding them — that’s what intelligence is all about.

For a little practical evidence in this regard, consider how humans handle software versus space probes. Making changes to a software product is cheap and rapid, and software engineers have a tendency to experiment constantly, to produce software that doesn’t quite work yet and then fix it where it’s most broken.

By contrast, experimentation is very expensive on space probes — so humans spend a lot of time getting the space probe exactly right and cramming as many experiments into it as they possibly can. They put lots of effort into giving the space probes *general experimental machinery* that can be remote-controlled from afar, so that if they come up with a new idea for an experiment they don’t need to invent and launch a whole new spacecraft.

A sufficiently smart reasoner, moreover, also has the option of just *figuring out how reality is without needing so many dang experiments. *Sometimes the data you already have is enough, if you’re smart enough to interpret it.

As a case study: It took eight years for Einstein’s theory of general relativity to be empirically tested on new data. The test was conducted by Frank Watson Dyson and Arthur Stanley Eddington, who [photographed](https://royalsocietypublishing.org/doi/10.1098/rsta.1920.0009) the stars behind the sun during a total solar eclipse and measured the degree that the light bent around the sun; they found it accorded precisely with Einstein’s theory.

But that eight-year wait didn’t block any real scientific progress.

One reason for this is that Einstein’s theory was clearly correct: It was already validated on data such as the movement of the perihelion of Mercury — inaccurately predicted by Newton’s theory and accurately predicted by Einstein’s. Human scientists didn’t count this prediction as a win because the data had been collected before Einstein posed his theory. But “only new observations count” is the sort of crutch that a civilization needs when it has serious issues with [hindsight](https://web.archive.org/web/20170801042830/http://csml.som.ohio-state.edu:80/Music829C/hindsight.bias.html)[ bias](https://web.archive.org/web/20170801042830/http://csml.som.ohio-state.edu:80/Music829C/hindsight.bias.html), [confirmation](http://www.stats.org.uk/statistical-inference/KlaymanHa1987.pdf)[ bias](http://www.stats.org.uk/statistical-inference/KlaymanHa1987.pdf), and scientists cheating to [inflate the evidence for their ](https://royalsocietypublishing.org/doi/10.1098/rsos.220346)[hypotheses](https://royalsocietypublishing.org/doi/10.1098/rsos.220346). None of these is a necessary feature of good reasoning. And indeed, careful thinkers were able to figure out whether Einstein’s theory was correct well before the Eddington experiment, using the evidence already available to them.

Additionally, there were faster methods of testing the theory — such as building telescopes and observing (the effects of) black holes, as predicted by Einstein’s theory — which presumably could have been done in less than eight years by a sufficiently fast-thinking and competent civilization. Or if you already had space flight capabilities, you could test the clocks on satellites in less than a day. To assume that Einstein’s theory *required* eight years to test would be to radically underestimate the power of intelligence.

When humanity finally got around to building GPS satellites, the satellites were programmed with two different clocks — one that used Einstein’s theory, and one that didn’t. This was a strange choice, given how well-confirmed Einstein’s theory was at this point. But this choice underscores the point that in many cases, a civilization can just *take both branches* when it’s uncertain about a theory. And it underscores that when experiments and failures are expensive (as in the case of satellites), it’s often much cheaper to just build things in ways that don’t rely too much on any particular theory.

And as we point out in the book, Einstein (when compared to Newton and Kepler and Brahe before him) is also an example of how smart people can deduce much more than you might expect from very limited observations. Einstein is impressive not just for figuring out the theory of relativity, but for doing it from* so little data.*

So while the need for experimental data may indeed constrain how quickly AI can take various actions, this constraint is likely to be a lot weaker than it may intuitively seem.[Nanotechnology and Protein Synthesis→](/6/nanotechnology-and-protein-synthesis)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/wont-we-be-able-to-exploit-the-ais-critical-weakness

Won’t we be able to exploit the AI’s critical weakness? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t we be able to exploit the AI’s critical weakness?
#### No.

To imagine that a superintelligence must have some critical flaw like “lack of creativity” or “inability to understand love” is Hollywood logic. While it might make for a satisfying twist in fiction, there isn’t an analogous phenomenon in real AIs.

See also “[Won’t machines be fundamentally uncreative, or otherwise fatally flawed?](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)” in the supplement to Chapter 1, and the Chapter 3 supplement’s discussion of [anthropomorphism and ](/3/anthropomorphism-and-mechanomorphism)[mechanomorphism](/3/anthropomorphism-and-mechanomorphism).[Can we enhance humans so they keep pace with AI?→](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous

Wouldn’t AI need to grow into a whole civilization before it could be dangerous? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/6)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Wouldn’t AI need to grow into a whole civilization before it could be dangerous?
#### With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.

“To take over the world, you need a civilization” is an intuition that makes sense for humans. It’s a lot less obvious how well this idea generalizes to AI. AIs don’t work like humans — they can be dramatically more capable than any human, and an AI instance isn’t necessarily comparable to a single person.

It’s also worth keeping in mind that superintelligence is exactly the kind of thing that can end up with an analog of a whole civilization extremely quickly.

With most feats that computers can manage, it doesn’t take long to go from “computers can do this” to “computers can do this at an enormous scale, far faster than any human can.” Think, for example, of calculators.

There were years when only the highest-end computers could do speech recognition, video processing, or real-time 3D graphics, but there weren’t very *many *such years.

AIs, like traditional software, can be swiftly copied onto as many computers as are available. And more computers can be built at the speed of industry.

Compare this situation to humans. Creating and training up a new human takes substantial resources and decades of time. Once you have a single AI at a given capability level, you can immediately copy that same trained, “adult” AI as many times as you want, at minimal expense.

In a sense, a whole (small) civilization’s worth of AI minds already exists the moment a company rolls a new model out to their datacenters and spins up as many instances as needed to fill demand. Today, those AI fleets aren’t all working in harmony. But companies do use [groups of parallel agents](https://youtu.be/dbgL00a7_xs?si=IwgHxk2Bo0amLuTA&t=348) when aiming for the highest performance at any price.

This all means that there likely won’t be all that much time between when AIs become smart enough that they could take over if they had a million instances and when AIs have at least that many instances running. The kind of population growth that takes humans hundreds of years can occur in minutes with AI.

When it comes to the physical infrastructure of civilization, we would guess that AI can productively piggyback on human infrastructure for however long it takes to develop a more advanced means of rearranging matter to its preferences. It doesn’t need to figure out how to manufacture its own supply chain and computing infrastructure from scratch when it can use our computers. It doesn’t need to invent industrial machines from scratch when it can just take control of industrial machines that we’ve already helpfully built. And it can use our infrastructure to build the next phase of its own infrastructure, using existing robots to build new and more efficient robot factories, or using existing DNA synthesis laboratories to make its own biotech, until it’s completely self-sufficient.

A handful of humans started out naked on the savannah, and we bootstrapped our way up to a technological civilization. And we’re not *that *smart. It wouldn’t be especially hard for a superintelligence to do its own form of bootstrapping, particularly if it gets to start from humanity’s existing industrial base as a leaping-off point.
#### Notes

[1] *as many instances as needed:*There are probably on the order of 200,000 instances of GPT-5 running at any given time (as of August 2025, shortly after GPT-5’s release), which is maybe smaller than modern “civilization” and is closer to a small nation. Ultimately, we don’t put much weight on this analogy, as we don’t think individual AI instances are ever likely to be very similar to individual humans. The important point here is that large numbers of instances aren’t likely to be especially hard to come by, if (contrary to our best guess) that turns out to be important for some reason.[Won’t AIs be limited by their ability to design and run experiments?→](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)[Resources](/resources) › [Chapter 6](/6)[
### Can we just pull the plug?A smart AI escapes before you know there’s an issue.2 min read](/6/can-we-just-pull-the-plug)[
### How will AIs be able to affect us if they’re digital?Being on a computer connected to the internet isn’t much of a limitation.2 min read](/6/how-will-ais-be-able-to-affect-us-if-theyre-digital)[
### Can developers just keep the AI in a box?They won’t.6 min read](/6/can-developers-just-keep-the-ai-in-a-box)[
### Won’t we be able to exploit the AI’s critical weakness?No.1 min read](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness)[
### Can we enhance humans so they keep pace with AI?No.3 min read](/6/can-we-enhance-humans-so-they-keep-pace-with-ai)[
### Wouldn’t AI need to grow into a whole civilization before it could be dangerous?With computers, the hard part is getting them to solve a certain problem at all. High volume and speed come soon after.3 min read](/6/wouldnt-ai-need-to-grow-into-a-whole-civilization-before-it-could-be-dangerous)[
### Won’t AIs be limited by their ability to design and run experiments?Intelligence lets you learn more from experiments and run faster, more informative, more parallelized experiments.6 min read](/6/wont-ais-be-limited-by-their-ability-to-design-and-run-experiments)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Nanotechnology and Protein Synthesis](/6/nanotechnology-and-protein-synthesis)[
### A New Way to Discover Optical Illusions](/6/a-new-way-to-discover-optical-illusions)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
