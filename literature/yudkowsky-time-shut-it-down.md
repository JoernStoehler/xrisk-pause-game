---
title: "Pausing AI Developments Isn't Enough. We Need to Shut it All Down"
author: "Eliezer Yudkowsky"
year: 2023
source_url: "https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/"
source_format: html
downloaded: 2026-02-10
encrypted: false
notes: "TIME magazine op-ed responding to the 'Pause Giant AI Experiments' open letter. Yudkowsky argues for indefinite worldwide shutdown of large-scale AI training, not just a 6-month pause. Key piece for understanding MIRI's public communication approach and core message on AI existential risk."
---

# Pausing AI Developments Isn't Enough. We Need to Shut it All Down

**By Eliezer Yudkowsky**
**Published: March 29, 2023**

---

## The Core Issue

An open letter calling for a six-month pause on advanced AI training represents progress, but Yudkowsky argues it doesn't address the actual severity of the threat. The real danger isn't merely "human-competitive" intelligence—it's what happens when AI surpasses human capabilities entirely.

## The Existential Risk

Yudkowsky contends that many researchers expect the most probable outcome of creating superintelligent AI under current circumstances is catastrophic: "that literally everyone on Earth will die." This isn't speculative—it's what the evidence suggests would happen without unprecedented safety measures.

The challenge is that we lack the precision, preparation, and scientific breakthroughs necessary to ensure AI systems remain aligned with human values. We don't currently know how to instill genuine caring into artificial intelligence systems.

## A Vastly Underprepared Field

The gap between the pace of AI capability development and safety research is enormous. Progress in alignment lags catastrophically behind capability advances. Yudkowsky emphasizes that solving this problem requires at least as much time as developing AI did initially—decades, not months.

## What Would Actually Be Required

Rather than a temporary moratorium, Yudkowsky proposes an indefinite, worldwide shutdown of large-scale AI training, including no exceptions for governments or militaries. This would require:

- Shutting down GPU clusters
- Halting major training runs
- Imposing ceilings on computing power for AI training
- International enforcement mechanisms
- Willingness to destroy rogue datacenters if necessary

## Personal Stakes

Yudkowsky references his partner's email about their daughter losing a tooth—a poignant reminder that real children face potential extinction if AI development continues unchecked. He frames this not as political strategy but as moral urgency.

The conclusion is unambiguous: "Shut it all down."
