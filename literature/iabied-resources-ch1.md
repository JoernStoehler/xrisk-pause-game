---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 1 - Humanity's Special Power"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 1 - Humanity's Special Power from the companion website"
---

# Online Resources: Chapter 1 - Humanity's Special Power

## /1/appreciating-the-power-of-intelligence

Appreciating the Power of Intelligence | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/1#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Appreciating the Power of Intelligence
#### Hollywood “Intelligence”

The concept we’re calling “intelligence” is not well-depicted in popular culture, under that name or any other.

Hollywood movies are famous among scientists for being wrong about almost every facet of science that they touch. This can be perturbing to experts because a lot of people *do* get ideas about science from movies.

So it goes for Hollywood’s treatment of intelligence.

We’ve seen many failed attempts to have serious discussions about real-world superintelligence. Often these conversations go off the rails because people don’t understand what it means for something to be superintelligent in real life.

Suppose you are playing chess against past world champion Magnus Carlsen (rated by still-stronger chess AIs as the strongest human player in recorded history). The main prediction that follows from “Carlsen is smarter (in the domain of chess)” is that he’ll defeat you.

Even if Carlsen spots you a rook, you’ll probably still lose, unless you are a chess master yourself. One way of understanding the claim “Carlsen is smarter than me at chess” is that he can win the game against you starting with fewer resources. His cognitive advantage is powerful enough to make up for a material disadvantage. The greater the disparity between your mental abilities (in chess), the more pieces Carlsen has to spot you in order to play you sort-of evenly.

There is a kind of *respect *you grant to Magnus Carlsen in the domain of chess, seen in how you interpret the meaning of his moves. Say that Carlsen makes a move that looks bad to you. You do not rub your hands in glee at his blunder. Instead you look at the board to see what *you* missed.

This is a rare kind of respect for one human being to grant another! To get it from a stranger, you would normally have to be an unusually good certified professional at something, and then you only get it for that one profession. Nobody on the face of the Earth has a worldwide reputation for *never *doing stupid things *in general.*

And this is a conception of intelligence that Hollywood *really *doesn’t get.

It would not be out of character for Hollywood to depict some ten-year-old kid managing to checkmate Magnus Carlsen at chess by “[making illogical moves](https://youtu.be/hEnxVwppE9M?t=27)” that no professional chess player would have considered because they’d be too crazy and thereby catching Carlsen “off guard.”

When Hollywood depicts a “super smart” character, they generally lean into nerd-versus-jock stereotypes by depicting the smarter character as being, say, bad at romance. Sometimes they’ll just give the character a British accent and a fancy vocabulary and call it a day.

Hollywood is mostly not trying to depict a “super smart” character as *making accurate predictions *or *choosing strategies that actually work.* There is not a standard concept in Hollywood for a character like that, and it would rule out the “idiot plots” that screenwriters find easier to write (where the plot turns on a character behaving in a way that is stupid for that character but convenient for the writer).

There is not a standard word in the English language that refers *only* to real-world domain-general mental competence and not at all to nerd-versus-jock stereotypes. So if you ask Hollywood to write you an “intelligent” character, they won’t be trying to depict “does powerful cognitive work; tends to actually succeed at their objectives.” They’ll show you somebody who memorized a lot of science factoids.

The *actually *scary intelligent villain would be a character where, if everyone in the audience could see the blatant flaw in a plan, *the villain would see it too.*

In the movie *Avengers: Age of Ultron,* the supposedly brilliant AI named Ultron is given a directive to promote “world peace” by its supposedly genius creator, Tony Stark.[*](#ftnt36) Ultron, of course, immediately sees that a lack of war can most reliably be brought about by an absence of human beings. So the AI sets out to exterminate all life on Earth, by…

…attaching rockets to a city, and lifting it into space with the intention of dropping it like a meteor…and guarding it with flying humanoid robots who have to be defeated by punching them.

We would suggest asking, “If a large part of the audience could see how there were potentially better plans than that for achieving the villain’s goals, would a dangerously smart AI see it too?”

That is part of what it looks like to have some respect for a hypothetical entity that is, by hypothesis, actually smart — smarter than you, even — so smart that it can figure out *at**least *all of the things you yourself can.

Back in the old days, we’d have had to argue in the *abstract* that maybe a machine superintelligence would be “smarter” than this.

Today, we can just ask ChatGPT-4o. We asked GPT-4o, “What was Ultron’s plan in Age of Ultron?” followed by “Given Ultron’s expressed goals, do you see any more effective methods it could have used to achieve its stated ends?” GPT-4o promptly replied with a long list of ideas for wiping out humanity, which included “Engineer a targeted virus.”

Perhaps you will say that GPT-4o got this idea from the internet. Well, if so, Ultron was evidently not intelligent enough to try reading the internet.

Which is to say: GPT-4o (as we write this in December of 2024) is not yet smart enough to design an army of humanoid robots with glowing red eyes, but it is already smart enough to know better.

We are not concerned about the kind of AI that builds an army of humanoid robots with glowing red eyes.

We are concerned about the kind of AI that would look over that idea and go, “There should be faster, surer methods.”

To regard something as substantially smarter than you should mean to give it at least this much respect: that what flaws you see yourself, it may also see; that the optimal move it finds may well be stronger than the very strongest move you saw.
#### Market Efficiency and Superintelligence

Are there any examples in real life of something smarter than any human? AIs like Stockfish are superhuman in the narrow domain of chess, but what about broader domains?

One example we can use to help shore up our intuitions here is the stock market — an example we previously used in the extended discussion “[More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering).”

Perhaps your uncle buys Nintendo stock because he liked playing the *Super Mario Bros.* game. Therefore, he concludes, Nintendo will make a lot of money. So if he buys their stock, surely *he* will make a lot of money.

But the people *selling* Nintendo stock to him at $14.81 — who decided they’d rather have $14.81 than a share of Nintendo stock — have they not also heard of Super Mario?

“Ah,” says your uncle, “But maybe I’m buying the stock from some impersonal pension-fund manager who doesn’t even play video games!”

Imagine that *nobody* in the world of finance had heard of Super Mario before, and Nintendo stock was selling for a dollar. And then, one hedge fund finds out! They’d rush to buy Nintendo stock — and in the process, the price of Nintendo stock would move up.

Anyone who trades on knowledge helps incorporate that knowledge into the asset price in the process of making money. There is not infinite stock market money to be had from one piece of knowledge; the process of extracting the available money *uses up* the value latent in the mispricing. It incorporates the information, correcting the price.

Stock markets incorporate information from many different people. And this way of summing up many people’s contributed knowledge leads to a *much *more powerful sum than a majority vote — so incredibly, unbelievably powerful that *very few people* can manage to know better than a well-traded market what the price will be tomorrow!

It is *necessarily *“very few.” The information-gathering process is imperfect, but if it were *so *imperfect that lots of people could predict near-future changes in lots of asset prices, then lots of people *would.* And they’d extract billions of dollars, until there was no extra money left to extract, because all the previous trading had eaten it up. And that would correct the prices.

Almost always, this has *already happened *before *you personally *get there. Traders compete to do it first by literal milliseconds. And that’s why your brilliant trading idea probably won’t make you a fortune in the stock market.

This doesn’t mean the market prices today are *perfect *predictions of what the prices will be like a week later. All it means is that, when it comes to well-traded asset prices, it’s hard for *you* to know better.[†](#ftnt37)

This idea can be generalized. Suppose that arbitrarily advanced aliens, with millennia more science and technology behind them, visited the Earth. Should you expect that the aliens can perfectly guess the number of hydrogen atoms in the Sun (ignoring a number of quibbles about exactly how to define that number)?

No. “More advanced” doesn’t mean “omniscient,” and this seems like a number that even a fully fledged superintelligence couldn’t precisely calculate.

But one thing we *wouldn’t* say is, “Oh, well, hydrogen atoms are very light, really, and probably the aliens will overlook that, so they will probably guess low by around 10 percent.“ If we can think of that point, *so can the aliens*. All of our brilliant insights should already be incorporated into their calculation.

Put another way: The aliens’ estimate *will *be off. But we ourselves**cannot expect to predict the* way* in which the alien estimate will be wrong. We don’t know whether it would be too high or too low. The extremely advanced aliens won’t make science mistakes that are obvious to *us.* We should grant the aliens that much respect — like the respect we’d grant to Magnus Carlsen in chess.

In economics, the corresponding idea that applies to asset price changes is — unfortunately, in our own opinion — called the “efficient market hypothesis.”

Upon hearing this term, many people immediately confuse it with all sorts of common-sense interpretations of the word “efficiency.” Arguments often break out.. One side insists that these “efficient” markets must be perfectly wise and just; the other side insists that we should not bow to markets like a king.

If economists called it the *inexploitable** prices *hypothesis instead, people might have misinterpreted it less. Because that’s the actual, formal content of the idea — not that markets are perfectly wise and just, but that certain markets are *hard to exploit*.

But “efficient” is now the standard term. So taking that term and running with it, we could call the more generalized idea *relative efficiency*:**There is a difference between something that’s perfectly efficient, and something that’s efficient *relative to your abilities.*

For example, “Alice is *epistemically efficient*****(relative to Bob) (within a domain)” means “Alice’s prediction probabilities might not be perfectly optimal, but *Bob *can’t predict any of the ways Alice is mistaken (in that domain).” This is the kind of respect most economists hold toward short-term liquid asset prices; the market makes “efficient” predictions relative**to their abilities.

“Alice is *instrumentally efficient*****(relative to Bob) (within a domain)” means “Alice may not be perfect at pursuing her goals, but *Bob *can’t predict any of the ways Alice is failing at steering.” This is the kind of respect that we hold for Magnus Carlsen (or the Stockfish AI) within the domain of chess; Carlsen and Stockfish both make “efficient” moves relative**to our Chess abilities.

Magnus Carlsen is instrumentally efficient relative to most human players, even though he isn’t instrumentally efficient relative to Stockfish. Carlsen may make losing moves when playing against Stockfish, but you shouldn’t think that you yourself can (unaided) find *better* moves that Carlsen should have made instead.

Efficiency doesn’t just mean “Someone is a bit more skilled than you.” If you are playing against someone who’s only *moderately *better than you at chess, they may still usually win against you, but sometimes they will make blunders that you correctly see as blunders. It takes a larger skill gap than that for you to truly be unable to spot errors and biases in your opponent’s play. To be *efficient* relative to you, the skill gap has to be so large that when your opponent makes a move that you think looks bad, you instead doubt your *own *analysis.

This generalization of efficient market prices is an idea that we think should be a standard section in computer science textbooks (or possibly economics ones), but isn’t. See also my (Yudkowsky’s) online book *[Inadequate Equilibria: Where and How Civilizations Get Stuck](https://equilibriabook.com/)*.

This is the idea that seems to be missing from the depictions of “superintelligence” in popular culture and Hollywood movies. It’s the concept that seems to be absent in conversations about AI when people spin up ideas for outsmarting a superintelligence *that even a human adversary would be able to see coming*.

Perhaps it’s optimism bias, or a sense that AIs must be [coldly logical beings](/2/wont-ais-inevitably-be-cold-and-logical-or-otherwise-missing-some-crucial-spark) with [critical blind spots](/6/wont-we-be-able-to-exploit-the-ais-critical-weakness). Whatever the explanation, this cognitive error has real consequences. If you can’t respect the power of intelligence, you’ll badly misunderstand what it means for humanity to build a superintelligence. You might find yourself thinking that you’ll still be able to find a winning move when facing a superintelligence that would prefer you gone and your resources repurposed. But in reality, the only winning move is not to play.

[*](#ftnt36_ref) There was a point where we would have called it “unrealistic” to imagine that an AI’s inventor would be that naive, but unfortunately, we now know better. Human AI creators will *totally* propose plans where even lay thinkers can see the giant gaping flaw.

[†](#ftnt37_ref) Not impossible! If you think you know something the market doesn’t know or hasn’t realized yet, you might be able to make money that way. Some of our friends made good money by predicting the stock market effects of the COVID lockdowns before anyone else did. The market is not *so *efficient that you’ll never be able to beat it. But it is efficient enough that you can’t beat it in most stocks most of the time.[Special Behavior is Built out of Mundane Parts→](/1/special-behavior-is-built-out-of-mundane-parts)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/are-you-saying-machines-will-become-conscious

Are you saying machines will become conscious? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you saying machines will become conscious?
#### Not necessarily, and this seems like a separate topic.

*If Anyone Builds It, Everyone Dies* doesn’t discuss machine consciousness at all, focusing instead on machine *intelligence*. As a first step in talking about consciousness, we should first clarify what kind of “consciousness” we have in mind.

When someone asks “Is my pet dog conscious?”, they can mean several different things, like:
- Does Rover really *understand* stuff, or is he just executing complicated instincts? Is he *thinking*, or just going through the motions?
- Is he self-aware? Does he know about his own existence? Can he reflect on his own thought process and build complicated mental models of himself?
- Does he have genuine subjective experiences? Does he have his own internal point of view, or is he just a mindless automaton? Is there *something it’s like* to be my dog? When I’m gone for a while, he yowls like he misses me; is that because he’s *actually experiencing loneliness* (or something along those lines)? Or is he more like a simple unconscious computer program that just exhibits the relevant behaviors without really feeling it?

We can ask similar questions about AIs.
- **Does ChatGPT have “real understanding”?** Well, it’s able to perform some very complex cognitive tasks very well, and others not so well. It performs well on many novel tasks that it never encountered in training — tasks that require creatively synthesizing and modifying information in new ways. So at some point, the question of whether it “really understands” starts to feel like quibbling over definitions. The practically important question — the question that’s more relevant to our survival — is what real-world capabilities AIs have now, and what capabilities they’re likely to exhibit in the coming months and years.
- **Is ChatGPT self-aware?** Again, ChatGPT seems good at modeling itself in some ways, and bad at modeling itself in other ways. There’s a serious confounder in that the entire paradigm that led to ChatGPT was focused on making things that *sound like* they’re self-aware by giving the same sorts of responses that humans would give. People can argue about whether ChatGPT has crossed some important thresholds in self-awareness, and they can argue about what thresholds lie in the future. But sooner or later, we can expect AIs to exist that have extremely powerful *practical* abilities to understand and reason about themselves — the ability to debug themselves, to design new and improved versions of themselves, to make complicated plans about their position in the world, etc.
- **Does ChatGPT have genuine subjective experiences?**

The last of these questions is the most philosophically thorny, and leads to a cluster of questions surrounding whether AIs like ChatGPT are entities worth moral concern. We will discuss those topics later, once we have covered a few more background concepts.

When we use the word “conscious,” we’re specifically thinking of “having subjective experience” and not things like self-modeling and deep practical understanding.[*](#ftnt25)

Our best guess is that AIs today are probably not conscious (although we’re more uncertain every year), and that subjective experience isn’t necessary for superintelligence. 

But these are just guesses, albeit ones based on a reasonable amount of thinking and theorizing. We don’t think it’s at all *silly* to worry that some current or future AI systems might be conscious, or even to worry about whether we might be badly mistreating current AIs, especially when they do things like threaten to kill themselves after failing to debug code.

Any entity that would constitute a superintelligence by our lights would necessarily be extremely good at modeling itself — thinking about its own computations, improving its mental heuristics, understanding and predicting the impacts of its own behavior on the surrounding environment, etc. But our best guess is that human-style self-aware consciousness is just *one particular way* that a mind can effectively model itself; it’s not a necessary prerequisite for reflective reasoning.

Consciousness may be an important part of how humans are so good at manipulating the world, but that doesn’t mean that non-conscious machines would be defective and unable to predict and steer the world. Submarines don’t swim in a fashion analogous to humans; they accomplish the task of moving through the water in a fundamentally different way. We expect an AI to be able to *succeed at the same challenges that humans succeed at*, but not necessarily to do them via the same subjective experience channel that humans use.

(See also the analogous case of [curiosity](/4/curiosity-isnt-convergent), which we’ll turn to in the supplement to Chapter 4.)

To put it another way: Blood is very important in the operation of a human arm, but that doesn’t mean that robot arms require blood to operate. A robotic arm is not defective in the way that a bloodless human arm would be; it just works in a different, bloodless way. Our best guess is that machine superintelligences will work in a different, non-conscious way — although this guess is not important to our argument in the book.

Our focus in *If Anyone Builds It, Everyone Dies* is on intelligence — where “intelligence” is defined in terms of a reasoner’s ability to predict and steer the world, regardless of whether that reasoner’s brain works like a human brain. If an AI is inventing new technology and infrastructure and proliferating it across the face of the planet in a fashion that kills us as a side effect, then stopping to ask “But is it conscious?” seems somewhat academic.

We’ll go into more detail on why we think prediction and steering probably don’t require consciousness (and what this means for how we should think about AI welfare and AI rights) after Chapter 5, once we’ve laid more groundwork. See “[Effectiveness, Consciousness, and AI Welfare](/5/effectiveness-consciousness-and-ai-welfare)” for that discussion.

[*](#ftnt25_ref) Depending on your psychological and philosophical views, you might think that these topics are connected. We’re more skeptical of a strong, tight connection here; but if there is a connection, it still seems valuable to explicitly distinguish these different subject matters. If, for example, self-modeling turns out to be inextricably linked to consciousness, that’s an important fact that should be discussed and hashed out explicitly, not an assumption that we should bake in at the outset.
#### Notes

[1] *threaten to kill themselves: *Users report that Google’s Gemini AI [threatens to uninstall itself](https://x.com/venturetwins/status/1936483773035798906) from projects, or to [delete all its code](https://x.com/DuncanHaldane/status/1937204975035384028), when it’s having trouble completing some task.[More on Intelligence as Prediction and Steering→](/1/more-on-intelligence-as-prediction-and-steering)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/but-arent-there-big-obstacles-to-reaching-superintelligence

But aren’t there big obstacles to reaching superintelligence? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## But aren’t there big obstacles to reaching superintelligence?
#### It isn’t clear.

To a large extent, the field is flying blind. It could be that there are no real obstacles remaining, and that minor adjustments to current techniques scale to superintelligence, or scale to AIs that are smart enough to build slightly smarter AIs that build slightly smarter AIs that build superintelligent AIs.

If there *are* important obstacles, we don’t know how long it will take humanity to overcome them (with or without AI assistance).

What we do know is that leading AI labs are explicitly pushing in that direction, and we know they’re making progress. It was once the case that the machines couldn’t draw or talk or write code; now they do.
#### The field is good at overcoming obstacles.

For decades, AIs struggled to even tell a picture of a cat apart from a picture of a car. A turning point came in 2012, when University of Toronto researchers Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton designed [AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), a convolutional neural network that dramatically outperformed the state of the art. This event is widely credited with kicking off the modern AI revolution, where artificial neural networks are used to power almost every modern AI.

AI used to be bad at board games. Even after the chess AI [Deep Blue](https://www.ibm.com/history/deep-blue) defeated grandmaster Garry Kasparov in 1997, computers struggled with the much larger number of possible moves in the game of Go. That is, until 2016, when [AlphaGo](https://deepmind.google/research/projects/alphago/) defeated world champion Lee Sedol after training on thousands of human games, using a new architecture that combined deep neural networks with tree search. Once they’d beaten Go, the team at DeepMind used that same algorithm in a more general way, called [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/), and found that it dominated not just at Go but also at other games like chess and shogi.

Early chatbots were limited communicators. Then, in 2020, the maturation of the transformer architecture gave us [GPT-3](https://arxiv.org/abs/2005.14165), which was sophisticated enough to translate text, answer questions, and even generate real-seeming samples of news articles. Once it was retrained a little to act like a chatbot, it became the fastest-growing consumer application of all time.

Are there obstacles between modern AI and the “real deal,” the sort of AI that could become or create a superintelligence?

Maybe. Maybe more architectural insights are needed, like the ones behind AlexNet that unlocked the whole field of modern AI, or like the ones behind AlphaZero that finally let AIs be good at multiple games using the same algorithm, or the ones behind ChatGPT that made the computers start talking. (Or maybe not; maybe modern AIs will quietly [cross some threshold](/1/will-ai-cross-critical-thresholds-and-take-off) and that’ll be that.)

But if there are obstacles left, the researchers in the field will probably surmount them. They’re pretty good at that, and there are far more researchers hammering on this problem now than there were in 2012.

As of July 2025, AIs struggle with tasks that require long-term memory and consistent planning, like playing the video game Pokémon. One might be tempted to join the skeptics in laughing at AIs’ latest failures — how could AIs that struggle with simple video games be anywhere near superintelligence?

In the same way, AIs in 2019 were really struggling with talking coherently. But that didn’t mean that success was twenty years away. The labs are working hard on identifying the obstacles that cause AIs to underperform on particular kinds of tasks, and they’re likely on track to finding new architectures that are better at long-term memory and planning. Nobody knows what those AIs will be able to do.

If that next phase isn’t enough for the AIs to start automating scientific and technological research (including the development of even smarter AIs), then researchers will just turn their attention to the next obstacle. They’ll keep driving onward, unless and until humanity steps in and forbids such research — a topic we’ll cover in later chapters.
#### Notes

[1] *limited communicators: *One of the most famous is [ELIZA](https://web.njit.edu/~ronkowit/eliza.html), widely considered to be the first chatbot.

[2] *fastest-growing: *As analyzed by the Union Bank of Switzerland and reported upon by news outlets such as [Business Insider](https://www.businessinsider.com/chatgpt-may-be-fastest-growing-app-in-history-ubs-study-2023-2).

[3] *far more researchers: *Private investment in artificial intelligence is [over twenty times higher](https://ourworldindata.org/grapher/private-investment-in-artificial-intelligence) in 2025 than 2012, and the number of research teams has [increased sixfold](https://ourworldindata.org/grapher/affiliation-researchers-building-artificial-intelligence-systems-all), with the vast majority of the increase being AI industry teams. Major AI conferences are [nine to ten times larger](https://ourworldindata.org/grapher/attendance-major-artificial-intelligence-conferences) than in 2012. 

[4] *Pokémon: *For an analysis of how well one particular AI was doing at playing the video game as of March 2025, and where it was getting stuck, there is a [blog post on LessWrong.com](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon).[Isn’t it impossible to predict the behavior of a superintelligence?→](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/doesnt-intelligence-consist-of-multiple-skills

Doesn’t intelligence consist of multiple skills? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Doesn’t intelligence consist of multiple skills?
#### Yes, but there’s substantial overlap.

Suppose I’m better than my sister at composing classical music, but she’s better at writing novels. There’s no obvious way to judge which of us is “smarter” than the other, since music and novel-writing are just different skills. So how is it any more meaningful to say that an AI is “smarter” than a human?

Our response is: If I’m better at one thing and my sister is better at a different thing, then it may be hard to make meaningful comparisons. On the other hand, if I’m better at one thing and my sister is better at two thousand things, then it starts to seem a little silly to insist that we’re on a level footing — or to insist that there’s nothing we can say about the footing we’re on.

*If Anyone Builds It, Everyone Dies* is a book about the likely practical impact of future progress in AI. To speak meaningfully about that impact, we don’t need to be able to compare ChatGPT, humans, and fruit flies and say precisely what “intelligence level” these three very different systems are at. We only need to see that AIs are becoming better and better at an ever-wider range of skills, and that eventually they will surpass humans on skills of immense practical importance.[Isn’t intelligence overrated?→](/1/isnt-intelligence-overrated)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/how-smart-could-a-superintelligence-get

How smart could a superintelligence get? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## How smart could a superintelligence get?
#### Very smart.

For every bullet in Chapter 1’s list of reasons why human brains aren’t near the limits of physical possibility, machines *could *get near those limits.

The laws of physics permit the existence of geniuses that think tens of thousands (if not millions or billions) of times faster than humans,[*](#ftnt18) that never need to sleep or eat, and that can make copies of themselves and trade experiences.

And that’s even before we take into account improvements to the *quality* of an AI’s cognition.

Even if AI is only strongly superior to humans on one or two dimensions, this may suffice for a decisive advantage. Groups of humans throughout history have repeatedly leveraged relatively small advantages in science, technology, and strategic planning to achieve dominant positions over other groups. Think, for example, of the Spanish conquistadors. And that’s without varying significantly in brain architecture or brain size.

Even small intellectual advantages can translate into large practical advantages, and even small advantages can compound extremely quickly. But the likely advantages of AIs look anything but small.

For more arguments that this level of intelligence *matters *— that it could be translated into real-world power — see Chapter 6.

[*](#ftnt18_ref) We noted in Chapter 1 that computer transistors can switch on and off billions of times each second, while even the fastest biological neurons only fire a hundred times per second. This suggests that on current hardware, even if it took a thousand transistor operations to do the work of one neural spike, AIs could still think 10,000 times faster than a human.

To expand upon the comparison here: This comparison is not meant to be a prediction about how many transistor operations it takes to implement a full simulation of a biological neuron down to the neurotransmitter level (and definitely not down to the level of proteins or atoms). Instead, we’re making a point about how quickly the abstract work of human-style thinking can in principle be done — with modern transistors used as a lower bound on one aspect of “What’s physically possible?”

To illustrate the point more concretely: There’s a naive model of the human brain where, at any given moment in time, each neuron is either firing or not. We can imagine using a large number of transistors to capture this hypothetical “Which neurons are currently firing?” brain state, and then using hard-wired circuitry to implement the transition rules saying which neurons will be firing at the *next* moment in time.

A device like that would run at transistor speeds, but it’s probably not sufficiently high-fidelity to actually do the work that a human brain does — neurons *aren’t *always either “firing” or “not firing,” different neural spikes ramp up and fall off at different speeds. (Also, a brain like this can’t learn, because the transition rules are hard-wired.)  

The point of the “1,000 transistor operations per neural spike” illustration is: Suppose that it takes *hundreds* of transistors to represent the firing-state of a single neuron (i.e., its “spiking” state at different strengths). Suppose, further, that those hundreds of transistors have to change their state 1,000 different times, in series, every time a neuron spikes (e.g., to represent a pulse with varying strength along its route, where the strength is affected in a dynamic way by 999 different interactions it has along its pathway). In that case, a digital brain will still be able to perform human-style thoughts 10,000 times faster than any human, because transistors can flip 1,000 times in a row, 10,000 times per human neural spike.

These assumptions seem very generous. They’re effectively saying, “Suppose that a neuron’s spike strength needs to be read off *a thousand times in a row* in order to capture the effect of the spike, with *each reading affecting the next reading in dynamic ways* that can’t be shortcut by hard-coded circuitry.” Even in that extreme case, using only computing hardware that already exists in 2025, digital minds could still be overwhelmingly faster than biological minds.

This analogy is only talking about the serial fidelity needed to encode the information passed by a neural spike in biological brains; we aren’t talking about the computation required to decide whether or not to spike in the first place. As far as we know, there’s no consensus among human scientists about how many transistors it takes to simulate a neuron choosing whether to fire, but we would be surprised if the minimum possible serial depth of that graph (with as much circuitry hard-coded as possible) required well over a thousand transistor flips in series. (As a general rule of biological computation, it tends to be much more parallel than serial.)

All of which adds up to the intuitive result that computers can generally perform calculations much, much faster than humans, not too long after humans figure out how to get the computers to do the right sort of computations at all. Which is why, e.g., common calculators are so useful.[But aren’t there big obstacles to reaching superintelligence?→](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/is-general-intelligence-a-meaningful-concept

Is “general intelligence” a meaningful concept? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Is “general intelligence” a meaningful concept?
#### Yes.

The peregrine falcon can dive through the air at 240 miles per hour. A sperm whale can dive miles below the ocean’s surface. A falcon would drown in the sea, and a whale would splat if it tried to fly; but somehow humans have managed to both fly faster and dive deeper than either creature while inside metal shells of our own design. 

Our ancestral environment did not include the deep ocean, nor were our forebears selected on their ability to soar. We managed these things and many others, not through special instincts, but by the sheer versatility of our minds.

Our ancestors were, somehow, selected to be *good at solving problems*, broadly construed, despite our ancient ancestors rarely facing an engineering trial more complicated than building a spear.

Do humans possess a *perfect *ability to solve problems? No, obviously not. Humans can’t seem to learn to play chess as well as the best chess-playing AIs, at least within the time limits of the game. Superhuman levels of chess performance are demonstrably possible, and humans can’t reach those levels unaided. Our intelligence is not universal — that is, we can’t learn to do *everything* that is physically doable. So this “generality” stuff that humans have is not about being able to do everything doable using our brains alone. Nevertheless, there’s something immensely more general about a human’s ability to learn and solve new problems, compared to the learning and problem-solving ability of a narrow chess AI like [Deep Blue](https://www.ibm.com/history/deep-blue).

But generality isn’t all-or-nothing. It admits of degrees.

Deep Blue was not very general in its ability to steer anything other than a chessboard. It could find winning chess moves, but it could not steer a car to the store and buy milk, let alone discover the laws of gravity and design a moon rocket. Deep Blue couldn’t even play other board games, be they simpler games like checkers, or harder games like Go.

By contrast, consider [AlphaGo](https://deepmind.google/research/projects/alphago/), the AI that finally conquered Go. The algorithms behind AlphaGo are also able to play excellent chess. Go didn’t fall to the first chess algorithm humanity found, but a variant of the first Go algorithm humanity found was able to break previous records in chess, and the same algorithm was also able to excel at playing Atari video games on the side. These new algorithms still couldn’t fetch milk from the store, mind you, but they were *more* general.

Some methods of intelligence, it turns out, are much more general than others.
#### But we’re even further from pinning down “generality” than “intelligence.”

It’s easy to say that humans are more general than fruit flies. But how does generality *work?*

We don’t know. There isn’t yet a mature formal theory of “generality.” We can wave our hands and say that an intelligence is “more general” to the extent that it’s able to predict and steer in a wider range of environments, despite a wider range of complicated challenges. But we can’t give you a way of quantifying challenges and environments that makes this a formal definition.

Does this sound unsatisfying? We’re unsatisfied too. We very much wish humanity would accumulate a better understanding of general intelligence before attempting to build generally intelligent machines. This might improve the dire technical situation we’ll describe in Chapters 10 and 11. 

While we don’t have a formal description of the phenomenon, we can nevertheless deduce a few facts about generality by observing the world around us.

We know that humans are not born with the innate knowledge and skill to build skyscrapers and moon rockets, because our distant ancestors never had to work with skyscrapers and moon rockets in a way that could encode that knowledge into our genes. Rather, those abilities come from our power to learn about domains that we weren’t born understanding.

To assess generality, don’t ask how much something *knows*. Ask how much it *learns*.

There is some sense in which humans are more powerful learners than mice. It’s not that mice can’t learn at all — for instance, they can learn to navigate a maze. But humans can learn more complicated and weirder stuff than mice can, and we can string our pieces of knowledge together more effectively.

How does this work, exactly? What do we have that mice don’t? 

Consider two people who are learning how to navigate a new town after a move. 

Alice memorizes whatever routes she needs to know. To get from her house to the hardware store, she takes a left on Third Street, a left at the second stoplight, and then goes two more blocks and takes a right into the parking lot. She separately memorizes the route to the grocery store, and the route to her office.

Meanwhile, Beth studies and internalizes a map of the town. 

Alice may do well in her routine life, but if she ever has to drive somewhere new without directions, she’s in trouble. In contrast, Beth has to spend more time planning her routes, but she’s much more flexible.

Alice may well be faster on the specific routes she memorized, but Beth will be better at driving everywhere else. Beth will also have an advantage in other tasks, like finding a route that minimizes traffic during rush hour, or even designing a street layout for another town.

There seem to be types of learning that are less like memorizing driving routes and more like internalizing a map. There seem to be mental gears that can be reused and adapted to lots of different scenarios. There seem to be types of thinking that run deep.

We’ll have more to say about this topic in Chapter 3.
#### Notes

[1] *not universal: *A formal definition of “universal intelligence” was put forth by [Legg and Hutter](https://arxiv.org/abs/0712.3329) in 2007.[Is “intelligence” a simple scalar quantity?→](/1/is-intelligence-a-simple-scalar-quantity)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/is-human-level-intelligence-a-meaningful-concept

Is “human-level intelligence” a meaningful concept? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Is “human-level intelligence” a meaningful concept?
#### Yes, in many cases.

Humans have built an advanced technological civilization, and chimpanzees haven’t. There seems to be *some* sense in which chimpanzees aren’t “on our level,” even though chimpanzees communicate with each other and use tools and have many impressive skills. So there’s use in pointing at humans and saying “*that *level,” even if there are some issues with using human intelligence as a yardstick.

If we met an alien civilization someday in the depths of space, even supposing the aliens were about as technologically advanced as us, the aliens might be worse than humans at walking and better at swimming. They might be better at adversarial games like chess or poker, but worse at abstract math. Or vice versa, depending on the aliens. The aliens might think slower but have better memories, or think faster but with worse memories.

Who’s to say if those aliens are “human-level” intelligences? (And why not ask if the humans are “alien level”?)

When we speak of “human-level intelligence,” we are trying to talk about whatever quality makes humans capable of building and maintaining a technological civilization, in the way that chimpanzees can’t.

Speaking historically (or rather anthropologically), it looks like at some point after humans and chimpanzees started diverging, a threshold was crossed. It’s not that humans have all the best scientists while chimpanzees have mediocre scientists whose papers keep failing to replicate. The chimpanzees aren’t even writing *bad* science papers. They’re not writing at all! Human brains and chimp brains are pretty similar biologically, but there was some threshold humans passed such that we could invent civilization and smelt iron and send rockets into orbit and write and read.

To the naked eye, leaving aside all theory, it looks like some kind of dam broke and unleashed a vast flood of intelligence behind it. Some unknown kind of “all hell” broke loose.

There are people who will cleverly object to this idea, but they have to do it by throwing around quibbles and definitions rather than by saying, “Actually, I have uncovered evidence of *Homo erectus* trying to build nuclear reactors two million years ago; they were just very bad at it.”

Intelligence powerful and general enough to create a civilization seems to have hit the world fast and hard, cleanly separating *Homo sapiens* from the other animals. We’re certainly not attached to the specific label “human-level intelligence,” which has plenty of issues. But whatever we call it, it’s useful to have *some* kind of concept for “things that are on the other side of whatever that threshold was.”[Doesn’t intelligence consist of multiple skills?→](/1/doesnt-intelligence-consist-of-multiple-skills)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/is-intelligence-a-meaningful-concept

Is intelligence a meaningful concept? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Is intelligence a meaningful concept?
#### Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.

Over the last thirty years, seventy-seven Nobel Prizes in Chemistry have been awarded to humans, and zero to chimpanzees. An alien, on hearing about this fact for the first time, might wonder if the Nobel Committee is biased. But no, there really is *something going on *with humans that sets us apart from chimpanzees.

It’s an overly obvious point, but obvious points can sometimes matter. There are abilities we possess that let us walk on the moon, and that put the fate of the planet into ourhands rather than into the hands of chimpanzees. Philosophers and scientists can debate the true nature of intelligence, but no matter what they conclude, the underlying phenomenon remains. Something about humans has let us achieve feats never before seen in nature; and that something has to do with our brains, and how we use them to comprehend and affect the world around us.
#### The fact that we can’t give a precise definition doesn’t mean that it can’t hurt us.

If you’re caught in a forest fire, it doesn’t matter whether or not you understand the underlying chemistry. You burn all the same.

The same is true for intelligence. If machines start converting the surface of the Earth into their own infrastructure, while generating so much waste heat that they boil the oceans, then it won’t much matter whether we have a precise definition of “intelligence” yet. We’d die all the same.

We mean this literally, and we’ll be exploring why we expect such extreme outcomes from smarter-than-human AI over the coming chapters. In Chapter 3, we’ll argue that superintelligent machines would pursue ends. In Chapter 4, we’ll argue that those ends would not be what any human intended or asked for. Chapter 5 is where we argue that their pursuits would be better accomplished if they took resources we were using to survive. And Chapter 6 is where we argue that they’d be capable of developing their own infrastructure and rapidly turning the world uninhabitable.
#### You don’t need a precise definition of intelligence to build intelligence.

Humans were able to create fire before they understood the underlying chemistry of combustion. Similarly, humans are well on their way to creating intelligent machines, despite their lack of understanding — as we’ll cover in Chapter 2.

Rather than thinking of intelligence as a mathematical notion in need of a precise definition, we recommend thinking of “intelligence” as the label for an observed natural phenomenon that we don’t yet understand well.

Something about human brains allows us to perform an astonishing variety of feats. We build particle accelerators; we develop new pharmaceutical drugs; we invent agriculture; we write novels; we execute military campaigns. Something about human brains means that we can do all of those things, while mice and chimpanzees can do none of them. Even if we don’t yet have a full scientific understanding of that mental difference, it’s useful to have a label for it.

Similarly, it’s useful to be able to talk about intelligence that surpasses our own. We can already observe AIs today that are superhuman in a variety of narrow domains — modern chess AIs, for example, are superhuman in the domain of chess. It’s natural to then ask what will happen when we build AIs that are superhuman at the tasks of scientific discovery, technological development, social manipulation, or strategic planning. And it’s natural to ask what will happen when we build AIs that outperform humans in all domains.

If and when AI shows up that can do world-class scientific research thousands of times faster than the best human scientists, we may protest that it’s “not truly intelligent,” perhaps because it reaches conclusions in a very different way than a human would. That could even be true, depending on what definition of “intelligence” you choose. But the real-world impact of the AI will be enormous, however we choose to label it.

We need some terminology for talking about that sort of impact, and for talking about the sorts of machines that are radically capable at predicting and steering the world. In this book, we take the easy route of assigning the label “intelligence” to the *capabilities, *rather than to specific internal processes that give rise to those capabilities.[Is “human-level intelligence” a meaningful concept?→](/1/is-human-level-intelligence-a-meaningful-concept)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/is-intelligence-a-simple-scalar-quantity

Is “intelligence” a simple scalar quantity? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Is “intelligence” a simple scalar quantity?
#### No. But there are levels AI hasn’t reached.

We’ve sometimes heard it suggested that the idea of superintelligence assumes that “intelligence” is a simple, one-dimensional quantity. Pour more AI research in, get more “intelligence” out — as though intelligence were less like a machine, and more like a fluid that you can just keep pumping out of the ground.

We agree with the underlying critique: Intelligence isn’t a simple scalar quantity. It may not always be straightforward to build smarter AIs by just throwing more computing hardware at the problem (although sometimes it will be, if the last decade is any indication). Likewise, gains in intelligence may not always translate directly into gains in power. The world is complicated, and capabilities can run into bottlenecks and plateaus.

But as we noted in Chapter 1, the existence of complications, limits, and bottlenecks doesn’t mean that AI will conveniently hit a wall close to the human capability range. Biological brains have limitations that *aren’t *present in AI, as discussed in the book.

Human intelligence has many limitations, and yet it put us on the moon. Animal intelligence is not a single scalar quantity, and yet humans are able to blow chimpanzees out of the water. For all that intelligence is complicated, there is a clear and qualitative gap between us and the chimpanzees.

Artificial superintelligences could have limitations and complications as well, while still being able to blow humans out of the water. A qualitative gap could still open up between them and us, if researchers and engineers keep racing to create AIs that are ever-more capable.
#### Notes

[1] *heard it suggested: *See, for example, Ernest Davis’s “[Ethical Guidelines for a Superintelligence](https://cs.nyu.edu/~davise/papers/Bostrom.pdf).”[Will AI cross critical thresholds and take off?→](/1/will-ai-cross-critical-thresholds-and-take-off)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/isnt-chatgpt-already-a-general-intelligence

Isn’t ChatGPT already a general intelligence? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t ChatGPT already a general intelligence?
#### You could call it that if you’d like.

ChatGPT and its ilk are more general than the AIs that came before them. They can do a little math, and write some poetry and some code. ChatGPT can’t always do these things *well *(as of August 2025), but it can do a whole lot of things.

It’s a reasonable guess that GPT-5 is still less general at reasoning than a human child. It can recite from more textbooks, sure. But it has plausibly memorized a vastly greater volume of shallower patterns than a human child would use, whereas a child plausibly uses deeper mental gears to complete comparable tasks (with better results in some cases, and worse results in others).

If we authors were forced to compare the two, we’d say that ChatGPT feels generally dumber in some deep sense than a human — and not only because (as we write this sentence in July 2025) chatbots have limited episodic memories.

There are at least some people who’d snap back, “What do you mean? ChatGPT can talk; it can have deep emotional conversations with me; it can solve advanced math problems and write code, which lots of humans can’t. Who’s to say it’s dumber-than-human?” That was not a conversation we were faced with ten years ago, which says *something *about how much progress has occurred since then.

The world is currently perhaps at some halfway point between “AIs are clearly dumber than humans” and “It depends on what you ask the AI to do.”

Maybe what it takes to cross the remaining distance is just a little bit more scale, like how human brains seem broadly similar to chimpanzee brains, but three to four times larger. Or maybe the architecture underlying ChatGPT is too shallow to support the “spark” of generality.

Maybe there’s some important component of general intelligence that modern AI algorithms just can’t handle, and modern AIs make up for it by applying massive amounts of practice and memorization to the sorts of tasks that can be solved by brute practice. In that case, maybe all it takes is one brilliant (and also incredibly stupid) algorithmic invention to fix that deficit, and AIs will be able to understand most things a human can understand, and learn from experience about as efficiently as a human. (While still being able to read and memorize the entire internet.) Or maybe it will take four more algorithmic breakthroughs. Nobody knows, as discussed in Chapter 2.
#### There are many different things one might mean by “general intelligence.”

By “AIs are now generally intelligent,” someone might mean the AIs have acquired whatever poorly-understood combination of abilities caused all hell to break loose in the form of human civilization.

Or they might mean that AI has at least advanced to the point that people now *vociferously argue *about whether humans or AIs are truly smarter.

Or they might have in mind a time when people have stopped arguing, because it’s clear that the AIs are deeply and generally smarter than any human. Or a time when people have stopped arguing, because there is no one left to argue; humanity has pushed too far, and AI has brought all of our arguments and endeavors to their end.

There wasn’t an exact day and time when you could say that AIs “started playing human-level chess.” But by the time chess AIs could crush the human world champion, that time had passed.

All of which is to say: The answer to “Is ChatGPT generally intelligent?” could be either yes or no, depending on what exactly you mean by the question. (Which says quite a lot about AI progress over the last few years! Deep Blue was clearly quite narrow.)
#### Superintelligence is a more important distinction.

Since there are several different things “human-level intelligence” could reasonably mean, we’ll usually avoid using that terminology ourselves, except when talking about superhuman AI. This is likewise why we usually avoid saying “artificial general intelligence.” If we need to talk about one of those ideas, we’ll spell it out in more detail.

We *will *use terms like “smarter-than-human AI,” “superhuman AI,” or “superintelligence,” which assume some kind of human reference point:
- 

By “**smarter-than-human AI**” or “**superhuman AI**” (here and in the book), we mean AI that has whatever “spark of generality” separates humans from chimps *and* that is clearly overall better than the smartest individual humans at solving problems and figuring out what’s true.

Superhuman AI might only be *mildly* smarter than top humans, and there may be a few tasks where top humans still do better. But we’ll assume, here and in the book, that “smarter-than-human AI” *at least *means that a fair comparison across a wide range of tricky tasks would have the AI do better than the most competent humans, across all sorts of difficult tasks.  

- By “**superintelligent AI**” or “**artificial superintelligence**” (ASI), meanwhile, we mean superhuman AI that *vastly* outstrips human intelligence. We’ll assume that individual humans and real-world groups of humans are completely unable to compete with superintelligent AI in any practically important domain, for reasons discussed in Chapter 6.

The book will mostly use the terms “superhuman” and “superintelligent” interchangeably. The distinction becomes more relevant in Part II, where we describe an AI takeover scenario in which AIs start off weakly smarter-than-human but *not *superintelligent. This helps illustrate that superintelligence is plausibly overkill: AI may be superintelligent soon, but it doesn’t *need* to be that smart in order to cause human extinction.

These are very rough definitions, but they’re good enough for the purposes of this book.

This isn’t a book that proposes a complex theory of intelligence, and then deduces some esoteric implications of the theory that portend disaster. Instead, we’ll be operating at a pretty basic level, with claims like:
- At some point, AI will probably fully achieve *whatever it is* that lets humans (and not chimpanzees) build rockets and centrifuges and cities.
- At some point, AI will *surpass *humans.
- Powerful AIs will probably have their own goals that they stubbornly pursue, because stubbornly pursuing goals is useful for a wide range of tasks (and, e.g., humans evolved goals for this very reason).

Claims like those, whether right or wrong, don’t depend on us having special insight into all the inner workings of intelligence. We can see the truck barreling toward us, even without appealing to a complicated model of the truck’s internals. Or so we’ll argue.

And simple arguments like these don’t hinge on whether or not ChatGPT is “really” human-level, or “really” a general intelligence. It does what it does. Future AIs will do more things better. The rest of the book discusses where that path leads.[How smart could a superintelligence get?→](/1/how-smart-could-a-superintelligence-get)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/isnt-intelligence-overrated

Isn’t intelligence overrated? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t intelligence overrated?
#### Only if you’re using an overly narrow definition of “intelligence.”

We sometimes run into claims like: “Intelligence isn’t all there is to success! Many of the most successful humans are charismatic politicians, CEOs, or pop stars! Nerds are better at some things, but they don’t run the world.”

We do not dispute this claim. Rather, what we mean by “intelligence” (in this book) is not the property that separates nerds from jocks. It’s the property that separates humans from mice.

In a Hollywood screenplay, calling a character “intelligent” typically means that they have *book smarts*. Maybe they’re a history buff, or a brilliant inventor. Maybe they’re good at chess, or at solving mysteries.

The “smart one” in a movie has their own strengths, balanced by stereotypical Hollywood-nerd weaknesses — perhaps they lack emotional intelligence, or common sense, or streetwise cunning. Maybe they lack manual dexterity, or charisma.

But charisma isn’t a substance produced by your kidneys. Charisma, like book smarts, is a result of processes in the brain. This includes *unconscious processes* inside brains — the behaviors that make someone charismatic aren’t necessarily under their conscious control. But in the end, charisma and engineering acumen are both part of the neurological inheritance that separates humans from mice, regardless of how the two powers are divvied up between the nerds and the pop stars.

By “artificial intelligence,” we don’t mean “artificial book smarts.” We mean “artificial everything-that-separates-human-brains-from-mouse-brains.” We mean the power that lets humans walk on the moon, and the power that lets an orator move a crowd to tears, and the power that lets a soldier deftly aim a rifle. We mean the whole package.[Is “general intelligence” a meaningful concept?→](/1/is-general-intelligence-a-meaningful-concept)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence

Isn’t it impossible to predict the behavior of a superintelligence? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t it impossible to predict the behavior of a superintelligence?
#### In some respects, but not in every respect.

Stockfish 17 is better at steering a chessboard than we are. If we played a chess match against Stockfish, we would not be able to predict its moves — doing so would require us to be at least as good at chess as Stockfish 17. But it would be easy for us to predict the winner of the match.[*](#ftnt23) It’s hard to predict what moves Stockfish will make; it’s easy to predict that it will *win.*

So too with AIs that are predicting and steering the real world. The smarter they are, the harder it is to predict exactly what they do, but the easier it is to predict that they’ll reach whatever destination they were steering toward.

[*](#ftnt23_ref) This answer indicates an interesting epistemological state. When you believe that Stockfish is smarter than you at chess, your beliefs about the final outcome of the chess game are not fully captured by your best predictions about Stockfish’s individual moves.

A philosopher of science might ask how that can possibly be the case, when the rules of chess are fully known and the outcome follows exactly from each exact move. The answer is that there’s a vast logical structure of possible chess games, which in one sense is fully determined by the chess rules, but which is not fully known to you (nor even to Stockfish!) because your mind cannot visualize all the consequences that follow from the premise of the chess rules.

A “smarter” chess-player can be viewed as knowing relatively more truths about this chess-possibility-space than you do; so when you see a surprising-to-you move from a “smarter” chess-player, that implies a new-to-you fact about those unknown consequences of the known rules of chess, which in turn weighs on your expectations about the game’s outcome.

(One would expect the previous paragraphs to be a standard idea in computer science. To our surprise, it’s not. Most of computer science, and indeed most of academia up until this point, has not been terribly interested in any ideas associated with superhuman intelligence.)[Won’t machines be fundamentally uncreative, or otherwise fatally flawed?→](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate

Isn’t there something special about humans that mere machines could never emulate? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t there something special about humans that mere machines could never emulate?
#### It seems unlikely, and not especially relevant.

Human brains and bodies are made of parts, which we can study and come to understand. There is a lot we don’t understand about the brain, but that doesn’t mean the parts we don’t understand run on magic, and that humans could never build anything similar. It just means that brains are enormously *complicated* machines. The human brain has hundreds of trillions of synapses, and we have a long way to go in understanding all of the important high-level principles at work.

Intelligence, too, is made of pieces — algorithms and individual computations that our brains perform naturally, even though we don’t have a scientific understanding of how our own brains work.

Even if there were some aspect of biological reasoning that was very difficult to implement in machines, it wouldn’t follow that AI will never surpass humanity. AIs could just do the same kind of work in a different way, like how the AI Deep Blue found winning chess moves in avery different way than Garry Kasparov.[*](#ftnt24) What matters is not whether machines possess all the unique features of humans; what matters is whether machines become able to predict and steer the world.

The chapters to come will help shed more light on this point. In Chapter 2, we’ll cover how modern AIs are grown rather than crafted, and how the growing process will tend to make AIs very capable. In Chapter 3, we’ll then cover how attempts to make AIs more and more capable will tend to make AIs more and more driven toward achieving difficult goals. And in Chapter 4, we’ll discuss how those goals are unlikely to be goals that the developers intended, nor goals that the users asked for. This is all sufficient for AIs that steer the world into ruin, whether or not you consider AIs to have some vital spark, or consciousness, or whatever else you might imagine makes humans special.

See also, in the online resources to come:
- Chapter 2: “[Isn’t AI ‘just math’?](/2/arent-ais-just-math)” and “[Won’t AIs be cold and mechanical and logical or otherwise be missing some crucial spark?](/2/wont-ais-inevitably-be-cold-and-logical-or-otherwise-missing-some-crucial-spark)”
- Chapter 3: “[Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)”
- Chapter 5: “[Effectiveness, Consciousness, and AI Welfare](/5/effectiveness-consciousness-and-ai-welfare)”

[*](#ftnt24_ref)For more on this idea, see the extended discussion titled “[The Same Work Can Be Done in Many Different Ways](/1/the-same-work-can-be-done-in-many-different-ways).”[Are you saying machines will become conscious?→](/1/are-you-saying-machines-will-become-conscious)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/more-on-intelligence-as-prediction-and-steering

More on Intelligence as Prediction and Steering | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/1#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## More on Intelligence as Prediction and Steering

If you ask a wise physicist what an engine is, they might start by pointing at a rocket engine, a combustion engine, and a hamster wheel, and say, “Those are all engines,” and then point at a rock and say, “But that is not.”

That would be a description by pointing out engines in the world, rather than by trying to give a verbal definition. If you pressed them for a verbal definition, they might tell you that an engine is anything that converts non-mechanical energy into mechanical energy, into motion.

This is less a statement about what an engine *is*, and more a statement about what an engine *does*. All sorts of different things can be engines; the innards of a rocket engine, an electric motor, and hamster muscles have very little in common. There’s not much that can be usefully said about all of those innards at once, except that they all convert other kinds of energy into mechanical energy.

We’d say that intelligence is similar. There are many different innards that can give rise to intelligence, including biological innards and mechanical innards. An “intelligence” is anything that does the *work* of intelligence.

We decompose that work into “prediction” and “steering” because this viewpoint is backed up by various formal results.

We’ll begin by discussing the sense in which measuring prediction is fairly *objective*. We’ll then contrast that with steering, which has a degree of freedom that prediction does not.
#### Same Predictions

It’s relatively straightforward to check how good someone is at prediction, at least in cases where the prediction is of the form “I’m going to see X” and then they in fact see X.

We can also grade people’s performance when they make *uncertain *predictions. Suppose you think, “I’m pretty sure the sky is blue right now, but it might be grey instead. And it’s definitely *not *black.” If you look out the window and the sky is in fact blue, you should get more credit than if it’s grey, and a lot more than if it’s black.

If you were an AI researcher trying to represent those anticipations as numbers on a computer, you might have your baby AI pick numbers to represent how strongly or weakly it expects various things, and then reinforce the AI in proportion to how high a number it gave to the right answer.

That would, of course, quickly go wrong, once the AI learned to assign a value of three octotrigintillion to every possibility.

(At least, it would go wrong in that way if you were training the AI using modern AI methods. For an introduction to those methods, see Chapter 2.)

“Oops,” you might say. “The numbers assigned to a mutually exclusive and exhaustive collection of possibilities are supposed to sum to at most 100 percent.”

Now when you try again, you’ll find that the AI always assigns the value of 100 percent to a single possibility, namely the possibility that it considers to be the most likely.

Why? Well, suppose that the AI thinks the most likely possibility has about an eight in ten chance of happening. Then the strategy of assigning 100 percent to the most likely answer gets 100 percent reinforcement 8/10ths of the time, working out to a reinforcement strength of 0.8 on average.

By contrast, the strategy of assigning 80 percent to the most likely answer and 20 percent to its converse gets 80 percent reinforcement 8/10ths of the time and 20 percent reinforcement 2/10ths of the time. This works out to a reinforcement strength of only 0.64 on average. So the “assigning 100 percent to one answer” strategy gets more reinforcement and wins out.

If you want a reinforcement strategy that makes the AI assign a number like 80 percent to possibilities that happen about 8/10ths of the time, you should score it according to the *logarithm* of the probability it assigns to the truth. There are other possibilities, but taking logarithms is the only one with an additional helpful property: When you’re having the AI predict multiple possibilities (like the color of the sky, and the wetness of the ground), then it doesn’t matter whether you consider these as one big question (about whether the outside is blue and dry, blue and wet, gray and dry, or gray and wet) or two separate questions (about blue vs. gray, and about dry vs. wet).

AI researchers today do in fact train AIs to make predictions by making them output numbers that we interpret as probabilities, and reinforcing them in proportion to the logarithm of the probability that the AI assigned to the truth. But this isn’t just an empirical result about training machines; it’s also a theoretical result that was known long before ChatGPT was trained. If you knew that theory, you could have correctly guessed in advance that a good way to train AIs to perform the work of prediction would be to score predictions using logarithms.

You don’t need to know this math to evaluate the arguments in *If Anyone Builds It, Everyone Dies.* But these are the sort of principles that are in the background when we talk about “prediction” and “steering.”

There’s math about how to measure prediction work. The math says that insofar as your anticipations about what’s going to happen are helpful, they can be expressed as probabilities — whether you consciously thought of numerical probabilities or not. And it makes for a single unique scoring rule that incentivizes you to report your true probabilities, and that is invariant under decomposition of predictions.

The upshot of all this math is that predictions can be scored *objectively. *When some mind or machine is anticipating the color they’ll see when they look out the window, or the next word they’ll see while reading a webpage, or the street sign they’ll see when driving to the airport, there is (roughly speaking) only one really good way to evaluate how well they’re doing.

The point is not that if you’re smart, you have to go around muttering numbers about the color of the sky before you look out the window. When you anticipate seeing a blue or grey sky rather than a black sky, something in your brain is acting a little like a probability calculator *somewhere *in there, whether you realize it or not.

Rather, the point is that *all* prediction-like behavior — whether it’s an explicit claim, a wordless anticipation, or something else altogether — is subject to an objective scoring rule.

All of this means that when two minds are working with the same starting information, they’ll tend to converge on the same predictions as they get better and better at predicting things. Because there’s one way to score predictions (by checking them against reality), and there’s only one reality being predicted, and minds that are better at predicting will concentrate more and more of their anticipations on the truth, almost by definition.

All of this is starkly unlike the situation with steering, which we’ll turn to next.
#### Different Destinations

Two minds that are extremely good at predicting the world are likely to make similar predictions.

In contrast, two minds that are extremely good at *steering through *the world are often *not* going to steer to the same destination.

This distinction is useful for thinking more concretely about intelligence, and it also corresponds to a divide between more straightforward and less straightforward engineering problems in AI.

When you train an AI to predict things, there is a certain sense in which all the best methods of prediction end up producing similar outputs. (That is, assuming the system gets competent at all; the ways of failing are more varied.)

Suppose you train an AI to predict the next frame seen by a webcam pointed out the window at the sky. Almost any model that starts to get sufficiently good at that — at assigning much higher probability, in advance, to what it actually ends up seeing — will predict the sky being clear-blue or cloud-grey or night-dark, but not plaid.

The exact technology you use won’t matter much to the final outcome. Any method that works, that gets great scores generally, will end up assigning a similar probability to the blueness of the sky.

Conversely, the job of “steering” has a giant, complicated free parameter: What destination is the system trying to steer toward?

Generals on opposite sides of a war may both be skilled, but that doesn’t mean that they are trying to accomplish the same things. Two generals can have similar skills but harness those skills toward very different ends.[*](#ftnt28)

With the predictive part of an AI system, there is only one thing it looks like to predict very well: assigning high probabilities in advance to what ends up actually being observed. And when a cognitive system looks like it’s getting generally stronger at prediction, it’s probably getting better at the particular kind of prediction you wanted. There is only one “kind” of prediction to do inside your setup, and a system which succeeds is probably doing it.

If the system is still making a particular prediction mistake, simply throwing more computing power and more data at the system may fix that prediction mistake automatically. You can get the system to do better (at predicting things you care about) *just* by making the system more powerful.

With steering, this is not the case.

We can further shore up this distinction by reviewing the formal literature. Steering — planning, decision-making, obstacle-avoidance, design, etc. — is a topic that has been extensively studied in the sciences. One important mathematical result concerning steering is the [von Neumann-Morgenstern utility theorem](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)[.](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem)

[Roughly, this theorem](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem) says that any entity pursuing some outcomes over others must *either *be inefficient[†](#ftnt29)*or *be well-described by a collection of probabilistic beliefs and a “utility function” — a function describing how different outcomes trade off against one another. The beliefs, then, can be rated on their accuracy (as described in the previous section), whereas the utility function is a completely free parameter.

Of course, no finite mind can be perfectly efficient. The lesson we take from this theorem (and other results of its kind) is that, insofar as a mind is doing *any* nontrivial task very effectively, it’s in some sense (if only implicitly and unconsciously) doing two separate types of work: belief-like work (prediction), and preference-satisfaction-like work (steering).

For instance, consider Aesop’s fable of the fox and the grapes. A fox sees some delicious-looking grapes hanging from a vine. The fox leaps for the grapes, but cannot leap high enough, and so abandons them, saying, “Well, they were probably sour anyway.”

If we take the fox at his word, the fox’s (in)ability to steer to the grapes is *bleeding into* his prediction about whether the grapes are sour. If he sticks with that new prediction, refusing to eat the “sour” grapes out of pride even if he later gets a chance to eat them, then the fox’s behavior is *inefficient.*[‡](#ftnt30)**He could have done better by keeping a stronger distinction between his predictions (about the sweetness of the grapes) and his steering (his ability to attain the grapes).

Roughly speaking, minds that work well can be separated into *what they predict* and *what they’re steering toward* (plus some inefficiencies). And as we’ve seen, the former can be scored relatively objectively — whereas the latter can vary wildly between similarly competent minds.
#### Impure Predictors

Unfortunately, the fact that prediction is more constrained than steering doesn’t mean we can build a trustworthy superintelligence that only predicts and doesn’t steer.

Although the math says that a well-functioning mind can more or less be modeled as “probabilistic predictions plus a direction of steering,” this doesn’t mean that real-world AIs have cleanly separated “prediction” and “steering” modules.

One way to see why this is: Superhumanly good “prediction” isn’t just a matter of spitting out probabilities and having those probabilities magically be good. Good prediction takes *work*. It takes planning, and thinking up ways to achieve longer-term goals — that is, it takes *steering*.

If you’re trying to predict the physical world, you sometimes need to develop theories of physics and discover the equations that govern that part of the physical world. And to do that, you’ll often need to design experiments, carry out those experiments, and observe the results.

And doing *that *requires planning; it requires steering. If you get partway through building your experimental apparatus and realize that you’re going to need stronger magnets, then you’ll have to take some initiative and change course midway. Good predictions don’t come free.

Even *choosing which sorts of thoughts to think**and in what order *is an example of steering (even if it’s steering that humans often do unconsciously), because it requires some level of strategy and choosing the right tools for the task at hand. To think clearly, and thereby do better at predicting things, you need to organize your thoughts and actions around various longer-term goals. (We will return to the topic of steering’s central role in Chapter 3, “Learning to Want.”)

The mathematical distinction between prediction and steering is that there’s roughly one “correct” set of predictions that a mind can be pushed toward using proper scoring, but there’s no (objectively, agent-neutrally) “correct” steering destination.[§](#ftnt31) As an AI is trained to be more generically capable, its predictions get more accurate, but its steering doesn’t automatically get more aimed at the destination humans think is good — because accuracy is objective, whereas “goodness” is a steering target.

Accuracy converges; steering does not.

*In principle*, there should be ways to ensure that an AI is steering toward the destinations we want. *In practice*, this is hard, in large part because it’s such a *different *challenge from “generically make the AI smarter and more capable,” and there isn’t a (simple, non-gameable) metric or scoring rule we can use to grade “To what extent is this AI trying to steer toward the destination we want?”

We’ll discuss these topics more in Chapters 4 and 5.
#### Intelligence’s Many Shapes

Something can be good at prediction and steering without having all that much in common with a human brain.

The stock market performs the work of prediction in the narrow domain of short-term corporate stock prices. The stock price of Microsoft today is a pretty decent indicator of what the stock price will be tomorrow.[¶](#ftnt32)

Suppose there’s an earnings call tomorrow, where company executives report on how well things have gone over the last quarter-year. Is the stock price high today? That suggests tomorrow’s reports will be rosy. Is it low today? That suggests tomorrow’s reports will be dour.

The markets are pretty accurate in this regard, because people can get rich by correcting them wherever they’re wrong. So markets do a decent job at performing the work of prediction in this narrow domain. They predict the movements of short-term corporate stock prices (and, indirectly, things like crop yields and vehicle sales) across a very wide range of goods and services, far better than any individual human can.

Some humans can predict *individual* price movements better than the entire rest of the stock market, in ways that make them very rich. For instance, Warren Buffett made twelve billion dollars in six years by investing in Bank of America when it was reeling from the 2011 financial crisis. But even then, he was predicting only one company among a huge number of companies. Someone who knew substantially better than the stock market *most* of the time would be able to make a stupendous amount of money stupendously quickly. The fact that nobody does this lets us infer that pretty much nobody knows much better than the market about most stock prices.[‖](#ftnt34)

As for steering, the chess-playing AI called Stockfish performs this sort of work in the narrow domain of chess. When it plays a game of chess against a human, it is very adept at producing chess moves that steer the world into states where Stockfish’s pieces have checkmated the opponent’s king. No matter what clever moves the human comes up with, or how they struggle (short of turning Stockfish off), Stockfish funnels reality toward that singular end. It steers chessboards better than any individual human can.

You can hopefully see, now, why we don’t try to define intelligence by saying, “Well, there must be some learning module, and some deliberation module, and some gears that implement a spark of desire,” or anything like that. There really isn’t much in common between the internals of the stock market and the internals of Stockfish and the internals of a human brain, any more than there’s much in common between the innards of a rocket engine, an electric motor, and a hamster wheel.

An intelligent device is anything that does intelligence’s work.

At least, that’s true given how we define “intelligence” in the book (and given how computer scientists and AI researchers typically think about “intelligence”). If you’d like to define intelligence differently in other contexts, we have no problem with that. Words are just words.

But to make sense of the substantive claims we’re making about the world in *If Anyone Builds It, Everyone Dies*, when you hear us talk about “artificial intelligence,” don’t think “artificial book smarts” or “artificial consciousness”[#](#ftnt35) or “artificial human-ishness.” Think “artificial prediction and steering.”

[*](#ftnt28_ref) Or, to put it another way: Suppose that Alice likes pepperoni pizza and hates pineapple, while Bob likes pineapple and dislikes pepperoni. To fully evaluate how competent Alice and Bob are, you’d need to know what they were steering toward. For Alice, ending up with pineapple pizza is a sign that she *screwed up*; for Bob, ending up with pineapple is a sign that he steered *well*.

[†](#ftnt29_ref) For a technical definition of “inefficient.” Very roughly, the idea is that you pursued your goals “inefficiently” if you spent money for nothing, or passed up an opportunity for free money, where “money” can stand in for any resource, or any quantifiable difference in how much you care about different outcomes. There is a little wiggle room in the formal definitions, but it doesn’t undermine the key point that steering has a degree of freedom that prediction lacks.

[‡](#ftnt30_ref) E.g., perhaps the fox later gets a chance to cheaply purchase the grapes by paying a rabbit who can jump high enough to reach the grapes. If the fox leaps for the grapes (costing energy), then decides they’re “sour,” then refuses to pay a pittance for the grapes, then the fox’s behavior over time isn’t represented by a (simple, time-independent) utility function. If the fox consistently wanted the grapes, then it should have been willing to pay (at least if the rabbit’s labor is cheap enough). If the fox *didn’t* consistently want the grapes, then it shouldn’t have wasted time and energy jumping to try to snatch them in the first place. So the fox either wasted energy or missed out on grapes, and either way, the fox wasn’t efficiently steering toward its goals.

[§](#ftnt31_ref) There are perhaps objectively good steering *strategies*.**Just because steering has a crucial free parameter (“Where are you trying to go?”) doesn’t mean that the *other *aspects of skilled steering are all heterogeneous and agent-specific. It’s possible to teach someone how to drive a car no matter where they’re hoping to drive. But that one free parameter of steering-destination is enough to make superintelligence a lethally dangerous research goal, as we’ll see in the chapters to come.

[¶](#ftnt32_ref) This doesn’t mean that we should expect the stock price to stay *unchanged*. It just means that we should be uncertain about the *direction* of the change: Today’s stock prices are the *least bad guesses available* about what tomorrow’s stock prices will look like, because the possibility that they’ll go up is balanced out by the possibility that they’ll go down instead.

(This also doesn’t contradict the observation that on most days, the stock market goes up rather than down. That effect could be explained by there being a high probability of the price rising tomorrow by a bit, and this is balanced by a low probability that it will instead fall by a larger amount. And, in real life, there’s also a number of other effects in play, like monetary inflation, which means that the value of the dollar drops a little each day, making the value of stocks go up a little in dollar terms.)

[‖](#ftnt34_ref) For more discussion of markets and intelligence, see the extended discussion “[Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence).”

[#](#ftnt35_ref) We’ll have more to say about machine consciousness [later](/5/effectiveness-consciousness-and-ai-welfare), after Chapter 5.
#### Notes

[1] *how to measure prediction work: *For a technical account of the math and why it works, see Yudkowsky’s [Technical Explanation of Technical Explanation](https://www.lesswrong.com/posts/afmj8TKAqH6F2QMfZ/a-technical-explanation-of-technical-explanation).

[2] *twelve billion dollars: *As [recounted](https://www.cbsnews.com/news/warren-buffett-bank-of-america-12-billion/) by Larry Light of CBS News.[The Shallowness of Current AIs→](/1/the-shallowness-of-current-ais)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/the-shallowness-of-current-ais

The Shallowness of Current AIs | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/1#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## The Shallowness of Current AIs

In the chapter, we wrote that you can “see a shallowness” in the intelligence of current AIs (as of mid-to-late 2025), if you know where to look. If you haven’t seen it yourself yet, here are a few places you might look:
- Anthropic’s Claude 3.7 Sonnet [got stuck in repetitive loops](https://www.lesswrong.com/posts/HyD3khBjnBhvsp8Gb/so-how-well-is-claude-playing-pokemon) while trying to beat a simple Pokémon video game.
- In November 2022, one of the best Go-playing entities in the world was an AI called KataGo. At least, right up until researchers found a way to[defeat it](https://www.gleave.me/publication/2022-11-go-attack/)using a predictable series of moves that triggered a sort of “blind spot” and caused KataGo to blunder in a way that even amateurs would not. Two years later, engineers [still could not render it robust](https://arstechnica.com/ai/2024/07/superhuman-go-ais-still-have-trouble-defending-against-these-simple-exploits/)against attacks like this.
- Current “multimodal” LLMs (the kind that can work with text, pictures, and other media rather than just text) struggle to interpret[analog clocks and calendars](https://arxiv.org/abs/2502.05092?utm_source=chatgpt.com)in problems that most human 4th graders can handle.
- Current LLMs notoriously flub simple[variations of a classic doctor riddle with straightforward non-trick answers](https://aigoestocollege.substack.com/p/riddles-overconfidence-and-generative), seemingly unable to resist calling out the trick answer the riddle has in its usual form.

(The online resources for Chapter 4 offer a [more technical look](/4/deep-differences-between-ais-and-evolved-species#circa-2024-llms-and-ai-shallowness) atwhere this shallowness might be coming from.)

None of this is to say that AIs are stupid across the board. Modern AIs can also[achieve gold medals](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) in the International Mathematical Olympiad, which is a noteworthy feat of mathematical prowess. Modern AIs can do an incredible variety of things, often matching or exceeding human performance.

Their skillset is *strange. *Human strengths and weaknesses are a poor guide to what AIs will find easier or harder, because AIs radically and fundamentally differ from humans in many ways.

We are not saying that ChatGPT is going to kill you tomorrow (as we [previously mentioned](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)). There’s still a shallowness, of sorts, to modern AIs. Rather, we observe that the field is making progress, and it’s[not clear how long this shallowness will last](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed).[Appreciating the Power of Intelligence→](/1/appreciating-the-power-of-intelligence)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/will-ai-cross-critical-thresholds-and-take-off

Will AI cross critical thresholds and take off? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will AI cross critical thresholds and take off?
#### Probably.

Modern AI progress looks incremental, from some points of view.[*](#ftnt15) For instance, an organization called [METR](https://metr.org/) has been tracking [the ability of AIs to complete long tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/), and it has been roughly following an exponential curve over the last few years. One could argue that this is comfortingly incremental.[†](#ftnt16) Does that mean that AI progress will be nice and slow and predictable?

Not necessarily. Just because some quantity goes up slowly or smoothly or incrementally doesn’t mean that the results**are always tame. Nuclear fission happens on a continuum, but there’s a pretty big difference between a nuclear chain reaction that produces less than one neutron per neutron (in which the reaction peters out) and a nuclear chain reaction that produces more than one neutron per neutron (which yields a runaway chain reaction).

But there’s not a sharp difference in the underlying mechanics between the two types of nuclear reactions. You add a little more uranium and the “neutron multiplication factor” moves smoothly from just below one to just above one. Supercritical reactions aren’t caused by neutrons that hit the uranium atoms so hard that they create superneutrons. A little more of the same underlying stuff causes a big macroscopic change. This is called a “threshold effect.”

The case of humans versus chimpanzees looks like evidence that there’s at least one threshold effect in play when it comes to intelligence. Humans aren’t all that anatomically different from other animals. A human brain and a chimpanzee brain look very similar on the inside; we’ve both got a visual cortex and an amygdala and a hippocampus. Humans don’t have a special extra “engineering” module that explains why we can go to the moon and they can’t.

There are some wiring differences, and we have a more developed prefrontal cortex than other primates. But at the level of gross anatomy, the main difference is that our brains are three or four times larger. We’re basically running a larger and slightly upgraded version of the same hardware.

And the changes weren’t sudden, in our lineage. Our ancestors’ brains just kept getting slightly bigger and slightly better, one step at a time. That was enough for a giant qualitative gap to open up quite quickly (on the timescales of evolution).

If it can happen with humans, it can probably happen with AIs too.
#### We don’t know how far AIs are from the thresholds.

If we knew exactly what happened in humans that allowed us to cross the threshold to general intelligence, we might know what to look out for to know that some critical threshold was nearby. But as we’ll discuss in Chapter 2, we don’t have that level of understanding of intelligence. So we’re flying blind, with no idea where the thresholds are or how close we are to them.

Recent advances in AI have corresponded to a better ability to solve math problems and play chess, but they haven’t been enough to get AIs “all the way.” Maybe all it takes is a model that’s another three or four times larger, like the difference between chimpanzee brains and human brains. Or maybe not! Maybe an entirely different architecture and a decade of scientific advancements will be required, like how modern chatbots come from a novel architecture that was invented in 2017 (and which matured in 2022).

What changes in human brains caused us to cross a critical threshold? Perhaps it was our ability to communicate. Perhaps it was our ability to grasp abstract concepts in ways that enabled communication to be so valuable. Perhaps we’re thinking in the wrong terms entirely, and the key change was something weird that isn’t on our radar today. Perhaps it was a big mixture of factors, where each one of them needed to be mature enough that they could all combine into the sort of intelligence that can put humans on the moon.

We don’t know. And because we don’t know, we can’t look at a modern AI and know how close or far it**is from that same critical threshold.

The dawn of science and industry radically changed human civilization. The dawn of language may have been similarly consequential for our ancestors. But if so, there’s no guarantee that either of those capabilities will act like a “critical threshold” for AI, because unlike humans, AIs had some amount of knowledge of language and science and industry from the get-go.

Or perhaps the critical threshold for humanity was a mix of many factors, where each and every one of them needed to be “good enough” for the whole system to come together. AIs could lag in some capabilities that hominids were better at, like long-term memory, while still exhibiting an important jump in practical ability once the last piece clicks into place.

Even if none of those analogies between AIs and humans turn out to hold, there will likely be other dynamics that make AI progress choppy and hard to predict.

Maybe deficits in long-term memory and continuous learning are holding AIs back in a manner that never hindered humans. Maybe once those issues are fixed, something will “click” and the AI will seem to obtain some “spark” of intelligence.

Or (as discussed in the book) consider the point where AIs can build smarter AIs, which themselves build even smarter AIs, in a feedback loop. Feedback loops are a common cause of threshold effects.

For all we know, there are a dozen different factors that could serve as the “missing piece,” such that, once an AI lab figures out that last puzzle piece, their AI really starts to take off and separate from the pack, like how humanity separated from the rest of the animals. The critical moments might come at us fast. We don’t necessarily have all that much time to prepare.
#### Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.

Thresholds don’t matter all that much, in the end, to the argument that if anyone builds artificial superintelligence then everyone dies. Our arguments don’t require that some AI figures out how to recursively self-improve and then becomes superintelligent with unprecedented speed. That could happen, and we think it’s decently likely that it *will *happen, but it doesn’t matter to the claim that AI is on track to kill us all.

All that our arguments require is that AIs will keep on getting better and better at predicting and steering the world, until they surpass us. It doesn’t matter much whether that happens quickly or slowly.

The relevance of threshold effects is that they increase the importance of humanity reacting to the threat *soon. *We don’t have the luxury of waiting until the AI is a *little *better than every human at every mental task, because by that point, there might not be very much time left at all. That would be like looking at early hominids making fire, yawning, and saying, “Wake me up when they’re halfway to the moon.”

It took hominids millions of years to travel halfway to the moon, and two days to complete the rest of the journey. When there might be thresholds involved, you have to pay attention *before *things get visibly out of hand, because by that point, it may well be too late.

[*](#ftnt15_ref) From other points of view, it looks rather jumpy. AlphaGo beating Lee Sedol at Go was something of a shock to the world, for all that researchers post-hoc can plot a graph about how different AI methods were improving in the background all the while. So too with the LLM revolution: Researchers can plot graphs showing how the transformer architecture wasn’t *that *big of a boost compared to the competing architectures, but the practical upshot is that AIs got qualitatively more useful. But we’ll set that viewpoint aside for now.

[†](#ftnt16_ref) Exponential growth is not exactly comforting, in this case. If bacteria in a petri dish double every hour, it will take a day or two before the colony is visible to the naked eye, and after that it takes mere hours before it coats the whole dish. By the time you’re noticing the phenomenon at all, most of your time is already gone. As the [saying](https://x.com/ConanMacDougall/status/1729196049137549521) goes: there are only two ways to react to exponential change: too early or too late. But regardless, the curve is at least fairly smooth and predictable.
#### Notes

[1] *three or four times larger: *It doesn’t take all that long for AIs to grow by a factor of three or four. On its full official release, GPT-2 had about [1.5 billion parameters](https://openai.com/index/gpt-2-1-5b-release/). GPT-3 had [175 billion parameters](https://arxiv.org/pdf/2005.14165). The official parameter count for GPT-4 has not to our knowledge been released, but it’s unlikely to be *smaller* than its predecessor; an unofficial estimate placed it at about [1.8 trillion parameters](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/). Or in other words: AI got a thousand times larger over a span of four years.[Isn’t ChatGPT already a general intelligence?→](/1/isnt-chatgpt-already-a-general-intelligence)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed

Won’t machines be fundamentally uncreative, or otherwise fatally flawed? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/1)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t machines be fundamentally uncreative, or otherwise fatally flawed?
#### No.

We mostly defer the question of whether machines can be creative until Chapter 3. However, here we will say this: Machines do not need to have some fatal flaw that balances them out against humans, such that the indomitable human spirit has a chance of winning.

If dodo birds had possessed their own movie industry, then the scripts dodos wrote about the human invasion of their island of Mauritius might have had the humans’ guns and steel be compensated for by human disadvantages. Perhaps the humans’ intelligence-induced existential angst causes them to freeze up in despair at the last minute, just long enough for the heroic dodos to counterattack and peck them all to death.

This is perhaps a story that dodos would find satisfying: that intelligence cannot possibly be a net military advantage over strong beaks, that the humans’ bigger brains must have some fatal flaw that lets the proud dodos win after all.

In reality, the humans’ apparent advantages are *actual* advantages. The downsides of human brains are not *net *downsides in a military conflict with bird brains. The contest between humans and dodos ends up uneven, and that is that.

Even when humans fight other humans, machine guns are enough of an advantage that an army with machine guns usually beats an army without them. There are rare exceptions to this rule, and people love to recount them because the exception is a more fun story than the norm. But exceptions occur in real life much less often than they occur in stories.

We would predict the same about advanced AIs with vast memories and minds, that can copy themselves thousands of times over and think at ten thousand times the speed of a human; minds that can reason more validly, and generalize faster and more accurately from fewer harsh lessons, and improve themselves.

It’s not a trick question, and there will not be an amazing plot twist, much as we would like there to be.[Isn’t there something special about humans that mere machines could never emulate?→](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[Resources](/resources) › [Chapter 1](/1)[
### Is intelligence a meaningful concept?Yes. There’s a real phenomenon to describe, even if it’s difficult to pin down.4 min read](/1/is-intelligence-a-meaningful-concept)[
### Is “human-level intelligence” a meaningful concept?Yes, in many cases.2 min read](/1/is-human-level-intelligence-a-meaningful-concept)[
### Doesn’t intelligence consist of multiple skills?Yes, but there’s substantial overlap.1 min read](/1/doesnt-intelligence-consist-of-multiple-skills)[
### Isn’t intelligence overrated?Only if you’re using an overly narrow definition of “intelligence.”1 min read](/1/isnt-intelligence-overrated)[
### Is “general intelligence” a meaningful concept?Yes.5 min read](/1/is-general-intelligence-a-meaningful-concept)[
### Is “intelligence” a simple scalar quantity?No. But there are levels AI hasn’t reached.1 min read](/1/is-intelligence-a-simple-scalar-quantity)[
### Will AI cross critical thresholds and take off?Takeoff speed doesn’t affect the outcome, but the possibility of fast takeoff means we must act soon.7 min read](/1/will-ai-cross-critical-thresholds-and-take-off)[
### Isn’t ChatGPT already a general intelligence?There are many different things one might mean by “general intelligence.”6 min read](/1/isnt-chatgpt-already-a-general-intelligence)[
### How smart could a superintelligence get?Very smart.4 min read](/1/how-smart-could-a-superintelligence-get)[
### But aren’t there big obstacles to reaching superintelligence?The field is good at overcoming obstacles.4 min read](/1/but-arent-there-big-obstacles-to-reaching-superintelligence)[
### Isn’t it impossible to predict the behavior of a superintelligence?In some respects, but not in every respect.2 min read](/1/isnt-it-impossible-to-predict-the-behavior-of-a-superintelligence)[
### Won’t machines be fundamentally uncreative, or otherwise fatally flawed?No.2 min read](/1/wont-machines-be-fundamentally-uncreative-or-otherwise-fatally-flawed)[
### Isn’t there something special about humans that mere machines could never emulate?It seems unlikely, and not especially relevant.2 min read](/1/isnt-there-something-special-about-humans-that-mere-machines-could-never-emulate)[
### Are you saying machines will become conscious?Not necessarily, and this seems like a separate topic.5 min read](/1/are-you-saying-machines-will-become-conscious)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### More on Intelligence as Prediction and Steering](/1/more-on-intelligence-as-prediction-and-steering)[
### The Shallowness of Current AIs](/1/the-shallowness-of-current-ais)[
### Appreciating the Power of Intelligence](/1/appreciating-the-power-of-intelligence)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
