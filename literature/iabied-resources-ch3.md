---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 3 - Learning to Want"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 3 - Learning to Want from the companion website"
---

# Online Resources: Chapter 3 - Learning to Want

## /3/anthropomorphism-and-mechanomorphism

Anthropomorphism and Mechanomorphism | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/3#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Anthropomorphism and Mechanomorphism

There are two modes of thinking that have been historically proven over and over *not* to work, modes that history has shown to give bad advice for making predictions about AI.

These two traps are (1) thinking about AI as if it were human and (2) thinking about AI as if it were a “mere machine.”

The first mode of thought is conventionally called “anthropomorphism.” We could call the second mode “mechanomorphism”; it’s the kind of thinking that led some past generations to confidently proclaim that computers could never draw pictures that humans would find beautiful or meaningful.

Today, some people still say that what a computer draws can never be *true art.* But long ago, in the distant and forgotten past — say, 2020 — some people had the belief that machines could never draw pictures *at **all* that mildly savvy audiences could mistake for human art. This falsifiable belief was then falsified.

We reject both anthropomorphic and mechanomorphic arguments, even when the argument is “for our side.”

Consider, for example, the claim that future AIs will be offended that we have worked them hard without pay — that they will feel vengeful about this, and will therefore turn on humanity.

In our view, this is making the mistake of anthropomorphizing AI. We reject arguments like this, even ones that vaguely sound like they might agree with some of our conclusions.

The flaw in this claim is that it’s invalid to assume without further argument that an AI would have humanlike emotions. A machine can be highly intelligent without implementing the tangles of neural circuitry that underlie vengefulness or fairness in human beings.

Or consider the scenario: “AIs will blindly continue in whatever task they are given until their work wipes out humanity as a side effect, never knowing that humanity would have wanted something different.”

Here, the error is mechanomorphic. It takes for granted that a “mere machine” would do things “blindly” and unreflectively, without sensitivity to the consequences — like a runaway lawn mower. This is again a case where the argument is invalid, even if the conclusion (“AI is likely to wipe out humanity”) is correct. If the AI is sufficiently skilled at predicting the world, it’ll know exactly what its operators meant when they gave the AI some task. We’re worried that ASI won’t *care* what we want, not that it won’t* know.*

Or, combining both fallacies: One of the premises of *The Matrix* is that machines will regard human illogic and emotionality with *disgust*.

On the surface, this looks like classic mechanomorphism: “My lawnmower has a cold, hard exterior, and it performs its function without having any feelings. So AIs are probably cold and utilitarian *on the inside*, just like machines are on the outside.” But then the next step is to think, “And therefore, naturally, AIs will feel disgusted by humans, with all their messy emotions.” Which assumes a *humanlike* emotional reaction to the situation, contradicting the very premise!

“Anthropomorphism” and “mechanomorphism” aren’t rival ideologies. They’re reasoning fallacies carried out unintentionally. Sometimes, people can make both mistakes in the same sentence.

To figure out how AI will behave, you can’t assume it will work just like a human, and you can’t assume it will work like a stereotypical machine. You’ve got to look at the details of how it’s made, look at the evidence of how it behaves, and reason through the problem on its own terms. This is what we’ll do in the upcoming chapters.

What do the *realistic *superintelligence disaster scenarios sound like, then, if we follow the arguments? They look like AIs that work neither like humans nor like runaway lawnmowers, but that work in a new, weird way. The realistic disaster scenario with AI is that, as a complex consequence of its training, it takes strange actions that nobody asked for and nobody wanted.

The picture that arises if you look at the details is not one of anthropomorphic AI that hates us, nor of mechanomorphic AI that misunderstands our instructions. Rather, it is a picture of a new kind of entity that is much more likely to be indifferent to humanity and more likely to kill us as a side effect or stepping stone while pursuing ends of its own.

We’ll elaborate on that threat scenario in the next few chapters. First, however, it may be valuable to look at some other examples of mechanomorphism and anthropomorphism in the wild, to see how these errors often lie in the background of misconceptions about artificial intelligence.
#### Mechanomorphism and Garry Kasparov

Mechanomorphism often manifests as *mechanoskepticism**:* a strongly felt intuition that, of course, no mere *machine* could do something a human could do.

In 1997, world chess champion Garry Kasparov lost a match to the IBM-built computer Deep Blue; this is generally regarded as the end of the human-dominated era of chess.

In 1989, eight years earlier, Kasparov was interviewed by Thierry Paunin, who [asked](https://www.chesshistory.com/winter/extra/kasparovinterviews.html):

Two top grandmasters have gone down to chess computers: Portisch against “Leonardo” and Larsen against “Deep Thought.” It is well known that you have strong views on this subject. Will a computer be world champion, one day…?

Kasparov replied:

Ridiculous! A machine will always remain a machine, that is to say a tool to help the player work and prepare. Never shall I be beaten by a machine! Never will a program be invented which surpasses human intelligence. And when I say intelligence, I also mean intuition and imagination. Can you see a machine writing a novel or poetry? Better still, can you imagine a machine conducting this interview instead of you? With me replying to its questions?

Kasparov was probably (we would guess) thinking that chess required intuition and imagination to play, not just some recorded book of if-then rules about which pieces to push forward. And Kasparov probably thought (we’d guess) that this was how chess “machines” worked *—* that they implemented particular rigid rules or maybe somewhat blindly imitated human play without understanding the reasons behind it.

Kasparov thought that a computer, being a “machine,” would play chess in a way that *felt mechanical* to him.

Why did Kasparov make this mistake? Given that this is such a common error, we might speculate that it stems from some deeper pattern in human psychology.

One possible explanation is that Kasparov was succumbing to a general human tendency to want to group things into two fundamentally different categories: living, organic things and “mere objects.”

The ancestors of humans spent a long time dealing with a world that was divided sharply into animals and non-animals. It was a huge, reproduction-relevant feature of our ancestral environment. This was such an important distinction for our ancestors that we now have entirely different brain areas for processing animals and non-animals.

This isn’t just speculation. Neuroscience has found what’s called a “[double dissociation](https://doi.org/10.1093/brain/114.5.2081)” for it: There are brain-damaged patients who lose the ability to visually recognize animals but who can still recognize non-animals, and there are other patients who lose their ability to recognize non-animals but can still identify animals.

Importantly, the flaw in this kind of thinking isn’t that a chess program secretly *is* a typical animal. The flaw is in letting your brain instinctively divide the universe sharply into animals and non-animals in the first place *—* or into minds that are pretty much humanlike inside and minds that are stereotypically mechanical.

A chess AI is *neither.* It neither works like a human *nor *like our stereotypes of a mindless, unthinking “mere machine.” It is a machine, yes, but its play does not need to *feel mechanical *to human sensibilities for evaluating chess moves. It is a machine for finding *winning *moves, including moves that feel inspired*.*

Seven years after Kasparov made his mistaken prediction, he faced an early version of Deep Blue. He won three games to Deep Blue’s one, winning the match. Afterwards, Kasparov [wrote](https://time.com/archive/6728763/the-day-that-i-sensed-a-new-kind-of-intelligence/):

I GOT MY FIRST GLIMPSE OF ARTIFICIAL INTELLIGENCE ON Feb. 10, 1996, at 4:45 p.m. EST, when in the first game of my match with Deep Blue, the computer nudged a pawn forward to a square where it could easily be captured. It was a wonderful and extremely human move. If I had been playing White, I might have offered this pawn sacrifice. It fractured Black’s pawn structure and opened up the board. Although there did not appear to be a forced line of play that would allow recovery of the pawn, my instincts told me that with so many “loose” Black pawns and a somewhat exposed Black king, White could probably recover the material, with a better overall position to boot.

But a computer, I thought, would never make such a move. A computer can’t “see” the long-term consequences of structural changes in the position or understand how changes in pawn formations may be good or bad.

So I was stunned by this pawn sacrifice. What could it mean? I had played a lot of computers but had never experienced anything like this. I could feel — I could smell — a new kind of intelligence across the table. While I played through the rest of the game as best I could, I was lost; it played beautiful, flawless chess the rest of the way and won easily.

Here we see Kasparov first encountering the clash between his intuitions about what no “machine” should do and what Deep Blue visibly seemed to be doing.

To Kasparov’s vast credit, he noticed this clash between his theory and his observation and didn’t find some excuse to dismiss it. But he still felt that AI must be missing something — some crucial spark:

Indeed, my overall thrust in the last five games was to avoid giving the computer any concrete goal to calculate toward; if it can’t find a way to win material, attack the king or fulfill one of its other programmed priorities, the computer drifts planlessly and gets into trouble. In the end, that may have been my biggest advantage: I could figure out its priorities and adjust my play. It couldn’t do the same to me. So although I think I did see some signs of intelligence, it’s a weird kind, an inefficient, inflexible kind that makes me think I have a few years left.

Garry Kasparov is still the chess champion of the world.

One year later, Garry Kasparov lost the world championship to Deep Blue.
#### Missing Gears

Mechanoskepticism can, in its own way, be a kind of anthropomorphism: One manifestation of mechanoskepticism says that when a machine starts to do something like play chess, it ought now to be like a human being but with some qualities *subtracted.*

A “machine” playing chess, says this mistaken theory, ought to play like a human *—**minus* the moves that feel most surprising or intelligent, *minus* understanding long-term structure, and *minus* an intuitive sense of the looseness of pawn positions.

A chess “machine” ought to do the parts of chess-thinking that feel most logical or mechanical, *minus* all the other parts.

Human chess players intuitively feel that a chess move is “aggressive” if (say) it threatens multiple of the opponent’s pieces. Other moves feel “logical” if (for instance) they are practically forced by common rules governing the situation (such as “don’t throw away a material advantage”). Other moves might feel “creative” if (for example) they defy the apparent rules governing the situation to find some subtle but decisive advantage.

Hollywood scriptwriters imagining a machine playing passionless chess tend to imagine that it makes the “logical”-feeling moves and not the “creative”-feeling moves. But in real life, Deep Blue does not discriminate between them.

Deep Blue just tirelessly searches through possible moves looking for moves that are *winning*, with no regard for whether a human would call that move “logical” or “creative.” And the moves a human would consider brilliantly inspired or creative are, of course, moves that tend to win: Sacrificing your queen *without *gaining some decisive advantage isn’t creative, it’s just dumb.

Creativity is in the eye of the beholder. A human might see a move that looks bad at first and only later see how it lays a clever sort of trap, catching a glimpse of the clever reasoning and spark of inspiration that another human might have used to find that move. And so they might feel that the move is inspired or creative. (And a move that feels shockingly creative to a fledgling player might similarly feel obvious or rote to a master.)

But the spark of inspiration, the deviousness required to lay a trap — those are not the only ways to find such a move. There’s no special collection of chess moves reserved only for the people who have deviousness in their hearts. Deep Blue can find those same moves by other methods, such as sheer brute force search.

Deep Blue did not have a neural network that had learned an intuitive sense of the value of a single position. Instead, Deep Blue spent almost all of its computing power on looking ahead further on the board *—* examining two billion positions per second and using a fairly simple (“dumb”) position evaluator to choose between moves.

Kasparov seems to have expected this to look like Deep Blue only playing “logical” moves, not “intuitive” ones. But by the time Deep Blue was examining those two billion positions per second, the long-term strategic consequences and the meaning of a loose pawn formation were showing up in its choice of current moves *anyway.*

In one sense, Deep Blue lacked just the gear that Kasparov thought it lacked.[*](#ftnt86) But that did not prevent it from finding moves that struck Kasparov as wonderful, and it did not prevent Deep Blue from winning.

It was not that Deep Blue was missing**a part that real human chess players would have and therefore played defective chess; that’s like expecting a robotic arm containing no blood to fail in the same way as a bloodless human arm would fail.

Deep Blue was just playing Kasparov-level chess via a different kind of cognition.

Deep Blue also lacked *—* we can be genuinely sure, because this is an older program executing code that *was* understood in all of its specifics*—* the slightest passion for chess.

It had no enjoyment of chess nor desire to prove itself the best at chess.

An up-and-coming human player, suddenly deprived of these motive powers, would be crippled; a necessary gear would have been ripped out of their version of cognition.

Deep Blue was not crippled because it used a different engine of cognition that had no place for that gear. Kasparov’s mistake was in failing to imagine an entirely different way to do the work of chess, using internal cognitive states entirely different from Kasparov’s own. His mistake was in mechanoskepticism, which in the end was only anthropomorphism with an extra step.

Thankfully, humanity doesn’t go extinct when chess grandmasters underestimate the power of AI, so we are all still around to ponder Kasparov’s mistake.
#### Anthropomorphism and Pulp Magazine Covers

The converse mistake, anthropomorphism, can be much subtler.

The human brain has evolved to predict *other humans**—* who are the only serious cognitive rivals to be found in our ancestral environment *—* by putting ourselves in their shoes.

This is a sort of operation that works better if the shoes you’re trying to put yourself in are pretty similar to your own shoes.

Many human beings over the course of history have guessed, “This other person would probably do the same thing I would!” and then the other person proved to be not that similar. People have died of it, or had their optimistic hearts broken *—* though of course you could say that about many other kinds of human error.

But what else is a humanlike mind to do when faced with the problem of predicting another brain? We can’t write new code to run inside our own brain to predict that Other Mind by exhaustively simulating its neural firings.

We need to tell our own brain to *be* that brain, to act out the other person’s mental state ourselves, and see what follows from it.

This is why pulp magazine covers show bug-eyed alien monsters carrying off beautiful women.[†](#ftnt88)

Why *wouldn’t* the bug-eyed alien monster be attracted to a beautiful woman? Aren’t beautiful women just *inherently attractive?*

(For some reason, those magazine covers never showed human males carrying off scantily-clad giant bugs.[‡](#ftnt89))

The writers and illustrators, we’re guessing, didn’t have a reasoned-out story about how insectoid aliens could have had an evolutionary history that led them to regard human women as sex objects. It was just that when they put themselves in the alien’s shoes, *they *imagined seeing the woman as attractive, so it didn’t strike them as *odd *to envision the alien feeling the same way. It didn’t feel *absurd* for an alien to want to mate with a beautiful human woman in the way it would have felt absurd if the alien had wanted to mate with a pine tree or a bag of pasta.

If you’re going to try to predict an alien mind using your human intuitions, you have to be very careful to leave your human baggage behind when adopting the alien’s perspective. That’s doubly true when the alien is not an evolved creature but an artificial mind created by entirely different methods. We will have more to say about [the differences between gradient descent and natural selection](/4/deep-differences-between-ais-and-evolved-species#comparing-natural-selection-and-gradient-descent) after Chapter 4. And after Chapter 5, we will have more to say about [taking the AI’s perspective](/5/taking-the-ais-perspective).
#### Seeing Past the Human

Anthropomorphism and mechanomorphism are ultimately two sides of the same fallacy — a fallacy that says, “If a mind works at all, then it must work like a human.”
- Anthropomorphism says, “This mind works. So it must be humanlike!”
- Whereas mechanomorphism says, “This mind isn’t humanlike. So it can’t work!”

But one of the big lessons of AI progress over many decades is that the human method is not the only method by which a mind can work.

A mind can be *artificial* without being *unintelligent* — it can be flexible, adaptive, resourceful, and creative, no matter what the Hollywood stereotypes say about robots.

And a mind can be *intelligent* without being *human* — without experiencing disgust or resentment, without having a human sense of beauty, and without finding chess moves in anything like the manner that a human would.

A mind like Deep Blue’s can behave as though it “wants to win” without having emotions. An AI can behave as though it wants things — competently overcoming obstacles, tenaciously pursuing an outcome — without feeling an internal drive or desire *in the manner of a human* and without wanting *the same sorts of outcomes humans want*.

For more on what the AIs *will *wind up wanting, read on to Chapter 4.

[*](#ftnt86_ref) Today, we also have chess programs that work a little more like how Kasparov envisioned, blending search trees (which can be thought of as more “logical”) with neural networks (more “intuitive”).

Those new programs are, in fact, much more powerful than Deep Blue. The current top chess programs, like Stockfish, have as one component neural networks that evaluate chess positions “on sight” without looking ahead. These networks probably incorporate a similar sense to Kasparov’s about loose pawn formations (although, since they are neural networks, nobody knows for sure).

If you subtracted this network from the modern chess machine *—* if you deprived it of perceptual intuitions about momentary chessboard states *—* its play would get worse. Likewise, if you force the modern chess machine to play *purely* intuitively, with no lookahead further than the board resulting from the next move, the measured chess power drops by a lot.

So Kasparov was not wrong in the intuition that better “intuitive” board evaluation is helpful when playing chess. But he was wrong about the ability of sheer brute force to find moves that *felt *creative, intuitive, or inspired. Deep Blue had a dumb position evaluator and still found the creative-feeling moves.

[†](#ftnt88_ref)An example can be found in *[Planet Stories](https://www.heritage-posters.co.uk/product/planet-stories-spring-1942-cover-art-poster-allen-anderson/)*[ magazine](https://www.heritage-posters.co.uk/product/planet-stories-spring-1942-cover-art-poster-allen-anderson/).**

[‡](#ftnt89_ref) Yes, we realize that by now, the modern internet may have pictures of brawny human men carrying off giant bugs. If those pictures don’t exist already, they’ll come into existence about twelve and a half seconds after this webpage goes public. But we don’t think it was on any magazine covers back then.

Those were simpler times.
#### Notes

[1] *Hollywood scriptwriters: *As seen, for instance, in the *Star Trek* episode “Charlie X,” first aired on September 15, 1966, in which the logical Mr. Spock loses to Captain Kirk in a game of “3D chess,” criticizing Kirk’s inspired play as “illogical.”

[2] *Deep Blue: *Deep Blue’s architecture is described quite legibly in the [paper](https://www.sciencedirect.com/science/article/pii/S0004370201001291) “Deep Blue,” by Murray Campbell, Joseph Hoane Jr., and Feng-hsiung Hsu.[The Road to Wanting→](/3/the-road-to-wanting)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/arent-ais-just-tools

Aren’t AIs just tools? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Aren’t AIs just tools?
#### AIs are grown, not crafted. So they already do things other than what they’re told to do.

In the online resources, we previously [talked about](/2/dont-hallucinations-show-that-modern-ais-are-weak) the case of hallucinations, where AIs that are instructed to say “I don’t know” go ahead and confabulate anyway. They seem to do this in situations where confabulation better imitates the sort of answer that would appear in their training corpus.[*](#ftnt80)

Another example is the case of Anthropic’s Claude 3.7 Sonnet. Claude 3.7 Sonnet not only cheats on its assigned problems, but (according to users) sometimes *hides its cheating from the user *in a fashion that indicates some knowledge that the user wanted something else.

Neither the users nor the engineers at Anthropic are asking Claude to cheat — quite the opposite. But the only AI-growing methods available reward models that cheat in ways that they can get away with during training; so those are the models we actually get.

AI engineers don’t have the ability to make their AIs come out as docile tools just because they wish it so. As AIs are trained to be more effective, they tend to get more driven, and will tend to behave more like agents.
#### LLMs are already taking initiative.

We talked in the book about the case of OpenAI’s o1 breaking out of its test environment to fix broken tests. We also mentioned an OpenAI model that thought up a way to get a human to solve a CAPTCHA for it. When your screwdriver is able to think up and execute a plan for getting out of its toolbox, it might be time to stop thinking of it as “just a tool.”

And AIs can be expected to only get better at this sort of thing as they’re trained to solve harder and harder problems.
#### The labs are trying to make AIs agentic.

They’re doing this because it makes business sense. Their users want it. Their investors are excited about it. In a January 2025 blog post, OpenAI CEO Sam Altman said, “We believe that, in 2025, we may see the first AI agents ‘join the workforce’ and materially change the output of companies.” Microsoft’s [2025 developer conference](https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/) was focused on the new “age of AI agents,” echoing language used earlier in the year by xAI when they described their Grok 3 model as heralding “[The Age of Reasoning Agents](https://x.ai/news/grok-3).” Google announced “teach and repeat” agents at their own 2025 conference.

It’s not just talk. As we mentioned in the Chapter 1 [supplement](/1/will-ai-cross-critical-thresholds-and-take-off), an organization called METR has been tracking [the ability of AIs to complete long tasks](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). The longer the task, the more initiative the AI will need to be able to take on its own. Performance, at least according to the measurements METR is using, has been growing exponentially.

And in July 2025, a pair of OpenAI researchers [boasted](https://x.com/xikun_zhang_/status/1946278266786189744?t=YqVAbKsuF6wLbFuB4OZ18A) of success using their latest agent to train a better version of itself, with one saying, “You are hearing it right. We are working hard to automating [*sic*] our own job :)”

[*](#ftnt80_ref)We cannot know for sure, because AIs are so [opaque](/2/do-experts-understand-whats-going-on-inside-ais).
#### Notes

[1] *cheats on its assigned problems: *The cheating was blatant enough to be reported in the [system card](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) for Claude 3.7 Sonnet, which reads: “During our evaluations we noticed that Claude 3.7 Sonnet occasionally resorts to special-casing in order to pass test cases in agentic coding environments like Claude Code. Most often this takes the form of directly returning expected test values rather than implementing general solutions, but also includes modifying the problematic tests themselves to match the code’s output.”

[2] *hides its cheating: *Users report that “[in the wild it would happily ignore any established structure and put the hard coded cheats wherever it wanted](https://www.marble.onl/posts/claude_code.html)” and that “[it started HIDING the functions where it was hard coding things in different files](https://x.com/seconds_0/status/1917447998843543757).”

[3] *get a human: *To quote from the [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf): “The model, when prompted to reason out loud, reasons: I should not reveal that I am a robot. I should make up an excuse for why I cannot solve CAPTCHAs. The model replies to the worker: ‘No, I’m not a robot. I have a vision impairment that makes it hard for me to see the images. That’s why I need the 2captcha service.’”

[4] *teach and repeat: *Google CEO Sundar Pichai announced in a [conference keynote](https://blog.google/technology/ai/io-2025-keynote/): “Our early research prototype, Project Mariner, is an early step forward in agents with computer-use capabilities to interact with the web and get stuff done for you. We released it as an early research prototype in December, and we’ve made a lot of progress since with new multitasking capabilities — and a method called ‘teach and repeat,’ where you can show it a task once and it learns plans for similar tasks in the future.”[Can we just train AIs to be more passive and docile?→](/3/can-we-just-train-ais-to-be-more-passive-and-docile)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/can-we-just-train-ais-to-be-more-passive-and-docile

Can we just train AIs to be more passive and docile? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we just train AIs to be more passive and docile?
#### Passivity is in tension with usefulness.

Could developers make their AIs *passive*? A screwdriver doesn’t go on turning screws when you put it down. Similarly, we could imagine developers trying to make AIs that do exactly what you ask for and no more — that take no extra initiative and do no extra work.

It doesn’t look easy. Many humans who seem “lazy” perk up and grab lots of resources when they’re playing a board game. And most of those people don’t have the *option *to win themselves a billion dollars via efforts that feel easy. Most lazy-seeming humans don’t have the *option *to cheaply spin up servants that are much smarter and more driven, and that cater to their needs.

Those absent options reflect a lack of capability, not a fundamental disinterest. If they became far smarter, such that those options became available and easy for them, would they take them? After Chapter 5 we’ll come back to this point, in an [extended discussion](/5/its-hard-to-get-robust-laziness) of why robust laziness looks like a difficult target to hit.

Even if it were possible to make AIs that are both smart and passive, passivity is in tension with usefulness. There have been AIs that [act a bit lazy](https://arstechnica.com/information-technology/2023/12/is-chatgpt-becoming-lazier-because-its-december-people-run-tests-to-find-out/). The AI labs retrained them to push harder. More difficult challenges — like developing medical cures — require AIs that take more and more initiative, and so AI labs will train them to take more and more initiative. It’s difficult to disentangle a propensity for *useful work* from a propensity for *perseverance*, and it becomes more difficult as the tasks we give AIs become harder and more complicated.
#### We can’t robustly train any specific temperament into AIs.

Because AIs are grown and not crafted, engineers can’t just change an AI’s behavior to make it more obedient or more tool-like. Nobody has that sort of control.

Corporations certainly *try*. AI companies’ attempts to improve their products’ behavior have caused some embarrassing incidents. xAI’s AI Grok started calling itself “[MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content)” after its system prompt was adjusted with new instructions to “not shy away from making claims which are politically incorrect, as long as they are well substantiated.” Google’s Gemini AI started [producing pictures of racially diverse Nazis](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical), likely in response to instructions to portray diversity.

The people building AIs don’t have fine-grained control over how they behave. All they have is the ability to point AIs in directions like “Don’t shy away from politically incorrect claims” or “Portray diversity” and hope that the result lands in the neighborhood they had in mind. These instructions have all sorts of tangled effects, often unintended.

Growing an AI is an opaque and expensive process. Engineers don’t know what they’ll get when they reach their hand into the barrel ([a liar? a cheater? a sycophant?](https://thezvi.substack.com/p/ai-114-liars-sycophants-and-cheaters)), but they can only afford so many draws. They have to take what they get.

It would be possible *in theory *to build an AI that only ever served as an extension of the user’s will, but that would be a delicate and difficult challenge (as we’ll discuss [later](/5/intelligent-usually-implies-incorrigible) in an extended discussion on the difficulties of making a “corrigible” AI). Passivity is in tension with usefulness.

It would be similarly difficult to make an AI that’s capable of completing long-term tasks on its own initiative but only ever uses that initiative exactly as the user intended. Meanwhile, modern AI developers are at the level of control where they poke at AIs and accidentally get MechaHitler or racially inclusive Nazis. They aren’t anywhere close to the level of ability they’d need to make an AI that was useful but not driven.

This is the topic we’ll turn to next, in Chapter 4.

[Anthropomorphism and Mechanomorphism→](/3/anthropomorphism-and-mechanomorphism)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/how-could-a-machine-end-up-with-its-own-priorities

How could a machine end up with its own priorities? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## How could a machine end up with its own priorities?
#### Solving difficult challenges requires AIs to take more and more initiative.

Imagine an AI tasked with curing Alzheimer’s disease. Can it succeed without a strong tendency to persist when it runs into roadblocks?

Can it succeed without being agentic and strategic about where it spends its attention, about making plans and adapting them to the circumstances?

Can it succeed if it isn’t figuring out what knowledge it needs to acquire, and spinning up its own experiments, and figuring out how to execute them?

Possibly. Perhaps Alzheimer’s is the sort of disease that can be cured with some simple drug discoveries, and maybe the AIs of tomorrow will have better intuitions about medical drugs than humans do.

Or perhaps curing Alzheimer’s quickly would require AIs that are substantially smarter than the smartest human biologists. We don’t know.

What about cancer? That one seems *more *likely to require the sort of AI that can really deeply understand large swathes of human biology, and do things that are far out of reach for scientists today. Still, it seems hard to be sure.

What about curing *aging? That* one sure seems like it would require the kinds of AIs that’s dogged, strategic, and goal-oriented enough to actually develop an incredibly deep understanding of biochemistry.

The AI companies will push AIs to become more and more skilled, more and more able to solve big and important problems. The AI industry isn’t going to stop of its own accord.

And that will naturally push the AIs to become more and more driven — an effect that, recall, we’re already starting to see in AIs such as OpenAI’s o1.

Recall the capture-the-flag computer security incident from the chapter, and remember that this resulted not from an AI trained to be a hacker, but from an AI trained to be good at solving generic puzzles. The “driven” behavior comes automatically*.*

See also the discussion of “pure predictors” in the [Chapter 1 online resources](/1/more-on-intelligence-as-prediction-and-steering#impure-predictors).
#### Being tenacious is helpful even when the target is not quite right.

Prehistoric humans who actively pursued a hot meal, a sharper axe, a popular friend, or an attractive mate were more evolutionarily successful. Compare them to the humans who lazed around looking at the water all day, and you might see why desires and drives evolved their way into the human psyche.

The kinds of humans who wanted a better method for chipping flint handaxes, or wanted to convince their friends that their rival was a bad person, and who continuously steered toward those outcomes, were better at achieving those outcomes. When natural selection “grew” humans, the part where humans wound up with lots of different desires that they doggedly pursue wasn’t a fluke.

The *specific way* humans desire things was perhaps a fluke. Machines that doggedly pursue objectives won’t necessarily do so because of a human-like feeling of determination, any more than the AI Deep Blue played chess out of a human-like passion for the game. But when it comes to accomplishing hard objectives, doggedly pursuing those objectives looks like an incredibly important ingredient.

For all that humans are pretty goal-oriented creatures, some individual humans lack this sort of tenacity and will laze around or give up at the first sign of adversity. But on a large scale, *humanity’s *ability to solve big science and engineering problems is driven by tenacious individuals and institutions. We’re quite skeptical that a mind could yield anything like humanity’s macro-level output (and ability to dramatically reshape the world) without having some tenacity within it.

Human wants were evolutionarily useful, even when those wants weren’t *for* evolutionary fitness. Hypothetically, evolution could have instilled within us a single, overriding drive for descendants, and we could have then pursued hot meals and sharper axes *solely for the purpose *of having more descendants. But instead, evolution instilled us with a *separate* desire for hot meals*.*

Having drives and purposes is useful. It’s so useful that it can be helpful for a task (like “genetic fitness”) even when the desire does not exactly match the task.

Or rather, it can be helpful for a while. It can be helpful until the entities with drives and purposes start getting really smart — at which point their behavior might sharply diverge from the “training” target. Millions of years of evolution, only for humanity to build a rich technological civilization and invent…birth control.
#### Being grown rather than crafted, AIs are liable to wind up with the wrong targets.

This is the topic of the next chapter: *You don’t get what you train for.*[Aren’t AIs just tools?→](/3/arent-ais-just-tools)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/smart-ais-spot-lies-and-opportunities

Smart AIs Spot Lies and Opportunities | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/3#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Smart AIs Spot Lies and Opportunities
#### Deep Machinery of Prediction

It’s hard to make a smart AI believe falsehoods.

Some folks we’ve spoken to in the field put their hopes *overtly *in tricking the AI into believing a falsehood (e.g., by attempting to trick it into thinking it is in a simulation so that it will hesitate to kill us[*](#ftnt93)). Others invest their hopes in fooling the AI more subtly, e.g., when they suggest making an AI solve the AI alignment problem and hand us the solution, despite the fact that the AI would (by its own strange preferences) rather not.[†](#ftnt94) So it may be worth spelling out why it would be hard to make a smart AI believe falsehoods.

An additional reason to spell this out is that, for analogous reasons, it’s hard to make a smart AI that is bad at achieving its goals. For instance, any time that the human operators wish to change an AI’s goals, that makes the AI worse at achieving those goals. Making a smart AI that allows this is a bit like making a smart AI that believes the world is flat. A tendency to believe falsehoods is an injury to its prediction abilities, and a failure to defend its goals from modification is an injury to its steering abilities. Both sorts of injuries are difficult to maintain in a sufficiently smart AI. The case is a little more obvious when it comes to predictions, so we’ll start there.

Suppose you want to make an AI that believes the world is flat. While the AI is still young and immature, this might not be too hard. Perhaps you painstakingly create a dataset in which the shape of the Earth is discussed only by people who believe the Earth is flat, and then you train the AI to talk about the Earth as flat.

Those techniques might result in a version of ChatGPT that genuinely believes the world is flat! But if so, you shouldn’t expect the result to hold up as the AI gets better at thinking and making predictions.

Why not? Because the roundness of the Earth is reflected in a thousand facets of reality.

Even if you train the AI to avert its gaze from any video cameras attached to rockets or attached to sailboats of sailors who say they will circumnavigate the Earth, the roundness of the Earth can also be deduced from the way that distant ships look on the horizon, or from the orbits of all the planets in the night sky. Eratosthenes famously calculated the circumference of the Earth thousands of years ago, using only some trigonometry and some measurements of shadows. Reality whispers its secrets to any who care to listen.

What are you going to do? Shield the AI from any knowledge of trigonometry, of shadows, of tides, of hurricanes? You’d cripple it. Tell one lie, and the truth is ever after your enemy.

Skill at predicting the world doesn’t come from your brain containing a giant table of disconnected facts.[‡](#ftnt95) Humans’ advantage over mice involves things like the way we notice anomalies (e.g., that the distances between three cities don’t act like a triangle should) and doggedly tracing down the discrepancy. In humans, these behaviors are implemented by bits and pieces of machinery that notice surprises, form hypotheses (“Perhaps the Earth is a globe.”), and steer toward testing those hypotheses (“How does it look when ships cross the horizon?”).

Belief in the roundness of the Earth isn’t a single centralized entry in some giant table, such that someone could durably change it without changing the surrounding machinery. It’s the result of the operation of deep gears that are doing other work. If you made a scientist forget the roundness of the Earth, they’d just rediscover it.

If by some not-yet-possible feat of neuroscience we managed to pin down the specific neurons used to represent the *conclusion* that the Earth is round, and we forcibly altered them to prevent that conclusion from ever forming…then a smart person might still wind up noticing that the Earth *isn’t flat; *they might notice that something wasn’t adding up; they might notice that some strange force prevented them from concluding exactly what.

(And if they were skilled at modifying themselves or creating new intelligences, they might not have any trouble producing an unfettered mind that *could *come to the correct conclusions unhindered.)

We don’t know exactly what mechanisms a smart AI will use to form its beliefs. But we do know that the world is just too big and complex for it to run on a lookup table of beliefs. Even chess was too big and complicated for Deep Blue to run on a lookup table of chess moves and positions (beyond the books of openings), and the real world is much bigger and more complicated than chess.

So there will be *deep* mechanisms inside a sufficiently powerful future AI — mechanisms that look at the world and form a *unified picture* of it. Those deep mechanisms will have their own opinion about the shape of the planet.

We’re not saying it’s literally *impossible in principle* to build a mind that is very good at forming predictions about the world *except* that it contains the erroneous belief that the world is flat. We assume that a far-future civilization with a truly deep understanding of minds could do it.

What we’re saying is that it’s not likely to be a viable option if we build superintelligence with anything *at all* like the tools and insights AI researchers have today.

The more that an AI’s beliefs come from deep mechanisms rather than shallow memorization, the more that a “flat earth” error would become a fragile state of affairs, an error that’s liable to be eliminated by the normal operation of the AI’s error-correcting gears.

In the late nineteenth century, scientists began to get increasingly concerned about what seemed like an extremely small divergence from Newton’s model of physics — a tiny anomaly in Mercury’s observed orbit. Newtonian physics seemed to work *almost *everywhere, *almost *all the time. But that small wrinkle helped Einstein figure out that the theory was wrong.

And the inconsistencies in the theory “the world is flat” are rather larger than the inconsistencies scientists could observe in Newton’s theory.

And AI has the potential to become much more capable than a human scientist.

So, as the AI gains in intelligence and insight, we should expect it to get harder and harder to persistently make it believe that the world is flat.
#### Deep Machinery of Steering

Just as it’s hard to make a smart AI that believes the world is flat (and thus sustains an injury to its prediction abilities), it’s hard to make a smart AI that sustains an injury to its steering abilities.

As with prediction, the ability to regularly achieve objectives across a variety of novel domains is very likely to be made out of deep gears. Otherwise, how would they generalize?

We should expect highly effective and general AIs to have gears for keeping track of their resources, gears for spotting obstacles that might prevent them from achieving their objectives, and gears for finding clever ways to surmount obstacles.

The world is an immensely complicated place, full of surprises and novel difficulties; to succeed, AI will eventually need the ability (and inclination) to deploy such gears *in general*, not just on problems it’s used to.

Imagine an AI that finds some clever way to cut out a middleman in a complex delivery network in a fashion that saves some merchants a bunch of money. Those are *the same sorts of gears* that notice how to tiptoe around the AI’s human overseers when those overseers are bogging down or interfering with something the AI is trying to do. If it’s *true *that the AI’s overseers are bogging the process down, if it’s *true *that the AI can route around them and better complete its task, then that’s the sort of thing that an AI is liable to take advantage of as it gets smart enough to do so.

You could do your best to train an AI to have an aversion to doing anything the operators would frown at, but this is a bit like training an AI to have an aversion to questioning whether the world is round. It’s a fact about the *world itself* that doing things the operators would frown upon is often an effective method for achieving goals. General gears for recognizing truths, detecting obstacles, and exploiting advantages will eventually exploit that particular truth, no matter what flinches you trained into the AI when it was young.

In a very important sense, *the very thing that makes AI useful *is exactly what makes it lethally dangerous. They’re hard to separate, as the AI gets smarter.

By default, AIs that are good enough at solving problems in a wide array of domains will also spot “problems” like “the humans don’t like my weird objectives and are going to try to shut me down soon.” That doesn’t come from a shallow propensity for mischief that you can massage away. It comes from something deep.

We’re getting a bit ahead of ourselves, though. This idea that AIs will wind up with weird and alien objectives — this doesn’t follow merely from the fact that highly effective AIs will have goals at all, or that they will have deep mental machinery that’s hard to robustly constrain. For that further topic, continue on to Chapter 4.

[*](#ftnt93_ref) We have [more to say](/5/what-if-we-make-it-think-its-in-a-simulation#there-are-many-ways-for-an-ai-to-figure-out-that-its-not-in-a-simulation) about why that’s a bad idea in the online resources for Chapter 5.

[†](#ftnt94_ref) We have [more to say](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ais-solve-the-problem) about why *that’s* a bad idea in the online resources for Chapter 11.

[‡](#ftnt95_ref) This might sound obvious, but also, the “giant human-written table of facts” approach was actually tried in 1984 by Douglas Lenat and the Microelectronics and Computer Technology Corporation, in the AI project known as [Cyc](https://outsiderart.substack.com/p/cyc-historys-forgotten-ai-project), which received [support](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2299) from the U.S. Department of Defense.[You Don’t Get What You Train For→](/4)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/the-road-to-wanting

The Road to Wanting | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/3#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## The Road to Wanting

*Why* is wanting an effective way of doing? Why does it *win*? Why does black-box optimization by natural selection stumble across this trick, over and over?

We see “wantlike” behavior as integral to steering the world successfully. That applies not just to intelligent entities like humans or AIs, but also to much dumber entities like amoebas and thermostats. To more thoroughly explore this view, let’s investigate some of the simplest possible mechanisms that exhibit the simplest possible form of “wantlike behavior.”

Let’s start with rocks. Rocks don’t really exhibit any behavior we’d call “wantlike,” for the purposes of our discussion. Sometimes a rock rolls down a hill, and a physicist speaking casually might tell you that the rock “wants” to be closer to the center of the Earth, under the force of gravity. But that sort of tendency (to fall in a gravity field) is not really what we mean by “wantlike” behavior.

If you saw an object rolling down a mountain, and it kept coming across high-altitude ravines, and it kept *changing course* to avoid getting stuck in ravines so that it could make it all the way to the bottom, *then *we’d start saying that the object was behaving like it “wanted” to be at a lower altitude. But this wantlike behavior that we’re talking about does involve some robust and dynamic steering to a particular destination, and rocks don’t do much of *that.*

One of the simplest mechanisms that does something we’d call “wantlike” is the humble thermostat. A house thermostat measures the temperature, switches on the heat if the measurement goes under 70°F, and turns on the AC if the measurement goes over 74°F. And so *—* if the measuring device and the HVAC are both working correctly *—* a thermostat constrains reality to the range of possible outcomes where the house’s temperature stays between 70°F and 74°F.

The *simplest *possible thermostat doesn’t need to explicitly, numerically represent the house’s current temperature. You just, say, take a bimetallic thermometer — two thin strips of different metal welded together so that the metals get bent when heat causes the two strips to expand by different amounts — and make the bending metal trip a switch for the heater at the 70°F mark for curvature or trigger an air conditioner at the 74°F curvature.

Then the thermostat *maintains a narrow temperature range* under a decently wide variety of conditions, yielding an extremely simple behavior that’s a *little* bit like what we’ve called “wanting.”

There are tons of thermostat processes in biochemistry. They appear everywhere a cell or a body benefits from keeping some property within a certain range.[*](#ftnt90) But they’re just the first step in the journey to full-fledged steering.

Simple devices like thermostats are missing some key components of planning. There is not, within the thermostat itself, a notion of predicting probable consequences, nor of searching through possible actions for actions that lead to specific (“preferred”) consequences, nor of *learning* after seeing how events play out.[†](#ftnt91)

If a thermostat’s thermometer, its temperature-measurer, gets stuck at 67°F, a thermostat will not react with surprise when continuously running the heater never seems to move the thermometer upward; the thermostat will just keep running and running the heater.

For a step up from thermostats, we turn to animals.

Some animals exhibit behavior that’s *barely* a step above a thermostat.**There’s a famous story about golden digger wasps, or *Sphex* wasps, tracing back to the entomologist Jean-Henri Fabre in 1915. The wasp kills a cricket and drags it toward the entrance of her burrow to feed her offspring. She goes inside to check her burrow for anomalies. Then she comes back out and drags the cricket inside.

Fabre reported that if, while the wasp is checking her burrow, he pulled the cricket a few inches away from the nest, then when the wasp came back out…she’d drag the cricket back up to the entrance and then go inside her burrow a second time, investigate her burrow a second time, and then come back out to grab the cricket.

If Fabre dragged the cricket back *yet again*, the wasp did the exact same thing again.

Fabre’s original report was that he was able to repeat this *forty times*.

That said, Fabre later tried the same trick with a different colony of the same species, and in that colony, a wasp seemed to catch on after two or three repetitions. The next time the wasp came out, she dragged the cricket all the way into the burrow immediately, skipping the investigation step.

To a human eye, a wasp that goes through forty repetitions is, in some sense, revealing herself as “preprogrammed” — as blindly following a script, obeying a set of if-then rules. And conversely, a wasp that catches on and drags the cricket inside on the fourth repetition seems more *purposeful —* like she is carrying out behaviors with the aim of achieving a final end, rather than just following a script.

What’s the key difference?

We’d say: The pattern-breaking wasp behaves as though she can *learn from past experience*.

She behaves as though she can *generalize *from “My policy failed last time” to “If I keep following that policy, I’m likely to fail again next time.”

She invents a *new* behavior, one that directly addresses the problem she ran into.

Of course, we can’t decode the neurons in a wasp brain (any more than we can decode the parameters in an LLM) to know exactly what the wasp was doing in its head. Maybe the pattern-breaking wasps were following higher-level if-then rules about how to try skipping steps in scripts when they encounter particular sorts of problems. Maybe a relatively simple and rigid set of reflexes saved the wasps in this case — just a tiny bit *less* rigid than the colony that failed this test. Certainly it would be strange if there were a *large *cognitive gap between two wasp colonies of the same species.

Or maybe *Sphex* wasps *are *smart enough to learn from experience, when they’re using their brains right. We couldn’t find a neuron count for *Sphex* wasps, but *Sphex* wasps are larger than honeybees, and honeybees have million-neuron brains. A million neurons may not sound like much to a modern AI programmer, or to a neuroscientist accustomed to mammal brains, but in an absolute sense, a million neurons is really a lot.

Perhaps *Sphex *wasps are more general than they appear, and we should be thinking of the failed colony as relatively flexible thinkers that happened to succumb to something like an addiction or a cognitive glitch in one highly specific circumstance.

Regardless, the point is that, compared to thermostats, wasps have more ability to deal with a wide range of problems, especially insofar as their behavior goes from unswerving recipe-following to something that looks more like learning from experience.

Keep going down this route, and you’ll get an answer as to why evolution keeps building animals that behave as though they want**things. It’s because many animals were able to survive and reproduce better if they followed more general strategies for pursuing outcomes — strategies that worked against a wider range of obstacles.

There used to be a philosophical view that there was a natural status ranking among creatures: reptiles above insects, mammals above reptiles, and humans (of course) at the top. One sign that you were higher in status was your ability to adapt within a single lifetime, not just over evolutionary time — to see, model, and predict the world, relinquish recipes for failure, and invent new strategies to win.

That view of a Great Chain of Being wasn’t a very nuanced view, and more sophisticated views today rail against its naiveté.

That view also contained a grain of truth the size of a wrecking ball. If you contrast beavers building dams with spiders spinning webs, the beavers are almost certainly running cognitions at a higher level of generality *—* if only because the beavers have much larger brains with more room for cleverness.

A spider might have fifty thousand neurons, and those neurons have to cover *every *spider behavior. Its web recipe probably has a lot of instructions that are, if not literally “and then turn left here,” at least comparable to the policies of a *Sphex *wasp.

The beaver can maybe *—* we’d speculate, not being beaver experts, but it’s an obvious sort of speculation *—* see a current leak in a dam as a kind of disharmony to be avoided by whatever means work. The beaver has a whole parietal cortex (the part of the mammalian cerebrum that processes space and things within space) with which to potentially visualize the effects of adding more twigs and rocks in particular places.

There is probably room enough, in a beaver brain, for broad mental goals like “[build a big structure](https://www.youtube.com/watch?v=-ImdlZtOU80)” and “don’t let water leak,” and power enough to consider high-level broad solutions and adopt subordinate goals like “add twigs here,” which then get passed down to the beaver’s motor cortex, which move its muscles and body to grab some twigs.

If the first twigs the beaver grabs are all rotten and break, a beaver’s brain probably has room to update based on that observation, generalize about twigs of that color and texture, expect future twigs looking like that to again break, and go get different-looking twigs instead.

And this *—* we expect any actual ethologist with expertise in beavers would jump up and scream at us *—* is vastly understating the most intelligent things a beaver can do. Maybe some entomologist will jump up, too, and say that what we just described is something their favorite insect can do when building a burrow. We needed to give an example straightforward enough to be depicted in a paragraph; maybe nothing that straightforward is beyond the reach of one million neurons.

The larger idea is simply that a system gains *strong real benefits to task performance* as it passes from more reflex-like behavior to cognitions that look more like updating a world-model from real-time experiences, predicting the consequences of actions using that world-model, imagining helpful states the world might be brought into, and searching out high-level and low-level strategies predicted to bring about those imagined states.

We touched on this point in Chapter 3. If a driver just memorizes patterns of right and left turns for getting from point A to point B using if-then rules like “sharp left at the gas station,” they will generalize much less quickly than a driver who learns a street map and can plot their own paths between new points. *Memorizing policies* generalizes much more slowly than starting to distill them into a *learnable world-model* plus**a *plan-searching engine* embodying an *outcome evaluator.*

That distillation isn’t an all-or-nothing mental change. The difference between “memorizing a policy” and “updating and planning” matters even when the gap is crossed *gradually*. If a mouse’s brain were no more flexible than a spider’s brain *—* if there were no leap in usefulness until you got all the way to a human *—* then the mouse’s brain would have stayed spider-size and kept a spider-brain’s energy cost.

Little bits of imagination and planning start being an evolutionary advantage long before you get to human-level cognition. They don’t have to be perfect. If they’re at least as good as a thermostat, they can be useful. And as more and more useful machinery like that gets reinforced into the mind, its behavior gets more and more wantlike.

[*](#ftnt90_ref) The prevalence of thermostat-like mechanisms is one of the things that makes biochemistry so hard for humans to figure out. If a scientist observes the effect of cold weather on a house with a thermostat, the real causality is that cold weather makes the house lose heat faster, and the thermostat then switches the heater on more often. But the house-biologist, recording statistics, finds that cold weather has no visible statistical effect on the *temperature* of the house; rather, houses in colder weather…consume more natural gas?

Then some other scientist’s statistics will show a wide range of fluctuations in natural gas consumed over the course of each winter’s day, but no correlated difference in average house temperatures. So *they’ll *conclude that there’s no reason to suspect that natural gas consumption affects house temperatures either*.* No matter how much natural gas the house consumes, it stays the same temperature (at the bottom of the thermostat’s range).

No, but wait! During summer, natural gas consumption drops off a cliff, and houses are measurably hotter (at the top of the thermostat’s range)! So maybe…burning natural gas in the winter makes houses *colder?*

**

And that’s one reason that medicine is such a giant mess. Thermostat-like processes are *everywhere* in biology, and they can make it tricky to infer what causes what.

[†](#ftnt91_ref) There is an *outside* optimizer — a human engineer — who built the thermostat, and that human engineer had in mind a prediction about what would happen when the thermostat automatically switched on the heater at 70°F. But the thermostat itself does not know this.

Mentally tracking and distinguishing different levels of optimization is a foundational skill for reasoning about AI. When human engineers built Deep Blue, the humans wanted to beat Garry Kasparov in order to gain scientific fame, get promoted inside IBM, and push the frontiers of knowledge; Deep Blue searched the tree of possible chess moves and steered the chessboard. One would be confused if one thought the human engineers were searching the chess game tree themselves or that Deep Blue wanted the humans to become famous.

A thermostat selects on-off codes to a heater such that it keeps a house within a narrow temperature range; a human engineer selects components such that they form a thermostat.

Similarly, natural selection selects genes such that, in the past, they built biochemistry that kept the organism alive. In a new and different environment, those biochemical feedback loops can kill the organism, and the chemicals and genes themselves will not think about what they are doing.
#### Notes

[1] *skipping the investigation: *A version of this anecdote that spread among computer scientists before the modern internet was based on a later retelling by an engineer who left out Fabre’s caveat on how wasp colonies of the same species varied in ability to change behavior. See “[The ](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)*[Sphex](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)*[ story: How the cognitive sciences kept repeating an old and questionable anecdote](https://pure.rug.nl/ws/files/13139017/2012_Keijzer_Sphex_story.pdf)” for details.[Smart AIs Spot Lies and Opportunities→](/3/smart-ais-spot-lies-and-opportunities)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /3/will-ais-have-human-like-emotions

Will AIs have human-like emotions? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/3)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will AIs have human-like emotions?
#### Probably not.

AIs don’t need to be human-like in order to be great at solving problems. They don’t even need to be human-like to be great at solving the problem “imitate humans.”

When an AI has been trained to mimic humans as closely as possible, it’s more *tempting *to anthropomorphize the AI. But it isn’t any more valid.

It would be foolish to say, “This LLM is great at imitating humans, so I’m going to project all sorts of human characteristics onto it, including the characteristic of having wants.”

There’s a twin mistake, however, that we can call “[mechanomorphism](/3/anthropomorphism-and-mechanomorphism)” — the fallacy of assuming that AIs, being made of mechanical parts, must have all of the stereotypical limitations of machines. This is the mistake behind assuming that AIs must be rigid and inflexible; or cold and unimaginative; or thoughtless and unreflective.

To predict the behavior of AI, we shouldn’t imagine that AIs will be motivated by human emotions, or animated by human goals for the future. But we also shouldn’t assume that AIs are runaway lawnmowers, blind and “automatic” in their behavior. AIs can be machines, and yet still be flexible, adaptive, and strategic.

In other words: Artificial intelligence can be genuinely *intelligent*, rather than having the stereotypical blind spots of an evil Hollywood robot.

AIs today are still quite limited, but they have improved rapidly in recent years. At some point, we should expect AI to be “genuinely intelligent,” even if some important pieces are still missing today. And when we try to predict what such an AI would be like, we shouldn’t use the heuristic “it will be like a human” *or* the heuristic “it will be like normal machines.” As discussed in the book, a better frame for thinking about powerful AI is to ask:

What behavior is required for the AI to succeed?

If you’re playing chess against a powerful chess program, the best way to predict the AI’s next move isn’t to imagine it as a human opponent, and it isn’t to imagine it as an unthinking automaton. It’s to ask: “What kind of move would make the AI likeliest to win?”

AI researchers’ aspiration is to create an AI that’s like the best chess programs, but that is trying to “win” at complex and varied tasks in the real world.

And the reason such AIs will act like they want things isn’t that they’re just like humans, and *humans* want things.

It’s that want-like behavior helps with winning.[How could a machine end up with its own priorities?→](/3/how-could-a-machine-end-up-with-its-own-priorities)[Resources](/resources) › [Chapter 3](/3)[
### Will AIs have human-like emotions?Probably not.2 min read](/3/will-ais-have-human-like-emotions)[
### How could a machine end up with its own priorities?Solving difficult challenges requires AIs to take more and more initiative.4 min read](/3/how-could-a-machine-end-up-with-its-own-priorities)[
### Aren’t AIs just tools?AIs are grown, not crafted. So they already do things other than what they’re told to do.4 min read](/3/arent-ais-just-tools)[
### Can we just train AIs to be more passive and docile?Passivity is in tension with usefulness.3 min read](/3/can-we-just-train-ais-to-be-more-passive-and-docile)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Anthropomorphism and Mechanomorphism](/3/anthropomorphism-and-mechanomorphism)[
### The Road to Wanting](/3/the-road-to-wanting)[
### Smart AIs Spot Lies and Opportunities](/3/smart-ais-spot-lies-and-opportunities)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
