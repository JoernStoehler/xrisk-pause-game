# Literature Index

<!-- One line per resource: filename | title | author | year | encrypted? -->
<!-- Keep alphabetical by filename -->

| File | Title | Author | Year | Encrypted |
|------|-------|--------|------|-----------|
| ai-2027.md | AI 2027 | Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland, Romeo Dean | 2025 | No |
| ai-economic-impact-jobs-2024-2025.md | AI Economic Impact on Jobs and the Economy | Various (Goldman Sachs, McKinsey, IMF, academic researchers) | 2024-2025 | No |
| ai-incident-database-2024-2025.md | AI Incident Database: Notable Incidents 2024-2025 | Various (AIAAIC, public reports) | 2024-2025 | No |
| ai-investment-bubble-2024-2025.md | AI Investment and Stock Market Signals 2024-2025 | Various (financial reports, analyst commentary) | 2024-2025 | No |
| aisi-frontier-trends-2025.md | Frontier AI Trends Report | UK AI Security Institute (AISI) | 2025 | No |
| altman-intelligence-age.md | The Intelligence Age | Sam Altman | 2024 | No |
| anthropic-alignment-faking.md | Alignment Faking in Large Language Models | Ryan Greenblatt, Carson Denison, Benjamin Wright, et al. | 2024 | No |
| anthropic-alignment-increasingly-solvable.md | Alignment is Not Solved But Increasingly Looks Solvable | Jan Leike | 2026 | No |
| anthropic-reward-hacking.md | Natural Emergent Misalignment from Reward Hacking in Production RL | Monte MacDiarmid, Benjamin Wright, Jonathan Uesato, et al. | 2025 | No |
| anthropic-rsp-v2-2024.md | Anthropic Responsible Scaling Policy v2 | Anthropic | 2024 | No |
| anthropic-sleeper-agents.md | Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training | Evan Hubinger, Carson Denison, Jesse Mu, et al. | 2024 | No |
| apollo-incontext-scheming.md | Frontier Models are Capable of In-Context Scheming | Apollo Research | 2024 | No |
| brookings-public-opinion-ai-governance-2024.md | What the Public Thinks About AI and the Implications for Governance | Brookings Institution | 2024 | No |
| carnegie-california-ai-survey-2025.md | Carnegie California AI Survey | Carnegie Endowment for International Peace | 2025 | No |
| cotra-ai-defeat-all-combined.md | AI Could Defeat All Of Us Combined | Holden Karnofsky | 2022 | No |
| cotra-ai-takeover-default.md | Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover | Ajeya Cotra | 2022 | No |
| dario-amodei-machines-of-loving-grace.md | Machines of Loving Grace | Dario Amodei | 2024 | No |
| deepmind-frontier-safety-framework-v3-2025.md | Google DeepMind Frontier Safety Framework v3 | Google DeepMind | 2025 | No |
| eu-ai-act-summary.md | EU AI Act: Comprehensive Summary | European Commission / European Parliament | 2024 | No |
| future-of-life-ai-regulation-survey-2025.md | The U.S. Public Wants Regulation (or Prohibition) of Expert-Level and Superhuman AI | Future of Life Institute | 2025 | No |
| gallup-ai-concerns-2024.md | Americans Express Real Concerns About Artificial Intelligence | Gallup | 2024 | No |
| gallup-ai-safety-data-security-2025.md | Americans Prioritize AI Safety and Data Security | Gallup | 2025 | No |
| gwern-acre.md | The Ones Who Walk Towards Acre | Gwern Branwen | 2024 | No |
| gwern-clippy-takeover.md | It Looks Like You're Trying To Take Over The World | Gwern Branwen | 2024 | No |
| gwern-fermi-paradox.md | A Fermi Paradox Story | Gwern Branwen | 2011 | No |
| iabied-book.md | If Anyone Builds It, Everyone Dies | Eliezer Yudkowsky, Nate Soares | 2025 | Yes |
| iabied-errata.md | If Anyone Builds It, Everyone Dies - Errata | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch1.md | IABIED Online Resources: Chapter 1 - Humanity's Special Power | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch10.md | IABIED Online Resources: Chapter 10 - A Cursed Problem | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch11.md | IABIED Online Resources: Chapter 11 - An Alchemy, Not a Science | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch12.md | IABIED Online Resources: Chapter 12 - "I Don't Want to Be Alarmist" | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch13.md | IABIED Online Resources: Chapter 13 - Shut It Down | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch2.md | IABIED Online Resources: Chapter 2 - Grown, Not Crafted | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch3.md | IABIED Online Resources: Chapter 3 - Learning to Want | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch4.md | IABIED Online Resources: Chapter 4 - You Don't Get What You Train For | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch5.md | IABIED Online Resources: Chapter 5 - Its Favorite Things | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-ch6.md | IABIED Online Resources: Chapter 6 - We'd Lose | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-intro.md | IABIED Online Resources: Introduction | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-resources-part2.md | IABIED Online Resources: Part II - One Extinction Scenario | Eliezer Yudkowsky, Nate Soares | 2025 | No |
| iabied-treaty.md | A Tentative Draft of a Treaty, with Annotations | Eliezer Yudkowsky, Nate Soares, MIRI Technical Governance Team | 2025 | No |
| jan-leike-openai-departure-2024.md | Jan Leike's Departure from OpenAI | Jan Leike (public statements) | 2024 | No |
| metr-gpt5-eval.md | METR's Evaluation of OpenAI GPT-5 | METR (Model Evaluation & Threat Research) | 2025 | No |
| miri-agent-foundations-agenda-2014.md | Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda | Nate Soares, Benya Fallenstein | 2014 | No |
| miri-communications-strategy-2024.md | MIRI Communications Strategy and Approach | MIRI / Eliezer Yudkowsky / Nate Soares | 2024 | No |
| miri-corrigibility-2015.md | Corrigibility | Nate Soares, Benja Fallenstein, Eliezer Yudkowsky, Stuart Armstrong | 2015 | No |
| miri-death-with-dignity.md | MIRI announces new 'Death With Dignity' strategy | Eliezer Yudkowsky | 2022 | No |
| openai-preparedness-framework-v2-2025.md | OpenAI Preparedness Framework v2 | OpenAI | 2025 | No |
| pew-ai-global-views-2025.md | How People Around the World View AI | Pew Research Center | 2025 | No |
| pew-ai-public-vs-expert-views-2025.md | How the US Public and AI Experts View Artificial Intelligence | Pew Research Center | 2025 | No |
| political-statements-ai-risk.md | Key Political Statements on AI Risk (2023-2025) | Various government officials and international bodies | 2023-2025 | No |
| public-citizen-voter-support-ai-regulation-2025.md | Years of Polling Show Overwhelming Voter Support for a Crackdown on AI | Public Citizen | 2025 | No |
| rand-ai-model-weight-security-2024.md | Securing AI Model Weights | RAND Corporation | 2024 | No |
| sb1047-analysis.md | California SB 1047 Analysis | Various (Carnegie Endowment, Brookings, legal analyses) | 2024 | No |
| us-ai-executive-orders.md | US Executive Orders on AI | The White House / US Government | 2023-2025 | No |
| yudkowsky-agi-ruin-lethalities.md | AGI Ruin: A List of Lethalities | Eliezer Yudkowsky | 2022 | No |
| yudkowsky-time-shut-it-down.md | Pausing AI Developments Isn't Enough. We Need to Shut It All Down | Eliezer Yudkowsky (TIME) | 2023 | No |
