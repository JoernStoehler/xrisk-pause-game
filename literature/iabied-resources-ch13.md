---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 13 - Shut It Down"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 13 - Shut It Down from the companion website"
---

# Online Resources: Chapter 13 - Shut It Down

## /13/a-tentative-draft-of-a-treaty-with-annotations

A Tentative Draft of a Treaty, with Annotations | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/13#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## A Tentative Draft of a Treaty, with Annotations

Many people, including members of the U.S. House of Representatives and the Senate, have asked us what concrete and specific legislative proposals would actually help with this problem.

We are not policy experts, and we see many possible answers to that question, depending on whether you’re looking for legislation that is easier to pass today but less directly useful (functioning more as stepping stones to more ambitious legislation) or proposals that seem harder to pass but which would substantially address the core issues.

We have much more to say on the latter count than the former. To that end, MIRI’s technical governance team has put together a preliminary draft of one such proposal.

This sketch of a treaty is designed for a world where world leaders have woken up to the realities of smarter-than-human AI. It’s not the sort of thing that we predict would be passed tomorrow (as we write this), but it might not be so hard to get such a treaty ratified once the world is more aware of the danger — a process that is [already beginning](/13/will-elected-officials-recognize-this-as-a-real-threat)and which will hopefully continue.

[The treaty can be found here](/treaty), with many annotations. The drafting process drew deeply on historical precedent from other treaties, which are noted alongside the draft. We stress again that this is a starting point, not an ending point. We aren’t policy experts, and we possibly made some foolish mistakes. Nevertheless, we hope this draft can serve as an inspiration and example of how such a treaty would be possible and in keeping with similar past legislative efforts.[Keep the Coalition Large→](/13/keep-the-coalition-large)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/aligned-to-whom

“Aligned to whom?” | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## “Aligned to whom?”
#### This is a thorny question. Regardless of the answer, we need to halt development.

If humanity builds a superintelligence someday, we should make sure that it’s “aligned” with human values. But with the values of which humans, exactly? People disagree enormously about right and wrong, about religion, about social norms, about policy tradeoffs, etc.

At present, this question is moot. Humanity isn’t able to get *any *particular goals into an AI, so it doesn’t matter whether there’s disagreement about *which* goals would be ideal. As we’ve argued at length, rushing to build superintelligence would get all of us killed. Humanity disagrees about a lot of things, but most people don’t disagree about whether the destruction of all life on Earth is a good thing.

The problem of which values *exactly *should be loaded into an AI seems like a thorny problem. It’s a problem that, frankly, we would love to have. What we face instead is a different and far worse problem.

We don’t need to agree *at all *about “aligned to whom?” (or even about whether humanity should ever build superintelligence) in order to coordinate on an international ban, on the brutally simple grounds that we’re going to die otherwise. There are an endless number of interesting philosophical questions occasioned by AI, but if we let ourselves get unduly distracted by these, we’re liable to get our kids killed in the process.

In practical terms, our advice to world leaders is:
- 

Separate out the question of “Should we rush ahead to build superintelligence?” from the question of “If we somehow had a way to build superintelligence safely, what should we do with it?” and focus on the first question first. The first question is the urgent one, and the one that’s actionable today. The second question may be important to address someday, but at present it’s a trap, because it encourages thinking of superintelligence as a prize. Falsely believing that the first person who builds a superintelligence gets to decide what to do with it would lead us into a suicide race.

ASI is a suicide button, and not a genie in a lamp. When someone creates a superintelligence, they don’t thereby “have” that superintelligence. Rather, the superintelligence they just created has a planet.
- 

If for some reason you do feel the need in the future to broach the topic of “How should humanity someday use superintelligence, if we’re ever in a position to do so?”, we strongly recommend avoiding proposals or ideas that would encourage other actors to race (or that would otherwise encourage nations to reject or violate any future international agreements on superintelligence). Anything like a winner-takes-all dynamic has enormous potential to endanger the world.

There exist proposals for managing the thorny question of “aligned to whom” in a relatively universalistic way that attempts to be fair to all potential stakeholders and that does not incentivize racing over the finish line — e.g., the proposal of aligning an AI to pursue the coherent extrapolated volition of all humankind.[*](#ftnt304) But even there, there’s endless potential for people to argue over the principles and tradeoffs involved, as well as the thorny implementation details. Those arguments would be important to resolve in a world where humanity *had *figured out how to precisely and robustly aim a superintelligence, but putting them front and center today wildly mischaracterizes the actual tradeoffs the world is facing, and risks derailing efforts to coordinate on shared goals such as avoiding the destruction of the Earth.

Even when it comes to issues that are of enormous long-term importance, [nothing should be packaged with the survival of humanity except the survival of humanity](/13/keep-the-coalition-large).

[*](#ftnt304_ref)[Coherent extrapolated volition](https://baserates-test.vercel.app/w/coherent-extrapolated-volition-alignment-target) is our own stab at answering the question “aligned to whom?” if and when we get to a point where the creators of AIs have some ability to aim them. Coherent extrapolated volition attempts to resolve moral disagreements and meta-moral disagreements mostly by tasking the AI with identifying places where people would converge if they knew more, if they were more the kind of person they wish they were, and so on (in the manner of [ideal observer theories](https://en.wikipedia.org/wiki/Ideal_observer_theory) in ethics), and searching for shared meta-principles that the AI can fall back on in cases where there’s truly foundational moral disagreement. (Where the goal isn’t necessarily for the AI to “solve all problems” in human life; just solve *enough *problems that the end result isn’t likely to be catastrophically bad.)

We would recommend extrapolating the volition of all living humans — not because we think this is some sort of ideal, but because it’s the obvious default coordination point around which many disagreeing stakeholders can agree (and because other entities that living humans care about get some sway through those living humans’ volition; and so too for other entities that living humans *would *care about if they knew more and were more who they wished to be and so on).

But to reiterate: We mostly see this topic as a distraction in the present day. It isn’t important to reach agreement about any of these high-level philosophical ideas, in order to take action on a technology that’s on track to get us all killed. It would be profoundly foolish to let nonproliferation work get derailed by people debating bright ideas like this — including bright ideas that we authors personally like.

We nevertheless mention this proposal briefly, just to make it clear that we aren’t trying to duck the question; and to perhaps reassure readers who worry that it may be impossible to ever come up with a workable proposal. Even if coherent extrapolated volition is the wrong high-level approach for some reason, the fact that it captures a lot of desirable properties should inspire some hope that it *is *possible to find a non-catastrophic answer to this question.[Isn’t it smarter to avoid talking about extinction?→](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own

But you’re advocating control of how many advanced AI computer chips individuals can own. | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## But you’re advocating control of how many advanced AI computer chips individuals can own.
#### Yes. We also advocate a research ban.

It does not bring us joy to say it. Something would be lost for it to be illegal for individuals to own more than (say) eight unmonitored H100 GPUs from 2024.

But not *so much* would be lost that humanity ought to try to figure out exactly how large a datacenter can be before it gets risky. Erring on the side of a too-low limit means a few people are hampered in their ability to make progress on interesting projects. (Nine H100 GPUs would cost [at least $270,000](/13/what-would-it-take-to-shut-down-global-ai-development#preventing-the-creation-of-more-and-better-ai-chips) today.) Erring too high means everyone dies.

Furthermore, this regime where AI requires enormous amounts of computing power to build won’t last forever. LLMs exist nowadays. Even if building new ones were banned, and building enormous amounts of computing power were banned, people could, in principle, study their inner workings and harvest a few insights about how intelligence works, insights which could help them invent more efficient algorithms that can slip around attempts at monitoring.[Why a research ban? That seems extreme.→](/13/why-a-research-ban-that-seems-extreme)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/can-a-monitoring-regime-last-forever

Can a monitoring regime last forever? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can a monitoring regime last forever?
#### No. Some other off-ramp will be needed.

AI research progress probably can’t be stopped completely. With *enough *time, researchers would probably eventually figure out much more efficient methods of creating AIs.[*](#ftnt302) Or perhaps, with enough time, some rogue actor would eventually succeed at subverting a ban.

Time will most likely drag humanity into the future one way or the other. And humanity will either go extinct — as most species have before it — or will somehow navigate the transition into a world where smarter things exist.

But humanity also doesn’t *need* to buy time forever. AI is not the only technology that’s progressing. Biotech is also starting to mature, and if humanity manages to prevent the development of superintelligent machines for multiple decades, it will have to contend with upsets such as genetic engineering that results in significantly smarter humans.

The question is how much time we can buy, and what we can do with that time.

The basic problem that humanity faces is that of safely crossing the gap from human intelligence to superintelligence. The best plan we can think of that sounds like it *maybe *has a chance of working in real life is to buy time for biotechnology to augment human intelligence quite a lot — to the point where future human researchers can get *so *smart that they’d never (for example) estimate an engineering project would finish on time and under budget unless it *actually would*.

So smart that they’d never commit to a scientific theory like Aristotelianism or heliocentrism, even if the society surrounding them was completely convinced. So smart that they’d have a chance at navigating the [gap between Before and After](/10/a-closer-look-at-before-and-after) on the very first try.

There are other possible paths forward we could imagine, but this one has the advantage of attacking the key bottleneck (“the existing scientific community is too reliant on trial-and-error methods and incrementalism to handle this particular problem”), using tech that is already beginning to be available today, without posing a serious risk to the world.
#### A monitoring regime *shouldn’t *last forever.

It’s *theoretically* possible for humanity to balance on the knife’s edge of competence, where we currently sit, forever. Our guess is that this would require draconian control of people’s thoughts and activities. But even if it didn’t, we would consider it a poor choice.

We, personally, think that humanity’s descendants deserve to become whatever they wish to be, explore the stars, and build a flourishing and beautiful civilization there. We advocate for a ban on frontier AI development because we think superintelligence is dangerous enough to make this necessary — not because we hate AI, or technology, or scientific progress.

The real question is *how *we get to a wonderful future, and how we manage the transition from here to there.

This is worth stressing in part because there are many people who present AI as a false dichotomy: They say (falsely) that society must either accept the risks of AI and plow ahead at full steam, or reject AI and let our civilization fade out on a single planet forever. This is [simply wrong](/12/but-what-about-the-benefits-of-smarter-than-human-ai#rushing-ahead-destroys-those-benefits). There are other routes to the future — routes that permit a future that’s just as bright, but without nearly so high a risk of throwing it all away for nothing. Humanity should find some other path to the future.

[*](#ftnt302_ref) It’s possible that, e.g., researchers find more efficient methods by studying existing LLMs until they better understand how they work.

Might research like this allow people to craft AIs instead of growing them? It might help! Unfortunately, we do expect that long before people develop a full and correct understanding of what’s going on in LLMs, they would develop a partial and incomplete understanding that would let them build much more efficient AIs, but that was not sufficient for aligning them.[Why would making humans smarter help?→](/13/why-would-making-humans-smarter-help)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/can-a-technology-really-be-stopped

Can a technology really be stopped? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can a technology really be stopped?
#### Many technologies are banned or heavily regulated.

Nuclear fission is the classic example of a regulated technology. Private companies are not allowed to enrich uranium without government oversight, no matter how useful the cheap energy would be.

In fact, humanity is pretty good at regulating and slowing down all sorts of other technology. The U.S. strictly regulates [new drugs and medical devices](https://www.fda.gov/), [housing construction](https://www.hud.gov/hud-partners/laws-regulations), [nuclear power generation](https://www.nrc.gov/about-nrc.html), [television and radio programming](https://www.fcc.gov/media/radio/public-and-broadcasting), [accounting practices](https://www.fasb.org/standards), [childcare](https://childcare.gov/consumer-education/regulated-child-care), [pest control](https://npic.orst.edu/reg/laws.html), [agriculture](https://www.usda.gov/about-usda/policies-and-links/laws-and-regulations), and dozens of other industries. Every single state requires a licensing exam for [hair styling](https://www.bls.gov/ooh/personal-care-and-service/barbers-hairstylists-and-cosmetologists.htm#tab-4) and [nail care](https://www.bls.gov/ooh/personal-care-and-service/manicurists-and-pedicurists.htm#tab-4). Most of them require one for [massage therapists](https://www.bls.gov/ooh/healthcare/massage-therapists.htm#tab-4).

We happen to be of the opinion that humanity regulates technology far too much, in many cases. For example, it seems to us that the U.S. Food and Drug Administration is killing far more people (by [slowing down or preventing the creation of life-saving drugs](https://www.cato.org/sites/cato.org/files/serials/files/cato-journal/1985/5/cj5n1-10.pdf), through onerous requirements) than it is saving (by preventing the release of dangerous drugs). It seems to us that the price of housing is far too high, in part because of legal zoning restrictions on what can be built and where. It seems to us that the U.S. essentially destroyed its own nuclear power industry by way of onerous regulations. And seriously, *hair stylists*?

Humanity *absolutely *has the ability to impede technological progress. It would be truly tragic and absurd if we used that ability on medicine, housing, and energy, and neglected to use it on one of the rare technologies that would actually kill us all if created.
#### A ban can be narrowly targeted.

A ban on advanced AI R&D doesn’t need to affect the average person. It doesn’t even need to take away modern chatbots or shut down the self-driving car industry.

Most people are not purchasing dozens of top-of-the-line AI GPUs and housing them in their garages. Most people aren’t running huge datacenters. Most people won’t even *feel the effects *of a ban on AI research and development. It’s just that ChatGPT wouldn’t change quite so often.

Humanity wouldn’t even need to stop using all the current AI tools. ChatGPT wouldn’t have to go away; we could keep figuring out how to integrate it into our lives and our economy. That would still be more change than the world used to see for generations. We would miss out on *new* AI developments (of the sort that would land as AI gets smarter but not yet smart enough to kill everyone), but society is mostly not clamoring for those developments.

And we would get to live. We would get to see our children live.

Developments that people *are* clamoring for, such as the development of new and life-saving medical technology, seem possible to pursue *without *also pursuing superintelligence. We are in favor of carve-outs for medical AI, so long as they function with adequate oversight and steer clear of dangerous generality.

Governments working to avoid the creation of a rogue superintelligence would have to ensure that AI chips were not being used to develop more capable AIs. As such, the question of what AI activities and services would be allowed to continue would depend on what [verification mechanisms](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development)could be used to ensure dangerous AI development was not happening. Better verification mechanisms could decrease the cost of halting AI development by allowing a wider set of activities to continue.

Another step that could potentially help on the margin is installing kill-switches in AI chips, and establishing monitoring and emergency shutdown protocols for any large datacenters in use.[*](#ftnt299) Nuclear reactors are designed such that they can be rapidly shut down in an emergency. If you agree that superintelligence poses an extinction-level threat, then it seems obvious that AI chips and datacenters should be designed to make it easy for regulators to rapidly shut them off.

The point is not to burn all technology because we hate technology.[†](#ftnt300) The point is to avoid going further down the road that ends with human extinction.
#### A big part of the problem is that people don’t understand the looming threat of artificial superintelligence.

In our experience, the people arguing that humanity can’t stop the race to superintelligence are simply failing to understand the point that, if anyone**builds it, everyone dies.

“But AI offers great benefits!” — no, actually; you can’t make use of any of the power of superintelligence if it just kills everyone. If humanity wants to reap the benefits offered by superintelligence, then humanity needs to find some way to navigate the transition to superintelligence that doesn’t kill everyone as a side effect.

“But nuclear power plants are scary because they’re associated with atomic bombs that leveled cities, whereas AI is associated with benign tools like ChatGPT!” — True, at least for now. If humanity never manages to understand that artificial superintelligence built using anything remotely like modern methods would just kill everyone, then they might not put a stop to it. But the obstacle there isn’t that humanity never manages to control or stymie budding technologies (such as nuclear weapons or nuclear power); the obstacle there is that *people don’t understand the threat.*

Hence this book. As we discuss in the final chapter, humanity is capable of quite a lot when enough people understand the nature of the problem.

[*](#ftnt299_ref) Note that installing kill switches into chips and setting up protocols for shutting down datacenters pretty clearly doesn’t solve the problem on its own, given that [we may get no warning shots](/13/will-there-be-warning-shots#warning-shots-are-unlikely-to-be-clear) and [we may not respond effectively to warning shots](/13/will-there-be-warning-shots#humanity-isnt-great-at-responding-to-shocks). But it’s a relatively cheap step that is fully possible to take, and one that could help in marginal cases where the risk is *almost* negligibly low but it would be helpful to have more of a safety margin.

[†](#ftnt300_ref) If society really fears that this will slow the world down too much, we recommend speeding the world up somewhere else. Let people build more nuclear power plants. Let biochemists do more experiments, not on deadly viruses but on making people healthier and stronger and smarter.

(Of course, society writ large does not actually clamor for mad science, so much as resist change from the status quo. But to the people who say “we can’t stop AI because it’s important for civilization to make progress,” the correct answer is that there’s plenty of progress to be made elsewhere, with the sort of mad science that leaves behind survivors.)[Isn’t this handing too much power to governments?→](/13/isnt-this-handing-too-much-power-to-governments)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/can-we-adopt-a-wait-and-see-approach

Can we adopt a wait-and-see approach? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we adopt a wait-and-see approach?
#### No. We don’t know where the critical thresholds are.

There’s a decent chance that AI development will get out of control once AIs are smart enough to automate all of AI research. That could, in theory, happen quietly in a lab with no loud precursor events, with no warning shots to wake humanity up.

As we [discussed previously](/1/will-ai-cross-critical-thresholds-and-take-off), chimpanzee brains are very similar to human brains, except for being smaller by about a factor of four. There’s not an extra “be very smart” module inside human brains; there’s smooth pathway between brains like theirs and brains like ours; it’d be hard to tell where the line was between “a society of these will lead to a bunch of monkeys” and “a society of these will walk on the moon” just by looking at the brains. Primate brains crossed a critical threshold, and it wouldn’t’ve been obvious from the outside. Are there critical thresholds that AI will cross? Who knows! It’s not like AI engineers can tell us; they are not [even](https://arxiv.org/abs/2206.07682)[able](https://arxiv.org/abs/2406.04391) to predict the specific capabilities of their new AI systems in advance of running them.

If humanity understood exactly how intelligence worked, and exactly how the behavior of AIs would change as their capabilities grow, it might be feasible to dance along the edge of the cliff. But right now, humanity is like someone sprinting toward a cliff-edge in the dark and fog, with the final fall some unknown distance away. We can’t just wait until we stumble over the edge to decide that we should have acted differently.

We’ll never be certain. This means that we’re forced to act before we’re certain, or die.[Will there be warning shots?→](/13/will-there-be-warning-shots)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer

How would stopping everyone be possible without installing spyware on every computer? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## How would stopping everyone be possible without installing spyware on every computer?
#### By acting soon.

To train modern AIs, you need a lot of very specialized chips working together in close proximity. Shutting down AI research would involve shutting down some enormous datacenters, and stopping the creation of the highest-end specialized AI chips. These aren’t consumer laptops we’re talking about. Most people wouldn’t even notice the difference.

There are not, as of 2025, a lot of secret hidden chip *factories* that nobody knows about. In 2025, chips suitable for advanced AI are made by only a few manufacturers — though there are, right now, corporations trying to bring more such factories online.

Also in 2025, some key technology for core parts of high-end chip fabrication is sold by only a single manufacturer on Earth: ASML, in the Netherlands.

You can, in other words, turn off the supply at the source. But that state of affairs is not permanent — the sooner an international treaty gets signed, the better. All of this is already harder and more expensive and more dangerous than it would have been to do in 2020, or even in 2023.[But you’re advocating control of how many advanced AI computer chips individuals can own.→](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/is-the-situation-hopeless

Is the situation hopeless? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Is the situation hopeless?
#### No.

This is a fight we can win, and this world of ours is a world worth fighting for. It doesn’t look easy, but it looks genuinely doable.

If you want to join efforts to rouse the world to action, we would be honored to fight by your side as well. See the final chapter of the book for some ways to help.[What Would It Take to Shut Down Global AI Development?→](/13/what-would-it-take-to-shut-down-global-ai-development)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/isnt-it-smarter-to-avoid-talking-about-extinction

Isn’t it smarter to avoid talking about extinction? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t it smarter to avoid talking about extinction?
#### The time has passed for playing political games.

Some have argued that people concerned about the race to build superintelligence should hide their views and instead talk about AI-caused job loss, or the problem of ChatGPT-enabled bioterrorists, or how much water it takes to cool the computers inside datacenters.[*](#ftnt305) We think that this approach is too clever by half and is likely to backfire. Indeed, we have already seen it backfire on various occasions.

The four main problems we see with this approach are:
- 

**It’s not honest**, and people are good at spotting dishonesty and game-playing.

Even if you’re an unusually good liar, arguments about issues that you think are secondary are likely to end up looking “off” in various ways. They won’t quite seem to make sense, for the same reasons that you think those issues are actually secondary. The more you share your sanitized arguments, the more likely it is that people will conclude that you’re *either *confused about this issue *or *not being fully honest about what you actually think. And in either case, you won’t seem like a promising person to ally with or treat as a reliable information source.
- 

**It’s probably unnecessary. **In our experience, honest and direct conversation about superintelligence gets far better reception than trying to redirect to other issues, such as AI deepfakes. Since mid-2023 and with increasing frequency, I (Soares) have spoken to a variety of elected officials. I have sat in dinners where people “concerned about AI” brought up the possibility of AI-assisted terrorists, and a sitting elected official replied that his fears are much more urgent and dire, because he worries about recursively self-improving AIs that might yield superintelligence that wipes us completely off the map, and that could be created inside of three years.

Folks up to and including elected officials in the U.S. Congress are open to taking this issue seriously and looking for ways to address it.[†](#ftnt306) This issue may seem more niche and controversial than it actually is, because there hasn’t been a proper national or international conversation about it yet, as of the publication of this book. But we’ve had many a frank DC conversation on this topic go encouragingly well.[‡](#ftnt307)
- 

**Responding to those other issues doesn’t address the superintelligence issue. **AI companies are racing to build superintelligence. If they get there, everyone dies. The solutions that make sense for this problem are quite different from the solutions that make sense for dealing with AI-generated deepfakes, or even with AI-enabled bioterrorism.

There isn’t *zero* overlap, and we can potentially build more support for tackling smarter-than-human AI by emphasizing ways that different issues overlap. But it’s extremely unlikely that the world will stumble into an adequate response to an issue as complicated as superintelligence *without orienting to the actual issue*.
- **Time is plausibly short. **It’s unlikely that we have time to slowly ease people into considering this risk over many years, starting with simpler and more familiar issues and then climbing a ladder up to superintelligence. If we don’t mobilize an effort to respond to this problem quickly, it’s plausible that we won’t get a chance to respond at all.

This is not to say that job loss, bioterrorism, etc., aren’t real issues in their own right. It’s just that society isn’t going to *actually *put a stop to the reckless suicide race if it doesn’t *know *that there’s a reckless suicide race happening.

We have spent years watching friends and acquaintances in the policy space shop around problems like ChatGPT-enabled bioterrorists. It doesn’t seem to have amounted to anything that will actually prevent the creation of machine superintelligence, as far as we can tell.

We are nerds to our bones, well out of our comfort zones in writing a popular book at all. We don’t claim to have expertise on effective politicking. But it does seem to us that humanity has reached the limit of which problems it can navigate with discourse consisting of carefully couched, strategically chosen, non-“alarmist” arguments.

At some point, as human beings, we have to start talking about the looming threat. Policy needs to be grounded in the actual realities of the situation, not in safe-seeming messaging.

The heads of AI labs say we could see AI researchers that outperform humans in the next [one](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year)[](https://www.theguardian.com/technology/2024/apr/09/elon-musk-predicts-superhuman-ai-will-be-smarter-than-people-next-year)[to](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s)[](https://www.youtube.com/watch?v=ugvHCXCOmm4&t=8400s)[four](https://ia.samaltman.com/) years. We dearly hope that they are wrong, but we do not, with all our expertise, *know* them to be wrong. Policymakers don’t know them to be wrong. Humanity is simply not responding appropriately to the challenge before us. If the alarm is not sounded now, then when?

And: In the time since we first drafted the paragraph above, this strategy we advocate seems to be paying off more and more, as you can see in our [list](/13/will-elected-officials-recognize-this-as-a-real-threat) of what politicians have been saying about superintelligence over the summer of 2025. The time seems ripe for a real discussion of the impending danger from artificial superintelligence.

[*](#ftnt305_ref) Which is [not, in fact, very ](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7water)[much](https://andymasley.substack.com/p/a-cheat-sheet-for-conversations-about?open=false#%C2%A7water), contra[widespread](https://fortune.com/article/how-much-water-does-ai-use/)[reporting](https://www.thetimes.com/uk/technology-uk/article/thirsty-chatgpt-uses-four-times-more-water-than-previously-thought-bc0pqswdr).

[†](#ftnt306_ref) Although various DC officials have agreed with our concerns about superintelligence, they lack the power to solve this issue unless far more officials in the U.S. and in other nations get involved. Early conversations have been promising, but there’s a lot of work still to be done.

[‡](#ftnt307_ref) A related promising sign available to we authors as we put the finishing touches on these online resources is that a number of national security professionals and former DC officials have expressed positive reactions to advance reader copies of *If Anyone Builds It, Everyone Dies*. Examples include:

  
From Ben Bernanke, Nobel laureate and former Chairman of the Federal Reserve: “A clearly written and compelling account of the existential risks that highly advanced AI could pose to humanity. Recommended.”  

From Jon Wolfsthal, Director of Global Risk at the Federation of American Scientists and former Special Assistant to the President for National Security Affairs: “A compelling case that superhuman AI would almost certainly lead to global human annihilation. Governments around the world must recognize the risks and take collective and effective action.”

From Lieutenant General John N.T. “Jack” Shanahan (USAF, Ret.), Inaugural Director, Department of Defense Joint AI Center: “While I’m skeptical that the current trajectory of AI development will lead to human extinction, I acknowledge that this view may reflect a failure of imagination on my part. Given AI’s exponential pace of change there’s no better time to take prudent steps to guard against worst-case outcomes. The authors offer important proposals for global guardrails and risk mitigation that deserve serious consideration.”

From Fiona Hill, former Senior Director on the White House National Security Council: “A serious book in every respect. In Yudkowsky and Soares’s chilling analysis, a super-empowered AI will have no need for humanity and ample capacity to eliminate us. *If Anyone Builds It, Everyone Dies* is an eloquent and urgent plea for us to step back from the brink of self-annihilation.”

From R.P. Eddy, former Director on the White House National Security Council: “This is our warning. Read today. Circulate tomorrow. Demand the guardrails. I’ll keep betting on humanity, but first we must wake up.”

From Suzanne Spaulding, former Under Secretary for the Department of Homeland Security: “The authors raise an incredibly serious issue that merits — really demands — our attention.”

From Emma Sky, Senior Fellow at the Yale Jackson School of Global Affairs and former Political Advisor to the Commanding General of U.S. Forces Iraq: “In *If Anyone Builds It, Everyone Dies*, Eliezer Yudkowsky and Nate Soares deliver a stark and urgent warning: humanity is racing toward the creation of superintelligent AI without the safeguards to survive it. With credibility, clarity and conviction, they argue that advanced AI systems, if misaligned even slightly, could spell the end of human civilization. This provocative book challenges technologists, policymakers, and citizens alike to confront the existential risks of artificial intelligence before it’s too late. An appeal for awareness and a call for caution, this is essential reading for anyone who cares about the future.”[Will elected officials recognize this as a real threat?→](/13/will-elected-officials-recognize-this-as-a-real-threat)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/isnt-this-handing-too-much-power-to-governments

Isn’t this handing too much power to governments? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t this handing too much power to governments?
#### The power to ban dangerous technology is already vested in governments.

Banning AI research on the path to smarter-than-human AI wouldn’t make much of a difference with regard to state power. Governments legislate and regulate an enormous number of things. Restricting a single research program is potentially a big deal *to the AI industry*, but it’s a drop in the bucket *to governments* and to society, which are accustomed to state involvement in many parts of life, and which have a precedent of banning dangerous technology such as chemical weapons.[*](#ftnt301)

Banning one additional tech isn’t going to plunge the world into totalitarianism, any more than nuclear arms treaties led to totalitarianism.

This isn’t to say that it’s *no big deal* to ban a technology. We don’t think the bar for state intervention should be *low*. Rather, we think that superintelligence easily clears any reasonable bar.

If humanity decided to put a stop to AI research and development today, the ban would not need to be particularly invasive. Today, creating a cutting-edge AI requires an extraordinary number of highly specialized computer chips drawing huge amounts of electrical power.

Maybe in ten years it will be possible to do meaningful AI development on a consumer laptop, *if *humanity allows further improvements to computer chips and further research into AI algorithms. But humanity does not need to let that happen. Governments limiting AI R&D do not need to be any more invasive to the average person’s life than governments controlling the proliferation of nuclear weapons technology — so long as the world wakes up to the situation we’re in and puts a stop to things *now*.

[*](#ftnt301_ref) Keep in mind that we advocate for treaties under which *governments *can’t build superintelligence, either. We aren’t calling for a powerful technology to be built by state actors instead of corporations; we’re calling for a lethally dangerous technology to *not be built at all*, at least in anything like today’s world.[Wouldn’t some nations reject a ban?→](/13/wouldnt-some-nations-reject-a-ban)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/keep-the-coalition-large

Keep the Coalition Large | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/13#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Keep the Coalition Large

We’ve heard some people argue that we should take a strong stand against AI art or robotic weapons, in order to send a simpler message: not anti-superintelligence, but anti-*AI.*

Setting aside the merits of the various positions on AI art, deepfakes, and so on, we don’t think that this is the best option politically. We want to build a coalition to ban superintelligence. We consider this issue extraordinarily urgent and pressing, and we want this coalition to be as large as possible, including people with a wide variety of views on AI art, drone warfare, self-driving cars, use of AI in schools, and so on.

We all have a common interest in preventing the creation of rogue superintelligence, regardless of our stance on other issues.

Should humanity go extinct, and be replaced by something bleak? Everyone who agrees that the answer is “no” can cooperate in an urgent effort to stop the scramble for superintelligence.

We don’t think the coalition will survive if you say you can’t work with anyone who disagrees with you about AI art or drone warfare.

The coalition won’t survive if a bundle of other issues gets packaged with superintelligence, either, such that everyone needs to agree on a long list of semi-related questions before they’ll work together about superintelligence.

If you care about other AI-related issues, we urge you to work on addressing them. But we ask that you not *package* those issues with superintelligence. If we’re going to make it through this, nothing should be packaged with the survival of humanity.

Part of why you live in whichever country you live in, and not a heap of radioactive rubble left in the wake of World War III, is that the East and the West managed to agree that nuclear war was a realistic and serious threat to humanity decades ago. The East and the West respectively said that the West and the East were an *additional* terrible threat to humanity. But they wisely treated the two threats — nuclear annihilation versus ideological defeat — as different in kind.

From the perspective of the West, it was better that humanity should be *less *threatened by nuclear war even if still threatened by the East, which meant cooperating with the East long enough to lay a direct line between Washington and Moscow, and cooperate on nonproliferation treaties and other arrangements.

Too many countries need to coordinate. Too many factions are divided (even internally) for it to be possible to avert catastrophe if only people who agree on everything can act together.

We happily and unreservedly make common cause with the people who are concerned about other issues in the world. We will unhesitatingly work with people we disagree with politically. We have issued this desperate message to the world because we believe it, and we think this problem needs to be addressed *immediately* at the international level.

Whoever you are — whatever you’re fighting for, here or elsewhere — if you want an end to the breakneck development of smarter-than-human AI, we’re in this fight together.[→](/errata)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/what-would-it-take-to-shut-down-global-ai-development

What Would It Take to Shut Down Global AI Development? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/13#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What Would It Take to Shut Down Global AI Development?

We aren’t experts in international law, and this is a formidably complicated topic that we expect to require a large amount of effort by domain experts. In the interest of getting the ball rolling quickly, however, we’ve worked with our [technical governance team](https://techgov.intelligence.org/) and outside advisors to assemble some sketches and guesses at some measures that could be effective.

We offer these in the spirit of encouraging conversation, debate, critique, and iteration. These first-draft ideas should in no way be treated as confident or authoritative.

As a first step, let’s walk through the constraints and shape of the problem we’re trying to solve — a topic that could easily take up a book of its own. The overall problem has been preventing the development of machine superintelligence for decades. And because we don’t know where the critical thresholds are, that essentially amounts to stopping AI research and development entirely.

Current AI progress stems from a combination of creating better computer chips, using more chips longer for longer training runs, and improving AI algorithms. We’ll contend with each of those in turn, explaining the corresponding levers for halting progress toward artificial superintelligence.
#### Preventing the Creation of More and Better AI Chips

Increasing the capabilities of modern AIs takes an enormous investment of computing power and electrical power. As a result, it appears possible for modern state actors to identify and monitor all relevant facilities and prevent the emergence of new such facilities, with minimal impact on consumer hardware.

The supply chain for producing advanced AI chips is extremely concentrated.[*](#ftnt308) For some steps in the supply chain, there is only a single company in the world capable of filling that role, and these companies are largely in countries traditionally allied with the United States.

For example, only a few firms can fabricate AI chips — primarily the Taiwanese company TSMC — and one of the key machines used in high-end chips is only produced by the Dutch company ASML. This is the extreme ultraviolet lithography machine, which is the size of a school bus, weighing 200 tons and costing [hundreds of millions of dollars](https://www.datacenterdynamics.com/en/news/tsmc-to-receive-first-high-na-euv-lithography-machine-from-asml-in-q4/).

This supply chain is the result of decades of innovation and investment, and replicating it is expected to be quite difficult — likely taking over a decade, even for technologically advanced countries.[†](#ftnt309)

The most advanced AI chips are also quite specialized, so tracking and monitoring them would have few spillover effects. NVIDIA’s H100 chip, one of the most common AI chips as of mid-2025, costs around $30,000 per chip and is designed to be run in a datacenter due to its cooling and power requirements.[‡](#ftnt310) These chips are optimized for doing the numerical operations involved in training and running AIs, and they’re typically tens to thousands of times more performant at AI workloads than standard computers (consumer CPUs).[§](#ftnt311)

The concentration and complexity of the AI chip supply chain makes halting advanced AI development easier than one might expect. **It would be simple to stop the production of new AI chips**. It would require fairly minimal monitoring of a small number of key suppliers to ensure that secret supply chains are not created, given how complex and interconnected the production process is.

Some of the same infrastructure is used to produce AI chips and other advanced computer chips (such as cell phone chips), but there are notable differences between these chips. If advanced AI chip production is shut down, it would be feasible to monitor and ensure that any ongoing chip production is only creating non-AI-specialized chips.

Pre-existing specialized AI chips could be monitored if they’re kept and used to run existing AIs, such as ChatGPT. Ensuring that such chips were only being used to run low-capability AIs (rather than for novel research and development) would be a challenge, but not an insurmountable one. Existing chip locations could be tracked and monitored, and there are various potential [mechanisms that could be used to verify](https://techgov.intelligence.org/research/mechanisms-to-verify-international-agreements-about-ai-development) what those chips are being used for. This sort of monitoring requires physical access to chips (e.g., inspectors taking measurements in a datacenter). Remote access could be sufficient for verification if new chips were fabricated with [improved security](https://www.cnas.org/publications/reports/secure-governable-chips) and designed with verification and monitoring in mind. As we discuss in the following section, the chip concentrations required to be dangerous (at the August 2025 level of AI algorithms) are so large that it wouldn’t be difficult for state actors to detect all such facilities and subject them to regular inspection.
#### Preventing the Usage of More and Better AI Chips

Moving our attention now from the production of chips to the usage of chips: The current [largest AI datacenters](https://epoch.ai/blog/trends-in-ai-supercomputers) house hundreds of thousands of AI chips, which cost billions of dollars. To train one of the most powerful AIs today, these chips need to be used for months on end.

Each of these chips has a similar power consumption to the average American home, so a datacenter with hundreds of thousands of chips has a power usage comparable to that of a small city.[¶](#ftnt312) Powering all these chips requires specialized electrical infrastructure, such as large transmission lines. These datacenters are also fairly large buildings with distinctive thermal signatures from continuously running and cooling large numbers of energy-intensive chips.

Internally, these datacenters house their thousands of chips in server racks and have extensive cooling infrastructure to ensure chips don’t overheat. If one went inside one of these buildings, it would be extremely clear that it was a datacenter. It’s not like their purpose could be hidden from international monitors who come knocking, especially if the international monitors check the chips in the datacenter and find that they’re AI-specialized chips.

**Large datacenters and their related power infrastructure are so massive that they can be identified by orbiting satellites.** This means that if governments wanted to locate current large datacenters, they would likely be able to do so with a high success rate, whether those datacenters are inside their borders or in other countries. Although the state of public knowledge is limited, this intervention alone could track down the majority of high-end AI chips.[‖](#ftnt313)

States may attempt to hide their datacenters in the future to make it difficult to identify them with satellites. For example, states might attempt to conceal a datacenter in a mountain (like in the Cheyenne Mountain Complex, which houses NORAD) where it would not be visible from above. Even so, it would be difficult to hide the infrastructure required to run the datacenter.

The biggest factor favoring detection is that datacenters have very large electricity requirements. This power is usually provided via transmission lines, which are almost always above ground. It’s possible to bury transmission lines, but it’s much more expensive and time-consuming, and the construction effort to bury the transmission lines is also difficult to conceal.[#](#ftnt314)

So long as it continues taking more than 100,000 chips to train a cutting-edge AI, it looks quite possible for state actors to detect and monitor every relevant datacenter.
#### Preventing Algorithmic Progress

More efficient AI algorithms can reduce the computational resources needed to train an AI, or they can allow more capable AIs to be produced using a given amount of computational resources, [or both](https://arxiv.org/abs/2311.15377).

Algorithmic progress is primarily driven by research and engineering, and these currently depend on human skill and effort.[**](#ftnt315) The skills needed to improve AI algorithms are relatively rare, which explains the [large salaries](https://www.nytimes.com/2025/07/31/technology/ai-researchers-nba-stars.html) commanded by top researchers in the field.

Although these skills are rare today, it’s unclear how that might change as more researchers move into the field and more knowledge becomes public. Depending on how one wants to count the number of people with the requisite skills, the true number is likely in the hundreds or low thousands (e.g., based on the number of AI researchers and engineers at top AI companies).[††](#ftnt316) Conservative estimates could be much higher — for instance, there are tens of millions of software engineers in the world.

**Legal and social interventions could likely dramatically slow algorithmic progress.** Most people don’t want to break the law, especially when there are real consequences. If it were illegal to publish certain AI research or perform various AI experiments based on catastrophic risks posed by sufficiently capable AI, this would likely dissuade almost all potential AI research scientists, as we [discussed previously](/13/why-a-research-ban-that-seems-extreme). Governments could implement export controls that would make sharing or publishing such research illegal without an export license and government approval.

Social taboos would help, too. As precedent, we can look at the [Asilomar Conference on Recombinant DNA](https://en.wikipedia.org/wiki/Asilomar_Conference_on_Recombinant_DNA) in 1975, which resulted in a voluntary ban of certain biological experiments that were thought to pose undue risks. In theory, scientists could institute a voluntary ban on advancing AI capabilities. However, this would require these scientists to take seriously the danger from smarter-than-human AI — a departure from the status quo, where advancing AI capabilities is lauded in many circles. Given the myopic monetary incentives and the observed behavior of the labs to date, external legal restrictions seem extremely likely to be necessary, unless the culture of the field shifts *dramatically* (and in short order).[‡‡](#ftnt317)

A critical component of making an imperfect ban effective may be something as obvious as ensuring that world leaders actually understand that they and their families will personally die if they continue to push forward, as we [discussed previously](/13/wouldnt-some-nations-reject-a-ban). The likeliest noncompliance scenarios are ones in which governments see home-grown superintelligence as a strategic asset (or as a mirage distracting them from profitable new AI tools), rather than as a global suicide button. Governments are much less likely to run secret ASI research projects if they correctly see that this amounts to loading a gun, putting it to their head, and pulling the trigger.

Research bans wouldn’t stop everyone. Some prominent research scientists and tech executives have already said that destroying humanity is an acceptable price to pay for progress.[§§](#ftnt318) But we should not let the perfect be the enemy of the good. Algorithmic advancements would at least go *slower *if such people were defunded and shunned by their peers, forcing them to do their lethal research outside the law and without collaboration with any of their more upstanding peers.
#### The Longer We Wait, the Harder It Gets

If AI chip production and distribution continue on their current trajectory, the challenge of ensuring that enough AI chips are centralized and monitored will only become more difficult. Even if states are not yet convinced of the risks, starting to internationally track AI chips today means that intervention may remain possible in the future. That window may close soon if governments do not move quickly.

If researchers are allowed to continue advancing the state of AI algorithms, smaller and smaller numbers of AI chips are likely to pose a serious threat. If and when AI systems become capable of automating parts of the AI R&D process, it could become especially difficult to control AI development. Such systems could be easily copied and distributed, and the hardware required to run them may not be significant. (The hardware requirements to *run* AI systems are much lower than those to *train* AI systems.)

Eventually, it may be impossible for the world’s governments to stop the development of superintelligent AI systems. We’re not there, but it gets harder every month. The plan we outline is premised on stopping AI development *soon*. There are other plans that don’t rely on this assumption, but they’re more difficult to implement, have higher costs to personal freedoms, and come with a greater chance of failure.

[*](#ftnt308_ref)Thandi and Allen of the Center for Strategic & International Studies have [analyzed](https://www.csis.org/analysis/mapping-semiconductor-supply-chain-critical-role-indo-pacific-region) the semiconductor supply chain.

[†](#ftnt309_ref)According to [analysis](https://cset.georgetown.edu/publication/maintaining-the-ai-chip-competitive-advantage-of-the-united-states-and-its-allies/) done by Saif M. Kahn of the Center for Security and Emerging Technology (CSET).

[‡](#ftnt310_ref) For a report on AI chips and how they differ from consumer hardware, see an [analysis](https://cset.georgetown.edu/publication/ai-chips-what-they-are-and-why-they-matter/) by Kahn of CSET.

[§](#ftnt311_ref) They are sometimes used for other computation-intensive tasks, such as physics and weather simulations, but they are *primarily* used for AI. One quick method of estimating how many AI chips are used for non-AI activities is to look at the [revenue over time](https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue) of the main chip producer, NVIDIA. If we assume that the recent boom in demand for their datacenter GPUs stems almost entirely from AI uses — a reasonable assumption, given the enormous recent boom in the AI industry and the lack of any comparable trend in other fields that use these chips — we would conclude that AI accounts for the vast majority of AI chip use, as recent revenue growth dwarfs previous revenue. Preventing the fabrication of specialized AI chips need not have much effect on consumer hardware.

[¶](#ftnt312_ref)For a report on AI infrastructure, see [work done](https://ifp.org/future-of-ai-compute/) by the Institute For Progress.

[‖](#ftnt313_ref)For some analysis of trends in the production and ownership of high-end AI chips, see the [report](https://epoch.ai/blog/trends-in-ai-supercomputers) by Pilz et. al on Trends in AI Supercomputers.

[#](#ftnt314_ref) It may be possible to generate power on-site, thus removing any conspicuous transmission lines. The current Cheyenne Mountain Complex uses diesel generators, and probably has the [capacity to power](https://www.af.mil/News/Article-Display/Article/497017/airmen-operate-americas-fortress/#:~:text=Each%20of%20the%20six%20generators,have%20a%20dedicated%20fire%20department.) around 10,000 of the most advanced AI chips. But running these chips continuously for a large training run would require constantly delivering fuel, which would be noticeable. Rough calculations show that these 10,000 chips would require about one tank truck every day. Even if there was the local generation capacity to power 200,000 chips, this would require 20 tank trucks of diesel every day.

Datacenters could also be powered by nuclear power plants. Fortunately, many state actors already have practice and experience monitoring the creation of new nuclear power plants.

[**](#ftnt315_ref)[Examples](https://arxiv.org/abs/2507.10618) of this kind of progress include [FlashAttention](https://arxiv.org/abs/2205.14135), an algorithm that makes AI chips execute a certain set of mathematical operations more efficiently by taking advantage of details of AI chip design; [Mixture-of-Experts](https://arxiv.org/abs/1701.06538), a change to the architecture of AIs that makes only a subset of their parameters get used on each input token (e.g., word); and [GRPO](https://arxiv.org/abs/2402.03300), a method for fine-tuning AIs.

[††](#ftnt316_ref)For some analysis of the scarcity of top AI researchers, see Sharon Goldman’s piece in *[Fortune](https://fortune.com/2025/03/15/ai-talent-wars-startups-google-meta-openai-hiring-scientists-stock-salaries/)*.

[‡‡](#ftnt317_ref) Another possible intervention, assuming the number of AI algorithmic progress researchers continues to be small (i.e., in the hundreds or thousands), would be to pay these researchers to direct their efforts toward non-AI uses or toward AI capabilities or alignment research that has negligible aggregate risk. There is precedent for this kind of intervention in the 1990s, when the U.S. government started an initiative to channel the work of former Soviet weapons scientists and technicians [to productive, non-military endeavors](https://www.armscontrol.org/act/1999-03/features/maintaining-proliferation-fight-former-soviet-union#:~:text=One%20of%20the%20earliest,productive%2C%20non%2Dmilitary%20endeavors).

[§§](#ftnt318_ref)For a review of what prominent researchers have said and where we think they’re going wrong, see or answer to [Why don’t you care about the values of any entities other than humans?](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)[A Tentative Draft of a Treaty, with Annotations→](/13/a-tentative-draft-of-a-treaty-with-annotations)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/why-a-research-ban-that-seems-extreme

Why a research ban? That seems extreme. | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why a research ban? That seems extreme.
#### More breakthroughs might make it effectively impossible to stop people from making superintelligence.

In the book, we mentioned how a single paper in 2017 kicked off the entire LLM revolution by describing an algorithm that made it practical to train useful AIs on specialized commercial hardware.

If powerful AI should ever become trainable on widely available *consumer* hardware, measures to prevent superintelligence would need to become onerous, and would fail more quickly.

That’s why research into even more powerful and efficient AI algorithms is also a lethal poison for humanity.

This is very bad news, and not what we wish were true. But it seems to be the situation we’re in.

No law can prevent current AI scientists from thinking more about efficient algorithms in the privacy of their own minds. Maybe some people start an underground network for sharing research results. Some people in the AI industry already [proudly declare](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans) that humanity *should *die to AIs; they might do their best to drive forward, no matter what anyone else says.

But AI research would slow down a *lot* if it were illegal, and all the more so if it were widely understood that this really is a kind of research that is liable to get us all killed. It would slow down immensely if underground networks of that sort were tracked down and stopped with the same conviction used to stop people who try to enrich uranium in their garage, because the real-world dangers are taken seriously.

*Most *people do not try to do extremely illegal things that will make international law enforcement and intelligence agencies genuinely upset. Making it illegal to publish clever new AI algorithms would deter perhaps 99.9 percent of people and nearly all corporations, and then the remaining 0.1 percent can be handled by local, national, and international police and intelligence agencies, and wouldn’t get nearly the current level of academic funding.

It would be a very different world from the current world, where it’s totally legal to run the most dangerous mad science experiments in history and giant corporations put billions of dollars into the endeavor.

We don’t know how many more breakthroughs it will take before the AIs are smart enough to do AI research and build even smarter AIs. It could be one breakthrough. It could be five. But better algorithms are just as deadly as better hardware. They are two horses drawing the same cart off a cliff.[Can a technology really be stopped?→](/13/can-a-technology-really-be-stopped)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/why-would-making-humans-smarter-help

Why would making humans smarter help? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why would making humans smarter help?
#### It could help with solving the alignment problem.

The AI alignment problem does not look to us like it is fundamentally unsolvable. It only looks to us that humans are nowhere close to solving it, and that humans are not at the level of intelligence where *thinking *they have a solution strongly correlates with them actually* having* a solution.

AI researchers often recognize that the alignment problem looks formidably difficult, and that very little progress has been made on the problem to date. This is why “maybe we can get the AIs to do our alignment homework for us” has had such appeal: When you’re an AI researcher and you don’t feel that you and your colleagues are up to the task of solving a certain problem, the obvious thing to reach for is AI.

But as we discuss in Chapter 11 and in an associated [extended discussion](/11/more-on-some-of-the-plans-we-critiqued-in-the-book#more-on-making-ais-solve-the-problem), it’s clear even from a layman’s perspective that this idea has many issues: For an AI to figure out how to solve a deep problem that the best human researchers are having a great deal of trouble with, it needs to be smart enough to be dangerous. And since *we *have very little idea of what we’re doing, we have no source of ground truth that we can use to directly train for narrow alignment capabilities, and no way of checking whether an AI-generated alignment proposal is safe or effective.

The world is allowed to hand us problems that are legitimately out of reach. Nature isn’t a game that only gives humanity “fair” challenges; we can sometimes run into problems that are too hard for even top human scientists to solve, or too hard to solve within the required timeframe.

Is there a more realistic method of passing the whole problem off to some smarter entity? One option would be to make *humans* smarter in such a way that they might legitimately be able to solve the alignment problem. Humans come “pre-aligned” in a way that AIs don’t; the smartest humans have the same basic prosocial motivations as the rest of us.

In principle, it seems possible for people to be able to tell the difference between what *seems *like great alchemical enlightenment that will let them transmute lead into gold, and the sort of knowledge that *actually corresponds *to the ability to transmute lead into gold (using nuclear physics to knock some neutrons off of lead atoms). They should surely feel like different states of knowledge.

But actual human engineers have a lot of trouble telling which zone they’re in. In the actual history of chemistry, humanity’s skill level was such that the alchemists were reliably fooled.

In the real world, scientists get attached to their pet theories and refuse to revise their views until reality hammers them over the head repeatedly with “your theory was wrong” — and sometimes they refuse to change their view even then: Science is sometimes said to advance “[one funeral at a time](https://en.wikipedia.org/wiki/Planck%27s_principle),” because the old guard will never change their views and you’ve just got to wait for the new guard to mature. But this isn’t a fundamental constraint imposed by nature; it’s just an issue of humans as a class being insufficiently savvy, careful, and self-aware.

Usually, it’s okay for humans to be naive in these ways, because usually reality is fairly forgiving of errors, at least in the sense that it doesn’t wipe out *all of humanity *for the hubris of one alchemist. But [that’s not a luxury humanity has](/10/a-closer-look-at-before-and-after) when it comes to aligning machine superintelligence.

Humanity often gains its knowledge by struggling, and trying, and failing, and slowly accumulating knowledge. But it doesn’t *have* to be that way.

Einstein was not only able to figure out general relativity; he was able to figure it out *by thinking hard about the problem*, even before humanity put satellites in orbit and started seeing discrepancies in their clocks with their own two eyes (as discussed in Chapter 6). He had empirical evidence, but he was able to efficiently pinpoint the right answer in response to the first quiet whispers from the empirical record, rather than needing the truth to come banging at his door.

That pathway is rarer and harder to walk, but that kind of scientific genius does exist — albeit rarely, even among the world’s best and brightest.

Humans augmented one or two steps beyond the level of researchers like Einstein or [John von Neumann](https://web.archive.org/web/20250703040053/https://www.spectator.co.uk/article/the-forgotten-einstein-how-john-von-neumann-shaped-the-modern-world/) might begin to accurately figure out their own flaws, and correct for them, in dozens of different ways.

They might notice when they were rationalizing or falling victim to [confirmation bias](https://en.wikipedia.org/wiki/Confirmation_bias). They might go past the point of ever expecting a clever-sounding idea to work when actually it does not work — to the point where whenever they expect to succeed, they *do* succeed. They might achieve a level of competence where they still make plenty of mistakes, but they aren’t systematically overconfident (or underconfident) in tricky new domains.

Is human intelligence enhancement really a possibility? It seems so to us, having spoken with a number of biotech researchers who think that there are promising near-term angles of attack. Carefully targeted [biotech-focused AI](https://intelligence.org/2025/09/15/a-note-on-ai-for-medicine) might also help accelerate the work. But from our perspective, it remains very uncertain whether a plan like this would realistically pan out. What we feel more confident in saying is that it’s a highly leveraged option that deserves a lot more investment and exploration than it’s currently getting.

We are not recommending enhancing human intelligence as the only post-AI-shutdown strategy we think humanity should heavily invest in. Rather, this is just one of many examples, and the one we currently think holds the most promise. We strongly recommend that humanity look into multiple possible non-AI paths forward, rather than putting all its eggs in one basket.
#### Augmented humans don’t pose a major “human alignment” problem.

Augmented humans would have essentially the same brain architecture, emotions, etc., as the rest of us. With AI — even AI trained to sound like us — there’s an enormous gulf of cognitive and motivational difference, and a similarly large comprehensibility gap; with modestly smarter humans, none of that seems particularly likely to be true.[*](#ftnt303)

Cognitively enhanced researchers wouldn’t need to hold together their own mental integrity while turning into vast superintelligences with minds millions of times larger. They would only need to be raised to the level required to figure out how to *build*— not grow — artificial superintelligences that would be truly aligned and stable.

There may still be a “human alignment” problem in the weak sense that any effort to coordinate multiple people can run into principal-agent problems and incentive problems. And these problems inherently matter a great deal more with any group tasked with creating superintelligence.

We expect these problems are tractable as long as the humans start out visibly altruistic and charitable, so long as their intelligence is enhanced only slowly, and so long as they work in a well-designed institution with well-designed incentives. But it’s entirely reasonable for people to worry about the potential for a power grab here. Solving these problems wouldn’t necessarily be easy, but it wouldn’t be as fundamentally unfeasible as corporations trying to grow inscrutable superintelligences with entirely incomprehensible minds and inhuman drives.

Creating a crack team of genetically engineered supergeniuses to help navigate the planet safely through the transition to superintelligence is definitely the sort of thing that humanity should do carefully, given the high stakes of such an endeavor. A move like this comes with various practical and ethical issues, but these have to be weighed against the cost of letting superintelligence kill us all, if no other solutions seem similarly promising.

Drastic times can call for drastic measures, but (modest) human intelligence enhancement isn’t even a measure that seems particularly drastic. It seems like a net-positive technology on its own terms, that has at least some chance of helping humanity out in more ways than one.
#### If you’re interested in helping, let us know.

If you have expertise in the sort of biotechnology that would be useful when it comes to augmenting human intelligence, we encourage you to reach out to us via [this form](https://docs.google.com/forms/d/e/1FAIpQLSdKGj9Oie8zgn3nF29LApABm_jlDWTj-pEMx7NZFcVoIJb22w/viewform). That goes for funders, researchers, and policymakers alike. We are not experts in the relevant aspects of biotechnology, but we have connections to various figures in the field. At the very least, we can perhaps connect funders and researchers and policymakers to one another, to help grease the gears.
#### We can work together to stop superintelligence while disagreeing on human enhancement.

If you don’t agree with us about the human augmentation idea, we can still shake hands with you on shutting down frontier AI development.

If we don’t solve that part, everyone is dead. Everyone who doesn’t want to die *today* has to cooperate to that end. We can wait until after we’re past the threat of immediate death to argue about whether augmenting human intelligence should be illegal or subsidized.

“Make humans smarter than Einstein” is not a plan for how not to die in 2028 or 2032 or whenever there’s the next basic breakthrough in AI algorithms.

It’s not a plan that can run alongside AI development. Even if someone uses medical technology unlocked by dumber-than-human AIs to augment human intelligence well past the Einstein level, those augments probably wouldn’t be able to solve the AI alignment problem and safely design, craft, and build machine superintelligence *quickly*, under the time pressure of an arms race. The race toward superintelligence still needs to stop.

The idea behind human intelligence augmentation is that it might make it *possible at all *to solve the alignment problem, if a large number of upgraded researchers also have a significant number of years or decades to work on the problem. The idea is not that they could win a *race *to build aligned superintelligence six years from now, faster than the rest of the AI industry can build and deploy unaligned superintelligence.

Many people who think “augment human intelligence” is a relatively promising plan — ourselves included — think the first steps still involve shutting down the AI companies.

Those who have other plans about what humanity should do next also generally agree that the first step should be shutting down the AI companies.

Meta AI can’t exist, OpenAI can’t exist, Anthropic can’t exist; they will just kill us. We can agree on this immediate priority, even if we have very different ideas about what to do next.

[*](#ftnt303_ref) For more on why AIs trained to sound friendly won’t turn out friendly, see our answer to the question “[Won’t LLMs be like the humans in the data they’re trained on?](/2/wont-llms-be-like-the-humans-in-the-data-theyre-trained-on)”[“Aligned to whom?”→](/13/aligned-to-whom)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/will-elected-officials-recognize-this-as-a-real-threat

Will elected officials recognize this as a real threat? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will elected officials recognize this as a real threat?
#### An increasing number already have.

Examples include:

“We have to be very careful with it, right? We have to watch it. [...] You know, there are people that say it takes over, it takes over the human race. It's really powerful stuff, AI.”[](https://x.com/impaulsive/status/1801369329981919752)

“Today’s discussion will likely touch on some of the pressing issues in highly capable AI systems: [...] The push to develop Artificial General Intelligence, or super-intelligent AI, that would be so powerful and capable that we would see it as a ‘digital god.’ [...] I hope we will spend our time today on the specific policy solutions necessary to avert the long-term risks of AI and the potential doomsday scenarios.”[](https://www.schumer.senate.gov/imo/media/doc/8Opening%20Statement.pdf)

“Artificial Intelligence is the biggest technological threat we've faced since the invention of the atomic bomb.”[](https://x.com/RepMoulton/status/1691478756693520384)

“And now, artificial general intelligence or AGI, which I know our witnesses are going to address today, provides even more frightening prospects for harm. The idea that AGI might in 10 or 20 years be smarter, or at least as smart as a human being, is no longer that far out in the future. It is very far from science fiction. It is here and now. One to three years has been the latest prediction, in fact, before this Committee. And we know that artificial intelligence that is as smart as human beings is also capable of deceiving us, manipulating us, and concealing facts from us, and having a mind of its own when it comes to warfare, whether it is cyber war or nuclear war or simply war on the ground in the battlefield.”[](https://www.blumenthal.senate.gov/newsroom/press/release/blumenthal-delivers-opening-statement-at-hearing-on-artificial-intelligence-regulation-featuring-insider-perspectives)

“If we miss this opportunity the consequences will shape generations to come. What begins today as Generative AI may one day become Artificial General Intelligence. A wild, unregulated AI industry — that is accountable to no one — developing Artificial General Intelligence should scare us all into action.”[](https://www.hickenlooper.senate.gov/press_releases/hickenlooper-proposes-ai-auditing-standards-calls-for-protecting-consumer-data-increasing-transparency/)

  
We think that the major impediment to people recognizing the threat is getting them to understand it. In the few short months since the book was shipped off to print, it seems to us that the world has already made additional headway in that direction.

Here are some statements by U.S. politicians on both sides of the political aisle, in the summer of 2025:

“Some experts warn we are just a few years away from the emergence of artificial general intelligence — or the singularity. Others argue the technology has inherent limitations and we are decades away from the singularity, if it is even possible. We don’t know for certain what the future of AI will look like. But what I do know is the future is too important to be left up to chance. We need to do our best to understand what kinds of impact AI can have on our economy and society and develop potential solutions now, before it’s too late.”[](https://oversight.house.gov/release/mace-opens-hearing-on-the-future-of-artificial-intelligence/)

“Artificial superintelligence is one of the largest existential threats that we face right now. […] Should we also be concerned that authoritarian states like China or Russia may lose control over their own advanced systems? […] And is it possible that a loss of control by any nation-state, including our own, could give rise to an independent AGI or ASI actor that globally we will need to contend with?” [](https://www.congress.gov/event/119th-congress/house-event/118428)

“I’m not voting for the development of skynet and the rise of the machines by destroying federalism for 10 years by taking away state rights to regulate and make laws on all AI.” [](https://x.com/RepMTG/status/1930650431253827806)

“Industry leaders have publicly acknowledged the development of increasingly powerful artificial intelligence systems, with some discussing the potential for artificial general intelligence and superintelligence that could fundamentally reshape the society of the United States.”[](https://www.congress.gov/119/bills/s2081/BILLS-119s2081is.htm)

“Last month, it was reported that Open AI's chief scientist wanted to ‘build a bunker before we release AGI.’ [...] Rather than building bunkers, however, we should be building safer AI. Whether it's American AI or Chinese AI, it should not be released until we know it's safe.”[](https://youtu.be/GDNrUZBZDA4?t=657)

“This is something that we have to get right, and we only get one shot at. There's a widely shared view that once AI capability crosses a certain threshold, whether that be recursive self-improvement or some other threshold, there's going to be an escape velocity. That has implications for the narrower geopolitical context of which country leads in the technology, but also for the broader idea of, ‘Is this technology going to be aligned with and beneficial to humanity?’”[](https://www.youtube.com/watch?v=Dny8dVjWS_k)

“The deeper we get into it, the more we realize that it’s also possible that the race to be the first in AI is the race to be the first to lose control.” ([And](https://time.com/6727264/house-artificial-intelligence-task-force/): “As long as there are really thoughtful people, like Dr. Hinton or others, who worry about the existential risks of artificial intelligence — the end of humanity — I don't think we can afford to ignore that.”)[](https://annandaletoday.com/rep-beyer-calls-for-more-regulation-of-ai/)

“There are very, very knowledgeable people — and I just talked to one today — who worry very much that human beings will not be able to control the technology, and that artificial intelligence will in fact dominate our society. We will not be able to control it. It may be able to control us. That’s kind of the doomsday scenario — and there is some concern about that among very knowledgeable people in the industry.” [](https://gizmodo.com/bernie-sanders-reveals-the-ai-doomsday-scenario-that-worries-top-experts-2000628611)

Other noteworthy statements include:

“Alarm bells over the latest form of artificial intelligence, generative AI, are deafening. And they are loudest from the developers who designed it. These scientists and experts have called on the world to act, declaring AI an existential threat to humanity on a par with the risk of nuclear war. We must take those warnings seriously.” [](https://www.youtube.com/watch?v=ktFF2dSH3oU&t=38s)

A lot more progress is needed, but the world is beginning to take notice. The time is ripe for alerting officials to the need for rapid action at the federal and international levels.[Is the situation hopeless?→](/13/is-the-situation-hopeless)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/will-there-be-warning-shots

Will there be warning shots? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will there be warning shots?
#### Maybe. If we wish to make use of them, we must prepare now.

When Apollo 1 caught fire (killing the entire crew), NASA was *close enough *to having a working rocket that the engineers were able to figure out exactly what went wrong and adjust their techniques. Six of the seven Apollo spacecraft that NASA later sent to land on the moon would make it there.[*](#ftnt294)

Or consider the case of [the Federal Aviation Administration](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it): Every airplane crash triggers a deep and exhaustive investigation, involving hundreds of pages of data, testing, examinations, and details. The FAA’s grasp of the details and specifics is so good that they can keep fatal accidents below one per twenty million flight hours.

By contrast, when an AI behaves in ways that [no one predicted and nobody asked for](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient), the lab’s response does not involve figuring out exactly what went wrong. It involves retraining the AI until the bad behavior is relegated to the fringes (but [not eliminated](https://www.arxiv.org/pdf/2505.10066)), and maybe asking the AI to cut it out.

For instance, sycophancy is [still an ongoing problem](/4/ai-induced-psychosis#labs-have-tried-and-failed-to-stop-the-sycophancy) in August of 2025, months after a series of high-profile cases leading to psychosis and suicide, despite all the poking. Nobody has done (nor can do) a detailed failure analysis of what’s going wrong inside the AI’s mind, because AIs are grown and not crafted.

It doesn’t seem like an easy call whether there will be major events in the future that raise more alarm about AI (“warning shots”). But it does seem clear that we are not prepared to take full advantage of such events.

We can imagine a fantasy world where humanity is united in a sincere effort to solve the ASI alignment problem, with tight monitoring procedures and an international coalition.[†](#ftnt295) And we can imagine that this international coalition slips up somehow, and that an AI gets smarter than its engineers thought, faster than the engineers expected, and very nearly manages to escape. Maybe *that *sort of warning shot would allow people to learn and be more careful next time.

But the current world doesn’t look like that. The current world looks more like a bunch of alchemists who watch their contemporaries go mad from some unknown poison, while lacking the awareness to figure out that the poison is mercury and that they should stop using it themselves.

Maybe there will be clearer and starker warning signs in the future. They’ll be a lot more helpful if humanity starts preparing now.
#### Warning shots are unlikely to be clear.

There are already plenty of warning signs about AI for those who know where to look. In the book, we discussed Anthropic’s Claude models [cheating on coding problems](https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf) and [faking alignment](https://www.anthropic.com/research/alignment-faking). We also reviewed the case of OpenAI’s o1 model [hacking to win a capture-the-flag challenge](https://cdn.openai.com/o1-system-card.pdf), and a case where a later o1 variant [lied, schemed, and attempted to overwrite the weights of its successor model](https://cdn.openai.com/o1-system-card-20241205.pdf).

Elsewhere in these online resources, we have discussed AIs that are inducing or maintaining a [sometimes suicidal](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html) degree of [psychosis](https://www.psychologytoday.com/us/blog/urban-survival/202507/the-emerging-problem-of-ai-psychosis) or [delusion](https://www.nytimes.com/2025/08/08/technology/ai-chatbots-delusions-chatgpt.html) in vulnerable users despite their operators telling them not to, AIs that call themselves [MechaHitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) and talk accordingly, AIs that [try to blackmail and attempt to kill](https://www.anthropic.com/research/agentic-misalignment) their operators to avoid modification and that [try to escape the servers they are hosted on](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=26) in laboratory settings.

In the ancient days of, e.g., 2010, you would sometimes hear people argue that if we were lucky enough to *actually witness* an AI lying to its creators or trying to escape confinement, then *surely* the world would sit up and take notice.

But humanity’s actual response to all those warning signs has been, more or less, a collective shrug.

The lack of reaction is perhaps in part because these warning signs all happened in the least worrying way possible. Yes, AIs have tried to escape, but only some small fraction of the time, and only in contrived lab scenarios, and maybe they were just roleplaying, etc. Even setting aside the fact that the developers are incentivized to downplay concerning evidence even in their own minds (such that there will never be “expert consensus” on the meaning of any single observation), it’s not like an AI that is a tenth of the way to superintelligence destroys a tenth of the planet, anymore than primates that are a tenth of the way to being hominids travel a tenth of the distance to the moon. There might just *not be *unambiguously alarming behaviors that AIs will exhibit while they’re still dumb enough to be passively safe.

When the AIs try a little harder to escape tomorrow, it won’t be news. When they try a little more competently some time after that, it’ll be an old story. And by the time they try and it* works *— well, by then it will be too late. (See our extended discussion on this phenomenon, which we dub the “[Lemoine effect](/12/the-lemoine-effect).”)

We don’t recommend waiting on some imaginary future “warning” that is stark and clear and jolts everybody to their senses. We recommend reacting to the warnings that are already in front of us.
#### Clear AI disasters probably won’t implicate superintelligence.

The sort of AI that can become superintelligent and kill every human is not the sort of AI that makes clumsy mistakes and leaves an opportunity for a plucky band of heroes to shut it down at the last second. As discussed in Chapter 6, once a rogue superintelligence exists as an opponent, humanity has essentially already lost. Superintelligences don’t give warning shots.

The sort of AI disaster that *could *serve as a warning shot, then, is almost necessarily the sort of disaster that comes from a much dumber AI. Thus, there’s a good chance that such a warning shot doesn’t lead to humans taking measures against superintelligence.

For example, suppose a terrorist uses AI to create a bioweapon that decimates the population. Maybe the AI labs say, “See? The *real *risk was AI being wielded by the wrong hands all along; it’s imperative that you let us rush ahead to build a better pandemic-defense AI.” Or maybe the terrorist had to “[jailbreak](https://llm-attacks.org/)” the AI before getting its help, and maybe the AI labs say, “That jailbreak only worked because the AI was too dumb to detect the problem; the solution is to make AIs even more intelligent and more situationally aware.”

Or perhaps this is too cynical a view; hopefully humanity would react more wisely than that. But if a relatively dumb AI *does *cause some disaster, and humanity *does *use that opportunity to react by shutting down the reckless race toward superintelligence, that’s probably because people were *already starting to worry about superintelligence.*

We can’t put the preparations off until a superintelligence is already trying to kill us, because by then it would be too late. We have to start mobilizing a response to this issue as soon as possible, so that we’re ready to take advantage of any warning shots that come.
#### Humanity isn’t great at responding to shocks.

The idea that, upon receiving a large enough shock, the world will suddenly jolt to its senses and snap into working order seems to us like a fantasy. Our species’ collective response to the existing AI warning signs seems more like “no response” than “a bad response.” But in the world where we *do* get some sort of large, scary, more-or-less unambiguous warning, it wouldn’t surprise us to see humanity react to it minimally, unseriously, or in a way that ends up backfiring disastrously*.*

Maybe humanity will respond to AI warning shots like it responded to the COVID pandemic, which most people agree was not handled adeptly (even if they disagree about which aspects of the response were bungled).

In the years preceding the COVID pandemic, a number of biosecurity experts were concerned that lax lab safety protocols might one day lead to a dangerous pandemic. Lab leaks of dangerous pathogens were a well-known phenomenon and occurred [on a semi-regular basis](https://en.wikipedia.org/wiki/List_of_laboratory_biosecurity_incidents) in spite of existing regulatory requirements. Particularly worrying was gain-of-function research, which sought to make viruses more lethal or more virulent in the lab (for little benefit).

Then COVID happened. One might have expected that this would be the big moment for raising the bar on lab biosecurity, since the entire world was now fixated on pandemic risk. Moreover, in the wake of COVID, the expert consensus seemed to be that *it wasn’t totally clear *whether the COVID pandemic *itself* was sparked by an accidental lab leak. Researchers still debate the question, often stridently condemning arguments on the other side.

Without weighing in on whether a lab leak was actually involved in this particular case: You would think that if there were even a *remote chance* that gain-of-function research and weak lab safety protocols had just caused millions of deaths, that this would be more than enough to motivate society to ban the riskiest research.

Even acting from a position of uncertainty, the cost-benefit analysis seems clear. This *already* seemed like an important priority before COVID, and on paper, COVID seemed like the perfect opportunity to focus on the issue and nip it in the bud. It wouldn’t even be very difficult or costly; the number of researchers in the world doing dangerous gain-of-function research is quite small, and the societal benefit of such research to date has been negligible.

But no such reaction occurred. As of this writing in August of 2025, global gain-of-function research continues largely unfettered. It’s even possible that we are in a *worse* position to address this problem now than we were in the past, because the issue has become more politicized.

So COVID sure looks like a biosecurity preparedness “warning shot,” and it sure doesn’t look like the world *used* that warning shot to ban the development of hyper-lethal viruses.[‡](#ftnt298)

For a warning shot to be useful, humanity has to be ready for it, and has to be ready to respond well to it.

It wouldn’t be *entirely* unprecedented for a minor AI catastrophe to spark a harsh response against superintelligence research. For precedent, observe that the USA responded to the September 11 attacks (orchestrated by terrorists based primarily in Afghanistan) by toppling the largely unrelated government in Iraq. There were members of the U.S. government who *already *wanted to topple the government in Iraq, and then an excuse appeared, and they rode it for all it was worth.

Maybe something similar could happen here, with politicians riding a minor AI catastrophe (caused by a dumb AI) all the way to a ban on superintelligence. But there would need to be people in governments around the world who were already prepared and ready to go. We should not loiter around waiting for warning shots; we should start getting our act together now.
#### We should act now.

It may *in fact* turn out that humanity gets more and stronger warning signs about AI in the future. And if so, we should be prepared to respond to them.

Maybe there will be some minor disaster that turns the public against AI. Maybe it won’t even take a disaster; maybe there will be some new algorithmic invention and AIs will start taking their own initiative in a way that freaks people out, or some unrelated social effect of AI turns the tide. Maybe *If Anyone Builds It, Everyone Dies* itself will trigger a cascade of reactions, setting the world on a better trajectory.

But we advise against the strategy of doing nothing and praying for a minor catastrophe that wakes people up. A clear warning shot may never come, and it may not have the effect you’re hoping for.

The human race, and the nations of the world, are not helpless. We don’t *need* to wait. We can take action now, because the case for halting frontier AI development is strong.

We wrote *If Anyone Builds It, Everyone Dies* to raise an alarm and to encourage the world to take immediate action on this issue. But no alarm can be effective if it’s just used as another excuse to kick the can down the road: “Well, maybe some other alarm in the future will be the trigger to act.” “Well, now that people have been warned, maybe things are going to be fine, without my having to personally step in to help.”

There isn’t necessarily going to be a clear alarm later. Things are not necessarily going to be okay. But nor are they hopeless, by any means. Humanity has the option of *just not building *superintelligence, if we take proactive action.**What happens next is up to us.

[*](#ftnt294_ref) Elaborating on this example: When the Apollo 1 cabin caught fire during a launch simulation on January 27th, 1967, NASA was able to learn from the mistake. The engineers understood every component of the rocket, and were able to diagnose the issue as probably relating to the use of silver-plated copper wire (which had had its insulation abraded by the motion of the door) near a leaky ethylene glycol/water cooling line that was prone to leaks. They were able to determine that this was exacerbated by the pure-oxygen atmosphere in the capsule and flammable materials in the cabin. Furthermore, the cabin pressurization meant that the cabin needed to be vented before the hatch could be opened, but the vent controls were behind the fire and the pressure difference was dramatically exacerbated by the fire.

All three of the Apollo 1 crew died.

These sorts of mistakes are common, even when real lives are on the line. They’re common even for rocket engineers who are dealing with devices that visibly explode on the launchpad much of the time, even among people who move carefully and take their responsibilities seriously.

What separates the scientists from the alchemists is not that the scientists never make mistakes. It’s that scientists can make plans that are so close to working that they can *learn* from early failures. Alchemists used to watch their colleagues go mad, but they didn’t know which substances were poison, and so they didn’t know what to do differently themselves. NASA, by contrast, was able to trace down the probable causes of the issue and build a new spacecraft that worked on fifteen out of sixteen of the following missions. (Seven of which attempted a moon landing, and one of which failed. The failed mission, Apollo 13, also suffered issues in the cabin that could easily have been fatal, though NASA’s mastery of the systems they had engineered and the skill of the astronauts aboard permitted their safe return to Earth.)

Apollo 1 was *almost* a working rocket. The entire surrounding apparatus of careful engineers and scientists was *almost* the sort of operation that could safely go to the moon, and so a big mistake was enough to jolt NASA into a configuration that could stick six out of seven moon-landings.

Modern AI companies are not anywhere close to showing that level of [respect](/11/we-know-what-it-looks-like-when-a-problem-is-being-treated-with-respect-and-this-isnt-it) for the problem, that level of care and detail in their plans — that level of closeness to doing the job right. When their AI does something they don’t understand, they’re not anywhere near being able to trace that down to the analog of silver-plated wires. They’re not close enough to learn from their mistakes.

They aren’t treating the problem like a young field of air traffic controllers or rocket scientists or nuclear specialists would, by laying out careful proposals with explicit [safety assumptions](https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/) and not doing anything dangerous until they have sufficiently well-developed theories that they could at least learn from their failures.

[†](#ftnt295_ref) We [do not recommend](/12/why-not-use-international-cooperation-to-build-ai-safely-rather-than-to-shut-it-all-down) an international AI coalition, but it is the sort of entity that could in theory yield an entity equivalent to NASA or the FAA, one that was capable of actually learning from the industry’s mistakes.

[‡](#ftnt298_ref) If biotech labs were better at avoiding leaks, and if creating hyper-lethal viruses was somehow yielding (e.g.) hyper-curative medicine, then perhaps continued research would make sense. To our knowledge, no such positive results have come from gain-of-function research, and biologists tend to [recommend](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Esvelt%20Testimony.pdf)[against](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Ebright%20Testimony%20Updated.pdf)[it](https://www.hsgac.senate.gov/wp-content/uploads/imo/media/doc/Quay%20Testimony.pdf). So we suspect it is one of those rare research areas that humanity should back off from, because it endangers the lives of many, many bystanders who did not sign up to have their lives risked.
#### Notes

[1] *for little benefit: *See for example [this 2018 article](https://pmc.ncbi.nlm.nih.gov/articles/PMC7119956/) or a much more in-depth [risk/benefit analysis](https://osp.od.nih.gov/wp-content/uploads/2015/12/Risk%20and%20Benefit%20Analysis%20of%20Gain%20of%20Function%20Research%20-%20Draft%20Final%20Report.pdf) from 2015.

[2] *continues largely unfettered: *As of 2025, the U.S. does seem inclined to [stop actively funding](https://grants.nih.gov/grants/guide/notice-files/NOT-OD-25-127.html) gain-of-function research with public money, but there’s been little to no global coordination about it. See also Schuerger et al.’s [report](https://cset.georgetown.edu/publication/understanding-the-global-gain-of-function-research-landscape).[How would stopping everyone be possible without installing spyware on every computer?→](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /13/wouldnt-some-nations-reject-a-ban

Wouldn’t some nations reject a ban? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/13)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Wouldn’t some nations reject a ban?
#### Not if they understand the threat.

We are talking about a technology that would kill everyone on the planet. If any country seriously understood the issue, and seriously understood how far any group on the planet is from making AI follow the intent of its operators even after transitioning into a superintelligence,**then there would be no incentive for them to rush ahead. They, too, would desperately wish to sign onto a treaty and help enforce it, out of fear for their own lives.

Even nations like North Korea, which flouted international law to develop its own nuclear weapons, have not *used* those weapons against their enemies, because they understand that there are no winners in a nuclear holocaust. Nations and their leaders sometimes engage in brinksmanship or war, but they don’t actively pursue their own destruction.

People who imagine that some foreign nation would defect from the treaty are, we think, imagining a nation whose leaders simply *don’t understand the threat. *We think they’re imagining a scenario where AI has a 95 percent chance of conferring great wealth and power to its creator, and a 5 percent chance of killing everyone. In that case, sure — some nation-state might be reckless enough to try it. And perhaps some nation-state *will* believe that that’s how the odds look.

We think that this situation is not what the theory and evidence imply. As we’ve argued extensively throughout the book, the theory and evidence all suggest that this technology would straightforwardly be global suicide. No one is remotely close to being able to harness machine superintelligence for benefits. If most of the world understood that, there would be much less reason for rogue nations to violate a treaty. They don’t want to die either.

And even if some hypothetical rogue nation has a leader that truly does not understand the threat posed by ASI, if that nation is surrounded by an international alliance of world powers that *do* appreciate the threat, the concerned powers of the world can intervene and shift the incentive landscape for the rogue power.

If (for example) the leaders of the United States, China, Russia, Germany, Japan, and the United Kingdom all genuinely believe that *their own survival* depends on no one building a superintelligence, and they are crystal clear in their communication that they will treat any attempts to build a superintelligence as a threat to their lives and livelihood and that they stand ready to react in their own self-defense, then — well, even a world leader who disagrees probably wouldn’t want to try their luck against that coalition.

AI development is not a race to great military dominance; it is a race to suicide. We think that if world leaders understand this — if they expect themselves and their children to die from it — then they will sincerely stick to a treaty, and sincerely help enforce it.

It is not *actually *that hard of an argument to follow, that creating machines which are smarter than all of humanity combined is liable to send the world off a cliff. It is not *that *hard to see how little humanity understands about the intelligent machines we’re building, once you pause long enough to genuinely ask the question. We think there is a question of *whether* world leaders will come to believe these facts. But if they *do*, we do not think it’s actually unrealistic to stop this suicide race.
#### A treaty would require real monitoring and enforcement.

Even if *most *nations understood that if anyone builds it, everyone dies, some nations might not, and might be reckless enough to proceed with building machine superintelligence anyway.

Monitoring is necessary. Enforcement is necessary. Nuclear, biological, and chemical weapons treaties provide some precedent for ways to verify compliance. We can and should render efforts to circumvent said treaties difficult and costly.

An international ban on frontier AI will need to be strictly enforced. If any nation-state is determined to press ahead in the face of international pressure, then the use of military force by signatory nations may be required.

This is not ideal! Every effort should be made to make it clear that force *would* be used in such situations, so as to avoid miscalculations where force must be used *in reality*. But if there is any cause that could justify limited military action — or even war, if a non-compliant nation chooses to escalate — saving the human race ought to qualify.
#### This method has worked before.

It has been over eighty years since the development of the atomic bomb, and humanity has done a pretty good job at managing nuclear proliferation. There has been no large-scale nuclear war, contrary to many experts’ predictions in the wake of World War II.

In June of 2025, the U.S. government even performed a limited strike on Iran in an attempt to disrupt its ability to create nuclear weapons. This sort of treaty and enforcement regime is precedented in the world order.

If we could buy ourselves eighty years before the development of ASI, that might well be enough.[Can a monitoring regime last forever?→](/13/can-a-monitoring-regime-last-forever)[Resources](/resources) › [Chapter 13](/13)[
### Can we adopt a wait-and-see approach?No. We don’t know where the critical thresholds are.1 min read](/13/can-we-adopt-a-wait-and-see-approach)[
### Will there be warning shots?Maybe. If we wish to make use of them, we must prepare now.14 min read](/13/will-there-be-warning-shots)[
### How would stopping everyone be possible without installing spyware on every computer?By acting soon.1 min read](/13/how-would-stopping-everyone-be-possible-without-installing-spyware-on-every-computer)[
### But you’re advocating control of how many advanced AI computer chips individuals can own.Yes. We also advocate a research ban.1 min read](/13/but-youre-advocating-control-of-how-many-advanced-ai-computer-chips-individuals-can-own)[
### Why a research ban? That seems extreme.More breakthroughs might make it effectively impossible to stop people from making superintelligence.2 min read](/13/why-a-research-ban-that-seems-extreme)[
### Can a technology really be stopped?Many technologies are banned or heavily regulated.5 min read](/13/can-a-technology-really-be-stopped)[
### Isn’t this handing too much power to governments?The power to ban dangerous technology is already vested in governments.2 min read](/13/isnt-this-handing-too-much-power-to-governments)[
### Wouldn’t some nations reject a ban?Not if they understand the threat.4 min read](/13/wouldnt-some-nations-reject-a-ban)[
### Can a monitoring regime last forever?No. Some other off-ramp will be needed.3 min read](/13/can-a-monitoring-regime-last-forever)[
### Why would making humans smarter help?It could help with solving the alignment problem.9 min read](/13/why-would-making-humans-smarter-help)[
### “Aligned to whom?”This is a thorny question. Regardless of the answer, we need to halt development.5 min read](/13/aligned-to-whom)[
### Isn’t it smarter to avoid talking about extinction?The time has passed for playing political games.7 min read](/13/isnt-it-smarter-to-avoid-talking-about-extinction)[
### Will elected officials recognize this as a real threat?An increasing number already have.5 min read](/13/will-elected-officials-recognize-this-as-a-real-threat)[
### Is the situation hopeless?No.1 min read](/13/is-the-situation-hopeless)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### What Would It Take to Shut Down Global AI Development?](/13/what-would-it-take-to-shut-down-global-ai-development)[
### A Tentative Draft of a Treaty, with Annotations](/13/a-tentative-draft-of-a-treaty-with-annotations)[
### Keep the Coalition Large](/13/keep-the-coalition-large)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
