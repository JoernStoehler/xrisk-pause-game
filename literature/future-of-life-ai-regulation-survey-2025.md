---
title: "The U.S. Public Wants Regulation (or Prohibition) of Expert-Level and Superhuman AI"
author: "Future of Life Institute"
year: 2025
source_url: "https://futureoflife.org/recent-news/americans-want-regulation-or-prohibition-of-superhuman-ai/"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Survey of American public attitudes toward advanced AI regulation, safety testing, and development timelines"
---

# The U.S. Public Wants Regulation (or Prohibition) of Expert-Level and Superhuman AI (2025)

## Overall Support for Advanced AI Regulation

### Robust Regulation Support

- **73%** support robust regulation on advanced AI development
- **12%** oppose strong regulation
- **15%** unsure

This represents overwhelming public support for regulatory oversight of frontier AI development.

## Superhuman AI: Development vs. Prohibition

### Safety-First Development Preference

- **64%** believe superhuman AI should not be developed until proven safe, or should never be developed
- **30%** support developing superhuman AI under current conditions
- **Only 5%** support developing either technology "as quickly as possible"

**Key insight**: Public prioritizes safety verification over speed, even for transformative technologies.

## Expert-Level AI Development

### Current Development Concerns

- **57%** oppose developing expert-level AI under current conditions
- **40%** support pausing expert-level AI development until safety is proven
- **17%** believe expert-level AI should never be developed

### Conditional Development Support

For those supporting expert-level AI development:
- Majority support **pharmaceutical-style regulation**
- Require **extensive testing before deployment**
- Want **independent verification** of safety

## Timeline Expectations vs. Preferences

### When Americans Expect Advanced AI

- **49%** expect expert-level AI within 2 years
- **49%** expect superhuman AI within 5 years

### Gap Between Expectation and Preference

Americans expect faster development than they preferâ€”suggesting unease about development trajectories they see as inevitable or out of their control.

## Regulatory Model Preferences

### Pharmaceutical-Style Approval Model

- **64%** favor pharmaceutical-style regulation with extensive testing before deployment
- Indicates preference for proven-safety before release rather than post-hoc regulation

### Key Requirements for Acceptable Development

For respondents accepting advanced AI development:
1. Extensive pre-deployment testing requirements
2. Independent verification of safety claims
3. Regulatory approval before release
4. Ongoing monitoring after deployment

## Primary Risk Concerns

### Why Restrict Advanced AI Development?

Top reasons cited against developing expert-level or superhuman AI:

- **Human extinction or replacement**: Core existential concern
- **Concentration of power** in technology companies
- **61%+** worry advanced AI will cause significant harm without proper safeguards
- **Inability to control** the systems once developed

## Trust in Decision-Makers and Institutions

### Who Should Guide Advanced AI Development?

**Trusted Institutions (High Confidence):**
- International scientific organizations: Highest trust
- University researchers: 69% trust for safety guidance
- Nonprofit AI safety researchers: 69% trust

**Distrusted Institutions (Low Confidence):**
- AI company leadership: Significantly lower confidence
- **59%** have little or no confidence in U.S. companies developing AI responsibly

### Institutional Trust Gap

Large gap between trust in independent researchers and industry executives indicates public wants oversight outside industry control.

## Safety Testing and Verification

### Independent Testing Demand

- **72%** want independent experts conducting safety evaluations
- Indicates skepticism of industry self-testing claims
- Preference for third-party verification

### Government Testing Role

- **48%** support government-led safety testing
- Lower than independent expert testing preference
- Reflects concerns about government technical competence

## International Cooperation on Advanced AI

### Preference for Multilateral Approaches

- **42%** prefer collaboration with broad coalition of allies
- **19%** support partnerships with smaller ally groups
- **14%** prefer independent U.S. development

Americans see advanced AI as requiring international coordination rather than national competition.

## Existential Risk Awareness

### Concern About Human Extinction from AI

- Significant portion cite "human extinction or replacement" as reason for restricting advanced AI
- Indicates existential risk has entered mainstream public consciousness
- Growing from baseline concern to policy-relevant consideration

### Risk Assessment

- Public views advanced AI (expert-level and superhuman) as qualitatively different from current AI
- Demands different regulatory approach than consumer AI applications

## Development Speed vs. Safety Tradeoff

### Safety Preferred When Trade-Offs Explicit

- **80%** believe government should maintain safety rules even if AI development slows
- **9%** prioritize development speed over safety
- **11%** unsure

Overwhelming agreement that development speed should not override safety requirements.

## Policy Implementation Preferences

### Regulatory Structures

Public supports:
- Pre-deployment safety testing and approval
- Independent third-party evaluation
- International coordination
- Ongoing monitoring requirements

### What Public Rejects

- Unilateral national development races
- Industry self-regulation without oversight
- Speed-first development approaches
- Minimal government involvement

## Demographic and Political Dimensions

### Cross-Party Agreement on Safety

- Strong bipartisan consensus on importance of safety for advanced AI
- Disagreement more about regulatory implementation details than core safety principle

### Knowledge and Concern Correlation

- Higher AI knowledge correlates with nuanced views
- Across knowledge levels, safety prioritization remains dominant

## Vulnerable Populations Concerns

### Equity in Advanced AI Development

- Concerns about concentrated power suggest worry about unequal impacts
- Questions about who benefits from advanced AI capabilities
- Desire for democratic input into development decisions

## International Dimension

### Global Competition Context

- Public aware of international AI competition
- But majority rejects "race" framing that would compromise safety
- Wants cooperative development approach rather than competitive one

## Takeaways from Future of Life Survey

1. **Regulation Is Baseline Expectation**: 73% support represents overwhelming mandate for oversight
2. **Safety Before Scale**: Public prioritizes safety verification before deployment
3. **Expert Distrust**: Large gap between academic/international and industry trust
4. **Existential Concern**: Human extinction from AI is salient public concern
5. **Pharmaceutical Model Appeal**: Pre-approval testing framework resonates with public
6. **International Cooperation**: Rather than nationalist approach, public prefers multilateral development
7. **Speed-Safety Trade**: Clear public preference for slower, safer development
8. **Power Concentration Fear**: Concern about tech industry monopolies on transformative technology

## Relevance to AI CEO Game Scenarios

- Public will demand advanced AI regulation as capability improvements become apparent
- Media will frame development race as dangerous if positioned as speed-first
- Political pressure will mount for international coordination on advanced AI
- Companies will face credibility challenges: need demonstrated safety commitment
- Existential risk framing becomes politically mainstream with advanced AI announcements
- Worker displacement concerns compound with capability concerns for political pressure
- Independent oversight mechanisms become necessary for public license to operate
