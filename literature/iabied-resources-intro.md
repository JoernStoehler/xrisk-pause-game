---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Introduction"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Introduction from the companion website"
---

# Online Resources: Introduction

## /intro/ai-experts-on-catastrophe-scenarios

AI Experts on Catastrophe Scenarios | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/intro#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## AI Experts on Catastrophe Scenarios

In a[2022 survey](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) of 738 attendees of the academic AI conferences NeurIPS and ICML, 48 percent of respondents thought there was at least a 10 percent chance that AI’s outcome will be “extremely bad (e.g., human extinction).” Concerns about AI causing an unprecedented disaster are widespread in this field.

Below, we’ve collected comments from prominent AI scientists and engineers on catastrophic AI outcomes. Some of these scientists give their “p(doom)” — i.e., their probability of AI causing human extinction or similarly disastrous outcomes.[*](#ftnt9)

From **Geoffrey Hinton **([2024](https://youtu.be/PTF5Up1hMhw?t=2285)), recipient of a Nobel Prize and a Turing Award for sparking the deep learning revolution in AI, speaking on his personal estimates:[†](#ftnt10)

I actually think the risk [of the existential threat] is more than 50 percent.

From **Yoshua Bengio **([2023](https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340)), Turing Award recipient (with Hinton and Yann LeCun) and the most cited living scientist:

We don’t know how much time we have before it gets really dangerous. What I’ve been saying now for a few weeks is “Please give me arguments, convince me that we shouldn’t worry, because I’ll be so much happier.” And it hasn’t happened yet. […] I got around, like, 20 percent probability that it turns out catastrophic.

From **Ilya Sutskever **([2023](https://openai.com/index/introducing-superalignment/)), co-inventor of AlexNet, former chief scientist at OpenAI, and (with Hinton and Bengio) one of the three most highly cited scientists in AI:

[T]he vast power of superintelligence could also be very dangerous, and could lead to the disempowerment of humanity or even human extinction. While superintelligence seems far off now, we believe it could arrive this decade. […]

Currently, we don’t have a solution for steering or controlling a potentially superintelligent AI, and preventing it from going rogue. Our current techniques for aligning AI, such as reinforcement learning from human feedback⁠, rely on humans’ ability to supervise AI. But humans won’t be able to reliably supervise AI systems much smarter than us, and so our current alignment techniques will not scale to superintelligence. We need new scientific and technical breakthroughs.

From **Jan Leike** ([2023](https://80000hours.org/podcast/episodes/jan-leike-superalignment/)), alignment science co-lead at Anthropic and former co-lead of the superalignment team at OpenAI:

[interviewer: “I didn’t spend a lot of time trying to precisely pin down my personal p(doom). My guess is that it’s more than 10 percent and less than 90 percent.”]

[Leike:] That’s probably the range I would give too.

From **Paul Christiano **([2023](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom)), Head of Safety at the U.S. AI Safety Institute (based in NIST) and inventor of reinforcement learning from human feedback (RLHF):

Probability that most humans die within 10 years of building powerful AI (powerful enough to make human labor obsolete): 20% […]

Probability that humanity has somehow irreversibly messed up our future within 10 years of building powerful AI: 46%

From **Stuart Russell **([2025](https://www.newsweek.com/deepseek-openai-race-human-extinction-2023482)), Smith-Zadeh Chair in Engineering at UC Berkeley and co-author of the top undergraduate AI textbook, *Artificial Intelligence: A Modern Approach*:

The “AGI race” between companies and between nations is somewhat similar [to the Cold War race to build larger nuclear bombs], except worse: Even the CEOs who are engaging in the race have stated that whoever wins has a significant probability of causing human extinction in the process, because we have no idea how to control systems more intelligent than ourselves. In other words, the AGI race is a race towards the edge of a cliff.

From **Victoria Krakovna **([2023](https://theinsideview.ai/victoria)), research scientist at Google DeepMind and co-founder of the Future of Life Institute:

[interviewer: “This is not a very pleasant thing to think about, but what would you consider is the probability of Victoria Krakovna dying from AI before 2100?”]

[Krakovna:] I mean, 2100 is very far away, especially given how quickly the technology’s developing right now. I mean, off the top of my head, I would say like 20 percent or something.

From **Shane Legg **([2011](https://baserates-test.vercel.app/posts/No5JpRCHzBrWA4jmS/q-and-a-with-shane-legg-on-risks-from-ai)), co-founder and chief AGI scientist at Google DeepMind:

[interviewer: “What probability do you assign to the possibility of negative/extremely negative consequences as a result of badly done AI? […] Where ‘negative’ = human extinction; ‘extremely negative’ = humans suffer”]

[Legg:] [W]ithin a year of something like human level AI[…] I don’t know. Maybe 5%, maybe 50%. I don’t think anybody has a good estimate of this. If by suffering you mean prolonged suffering, then I think this is quite unlikely. If a super intelligent machine (or any kind of super intelligent agent) decided to get rid of us, I think it would do so pretty efficiently.

From **Emad Mostaque** ([2024](https://perma.cc/HK8L-2KEL)), founder of Stability AI, the company behind Stable Diffusion:

My P(doom) is 50%. Given an undefined time period the probability of systems that are more capable than humans and likely end up running all our critical infrastructure wiping us all out is a coin toss, especially given the approach we are taking right now.

From **Daniel Kokotajlo** ([2023](https://www.lesswrong.com/posts/xDkdR6JcQsCdnFpaQ/adumbrations-on-agi-from-an-outsider?commentId=sHnfPe5pHJhjJuCWW)), AI governance specialist, OpenAI whistleblower, and executive director of the AI Futures Project:

I think AI doom is 70% likely and I think people who think it is less than, say, 20% are being very unreasonable[.]

From **Dan Hendrycks **([2023](https://perma.cc/9QKW-6CUD)), machine learning researcher and director of the Center for AI Safety:

[M]y p(doom) > 80%, but it has been lower in the past. Two years ago it was ~20%.

All of the above researchers signed the [Statement on AI Risk](https://aistatement.com/) we opened the book with, which says:

Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.

Other prominent researchers who signed the statement included: ChatGPT architect John Schulman; former Google director of research Peter Norvig; Microsoft chief scientific officer Eric Horvitz; AlphaGo research lead David Silver; AutoML pioneer Frank Hutter; reinforcement learning pioneer Andrew Barto; GANs inventor Ian Goodfellow; former Baidu president Ya-Qin Zhang; public-key cryptography inventor Martin Hellman; and Vision Transformer research lead Alexey Dosovitskiy. The list goes on, with further signatories including: Dawn Song, Jascha Sohl-Dickstein, David McAllester, Chris Olah, Been Kim, Philip Torr, and hundreds of others.

[*](#ftnt9_ref) We have concerns with the practice of trying to assign a “p(doom).” Assigning a single probability — as opposed to multiple probabilities that each assume a different response society could choose — strikes us as defeatist. There’s a world of difference between somebody who has high p(doom) because they think the world mostly *can’t *prevent catastrophe, versus somebody who has high p(doom) because they think the world *can* prevent catastrophe but *won’t*.

If it turns out that most people have a high p(doom) for the latter reason, but everyone assumes it’s for the former reason, then people’s statements of high p(doom) could serve as a self-fulfilling prophecy, putting us on track for a disaster that was completely preventable.

We also have the impression that many people in Silicon Valley trade “p(doom)” numbers a bit like baseball cards, in a way that often seems divorced from reality. If you’re paying attention, then even a probability as low as 5 percent of *killing every human being on the planet *should be an obvious cause for extreme alarm. It’s far beyond the threat level you would need to justify shutting down the entire field of AI *immediately*. People seem to lose sight of this reality surprisingly quickly, once they get into the habit of ghoulishly trading p(doom) numbers at parties as though the numbers were a fun science fiction story and not a statement about *what’s actually going to happen to all of us*.

This isn’t to say that people’s p(doom) numbers are anywhere close to reality. But at the very least, you should read these numbers as experts throughout the field reporting that we’re facing a genuine emergency.

[†](#ftnt10_ref) Contrary to what Hinton says earlier in the video, Yudkowsky’s confidence regarding the dangers is not “99.999” percent; five nines would constitute an insane degree of confidence.[When Leo Szilard Saw the Future→](/intro/when-leo-szilard-saw-the-future)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/are-you-suggesting-that-chatgpt-could-kill-us-all

Are you suggesting that ChatGPT could kill us all? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you suggesting that ChatGPT could kill us all?
#### No. The worry is about forthcoming advances in AI.

Part of why you’re reading this book now is that developments like ChatGPT have brought AI into the news. The world is now beginning to discuss AI progress and the way that AI impacts society. This presents a natural opportunity to discuss smarter-than-human AI, and how the current situation is not looking good.

We, the authors, have been working in this field for a long time. Recent AI progress informs our views, but our worries weren’t sparked by ChatGPT, nor by earlier large language models. We have been doing technical research to try to ensure that smarter-than-human AI goes well for decades (Soares since 2013, Yudkowsky since 2001). But we’ve recently seen evidence that this may be a conversation the world is ready to have. And it’s a conversation that we plausibly *need *to have now, or the world may lose its window of opportunity to respond.

The field of AI is progressing, and eventually (we don’t know when) it’s going to progress to the point where it makes AI that is smarter than we are. That is the explicit goal of all of the leading AI companies:

We are now confident we know how to build AGI [artificial general intelligence] as we have traditionally understood it. […] We are beginning to turn our aim beyond that, to superintelligence in the true sense of the word. We love our current products, but we are here for the glorious future. With superintelligence, we can do anything else.[](https://blog.samaltman.com/reflections)

I think [powerful AI] could come as early as 2026. […] By powerful AI, I have in mind an AI model […] with the following properties: In terms of pure intelligence, it is smarter than a Nobel Prize winner across most relevant fields — biology, programming, math, engineering, writing, etc. This means it can prove unsolved mathematical theorems, write extremely good novels, write difficult codebases from scratch, etc.[](https://www.darioamodei.com/essay/machines-of-loving-grace)

Overall, we are focused on building full general intelligence. All of the opportunities that I’ve discussed today are downstream of delivering general intelligence and doing so efficiently.[](https://www.facebook.com/share/p/16STVBshtn/)[](https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26)[](https://www.bloomberg.com/news/articles/2025-06-10/zuckerberg-recruits-new-superintelligence-ai-group-at-meta)

I think in the next five to ten years, there will be maybe a 50 percent chance that we’ll have what we’d define as AGI.[](https://youtu.be/CRraHg4Ks_g?feature=shared&t=41)

Wes: So, Demis, are you trying to cause an intelligence explosion?

Demis: No, not an uncontrolled one…[](https://x.com/WesRothMoney/status/1926669591163621789)

They are putting their money where their mouths are. [Microsoft](https://www.reuters.com/technology/artificial-intelligence/microsoft-plans-spend-80-bln-ai-enabled-data-centers-fiscal-2025-cnbc-reports-2025-01-03/), [Amazon](https://www.datacenterdynamics.com/en/news/amazon-2025-capex-to-reach-100bn-aws-revenue-hit-100bn-in-2024/), and [Google](https://www.datacenterdynamics.com/en/news/google-expects-2025-capex-to-surge-to-75bn-on-ai-data-center-buildout/) all announced plans to spend $75 to $100 billion on AI datacenters in 2025. The startup xAI [bought out the social media site X.com](https://www.reuters.com/markets/deals/musks-xai-buys-social-media-platform-x-45-billion-2025-03-28/) with a valuation of $80 billion, about twice as high as X itself, shortly before [raising $10 billion](https://www.cnbc.com/2025/07/01/elon-musk-xai-raises-10-billion-in-debt-and-equity.html) to support a massive datacenter and further develop its AI, Grok. OpenAI has announced the $500 billion [Project Stargate](https://openai.com/index/announcing-the-stargate-project/), in partnership with Microsoft and others.

Meta CEO Mark Zuckerberg has [said](https://www.datacenterdynamics.com/en/news/zuckerberg-says-meta-will-spend-hundreds-of-billions-of-dollars-on-ai-infrastructure-over-the-long-term/) that Meta [expects to spend $65 billion](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/) on AI infrastructure this year, and “hundreds of billions” on AI projects in the coming years. Meta has already invested $14.3 billion in ScaleAI and hired its CEO to run the new [Meta Superintelligence Labs](https://www.bloomberg.com/news/articles/2025-06-30/zuckerberg-announces-meta-superintelligence-effort-more-hires), in the process poaching over a dozen top researchers from rival labs with offers as high as $200 million for a single researcher.

None of this means that smarter-than-human AI is just around the corner. But it does mean that all of the major companies are trying as hard as they can to build it, and that AIs like ChatGPT are the result of this research program. These companies aren’t setting out to make chatbots. They’re setting out to make superintelligences, and chatbots are a pit stop along the way.

Our own view, after decades of trying to better understand this issue and think seriously about future developments, is that there isn’t a principled barrier to researchers achieving a breakthrough tomorrow and succeeding in building smarter-than-human AI.

We don’t know whether that threshold will in fact be hit in the near future, or whether it’s still a decade away, etc. History shows that timing new technologies is a lot harder than predicting that a technology will be developed at all. But we believe that the evidence of danger is vastly greater than is needed to justify an aggressive international response today. That argument is, of course, sketched out in the book.
#### Notes

[1] *poaching: *From [Bloomberg](https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million), July 2025: “Meta CEO Mark Zuckerberg has now successfully hired more than ten OpenAI researchers, as well as top researchers and engineers from Anthropic, Google and other startups.”[Aren’t people always panicking and overreacting to things?→](/intro/arent-people-always-panicking-and-overreacting-to-things)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/arent-people-always-panicking-and-overreacting-to-things

Aren’t people always panicking and overreacting to things? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Aren’t people always panicking and overreacting to things?
#### Yes. But this doesn’t mean that nothing is ever *actually* dangerous.

Sometimes people overreact to problems. Some people are fatalistic. Some social panics are groundless. None of this means that we live in a perfectly safe world.

Germany in 1935 was not a good place for Jews, Romani, or various other groups of people to stay. Some saw the warning signs and left. Others dismissed the warnings as alarmist and died.

The threat of nuclear annihilation was real, but humanity rose to the occasion and the Cold War never turned hot.

Chlorofluorocarbons really were burning a hole in the ozone layer, until they were successfully banned by international treaty. Afterward, the ozone layer recovered.

Some dangers that people warn about are fake. Others are real.

Humanity doesn’t always overreact to a challenge. Nor does it always underreact. In some cases, humanity even manages to do both simultaneously, e.g., countries building huge battleships for use in the next war when in fact they should have been building aircraft carriers. There isn’t a simple solution like “just ignore every supposed technological risk” or “just assume that every technological risk is real.” To figure out what’s true, you have to actually look at the details of each case.

(For more on this topic, refer to the introduction of the book.)[When is this worrisome sort of AI going to be developed?→](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai

Can we use past progress to extrapolate when we’ll build smarter-than-human AI? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we use past progress to extrapolate when we’ll build smarter-than-human AI?
#### We don’t have a good enough understanding of intelligence for that.

One class of successful predictions involves taking a straight line on a graph, one that has been steady for many years, and predicting that the straight line continues for at least another year or two.

This doesn’t always work. Trend lines sometimes change. But it often works reasonably well; it is a case where people make successful predictions in practice.

The great trouble with this method is that often what we really want to know is not “how high up will this line on the graph be by 2027?” but rather, “What happens, qualitatively, if this line keeps going up?” What height of the line corresponds to important real-world outcomes?

And in the case of AI, we just don’t know. It’s easy enough to pick some measure of artificial intelligence that forms a straight line on a graph (such as “[perplexity](https://en.wikipedia.org/wiki/Perplexity)”) and project that line outwards. But nobody knows what future level of “perplexity” corresponds to which level of qualitative chess-playing ability. People can’t predict that in advance; they’ve just got to run the AI and find out.

Nobody knows where the “now it has the ability to kill everyone” line falls on that graph. All they can do is run the AI and find out. So extrapolating the straight line on the graph doesn’t help us. (And that’s even before the graph is rendered irrelevant by algorithmic progress.)

For that reason, we don’t spend time in the book extrapolating lines on graphs to predict exactly when somebody will throw 1027 floating-point operations at training an AI, or what consequences this would have. That’s a hard call. The book focuses on what seem to us to be the easy calls. This is a narrow range of topics, and our ability to make a small number of important predictions in that narrow domain doesn’t justify making arbitrary prognostications about the future.[What are your incentives and conflicts of interest, as authors?→](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/isnt-this-ai-stuff-just-science-fiction

Isn’t this AI stuff just science fiction? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Isn’t this AI stuff just science fiction?
#### We can’t learn much from a topic’s prevalence in fiction.

Smarter-than-human AI hasn’t been built yet, but it has been depicted in fiction. We recommend against anchoring on these depictions, however. Real AI probably won’t be much like fictional AI, for reasons we’ll dive into in Chapter 4.

AI isn’t the first technology that was anticipated by fiction. [Heavier-than-air flight](https://www.weslpress.org/9780819577269/robur-the-conqueror/) and [travel to the moon](https://www.imdb.com/title/tt0000417/) were both depicted before their time. And the general idea of nuclear weapons was anticipated by H. G. Wells, one of the first science fiction writers, in a 1914 novel called *[The World Set Free](https://ahf.nuclearmuseum.org/ahf/key-documents/hg-wells-world-set-free/)*. Wells didn’t get the details right; he wrote about a bomb that went on burning intensely for days, rather than a bomb that exploded all at once and left lingering death behind. But Wells had the general idea of a bomb that ran on nuclear rather than chemical energy.

In 1939, Albert Einstein and Leo Szilard sent a letter to President Roosevelt that called for the U.S. to try to outrace Germany in building an atomic bomb. We could imagine a world where Roosevelt had first encountered the notion of nuclear bombs in Wells’s novel, causing him to dismiss the idea as science fiction.

As it happens, in real life, Roosevelt took the idea seriously, at least enough to create the Advisory Committee on Uranium. But this case demonstrates the peril of dismissing ideas just because a fiction writer talked about a similar-sounding idea in the past.

Science fiction can mislead you because you assume it’s true, or it can mislead you because you assume it’s *false*. Science fiction authors aren’t prophets, but they also aren’t anti-prophets whose words are guaranteed to be wrong. In the vast majority of cases, we’re better off ignoring fiction and analyzing technologies and scenarios on their own terms.

To predict what happens in reality, there is no substitute for just thinking through the arguments and weighing the evidence.
#### The consequences of AI are inevitably going to be weird.

We sympathize with the reaction that AI is *weird*, and that it would transform the world and violate the status quo. All of us have intuitions adapted, to some degree, to a world in which humans are the only species capable of feats like building a power plant. All of us have intuitions adapted to a world where machines, throughout all of human history, have always been unintelligent tools. One thing we can be very confident of is that a future with smarter-than-human AIs would look *different*.

Large, lasting changes to the world don’t happen every day. The heuristic “nothing ever happens” performs great most of the time, but the times when it fails are some of the most important times in history to be paying attention. Much of the point of thinking about the future at all is to anticipate those moments when something big does happen, so that preparation is possible.

One way to overcome a bias toward the status quo is to recall the historical record, as discussed in the introduction.

Sometimes, particular inventions end up upending the world. Consider the steam engine, and the many other technologies it helped enable during the Industrial Revolution, rapidly transforming human life:

Is the advent of truly general AI a similarly consequential development? It seems that artificial intelligence would be *at least *as consequential as the Industrial Revolution. Among other things:
- 

AI is likely to enable technological progress to develop much faster. As we’ll discuss in Chapter 1, machines can operate much faster than the human brain. And humans can improve AI — and AI will eventually be able to improve itself — until machines are far better than humans at making scientific discoveries, inventing new technologies, et cetera.

For all of human history, the machinery of the human brain remained fundamentally unchanged, even as humanity produced ever-more-impressive feats of engineering. When the machinery of cognition begins to improve in its own right, when it becomes capable of improving itself, we should expect *many different things* to start changing *very quickly*.
- Additionally, as we’ll discuss in Chapter 3, sufficiently capable AIs are likely to have goals of their own. If AIs were essentially just faster and smarter human beings, then that would be a huge deal in its own right. But AIs will instead be, in effect, a totally new species of intelligent life on Earth — one with its own goals, which are likely (as we’ll discuss in Chapters 4 and 5) to importantly diverge from human goals.

On the face of it, it would be surprising if these two major developments could occur *without* upending the existing world order. Believing in a “normal” future seems to require believing that machine intelligence will never surpass human intelligence at all. This never seemed like a truly viable option, and it’s become far harder to believe in 2025 than it was in 2015 or 2005.
#### The long-term future will likewise be weird.

If you look too far into the future, the result is going to be weird somehow. The 21st century looks downright bizarre from the perspective of the 19th century, which looked bizarre from the perspective of the 17th century. AI accelerates this process and adds a very novel player to the game board.

One aspect of the future that seems predictable today is that advanced technological species won’t remain stuck on their own planet indefinitely. Right now, the night sky is full of stars just burning off their energy. But nothing stops life from building the technology to travel the stars and harvest that energy toward some purpose.

There are some physical limitations on how *quickly *that travel can be done, but it looks like there are no limitations on doing it eventually. There’s nothing stopping us from eventually developing the kinds of interstellar probes that can go out and extract resources from the universe writ large and convert these resources into flourishing civilizations, with a side order of more self-replicating probes to colonize yet more regions of space. If we displace ourselves with AIs, there’s nothing stopping those AIs from doing the same, but swapping out “flourishing civilizations” for whatever ends the AI is pursuing.

In the same way that life spread to barren rocks on Earth until the whole world was teeming with organisms, we can expect life (or machines built by life) to eventually spread to uninhabited parts of the universe, until it’s just as strange to find a lifeless solar system as it would be to find a lifeless island on Earth today, devoid even of bacteria.

At present, most of the matter in the universe, like stars, is arranged by happenstance. But the sufficiently long-term future is almost surely one in which most of the matter is arranged according to some design, i.e., according to the preferences of whichever entities manage to harvest and repurpose the stars.

Even if nothing on Earth ever spreads through the cosmos, and even if most intelligent life that arises in distant galaxies never leaves its home planet, it only takes *one* spacefaring intelligence anywhere in the universe to light the spark and start spreading through the universe, traveling to new star systems and using the resources there to build more probes to expand outwards to yet more star systems — just as it only took one self-replicating microorganism (and a bit of exponential growth) to turn a lifeless planet into a world teeming, on every island, with life.

So the future will look different from the present. Indeed, we can expect it to look radically different. The stars themselves will predictably be transformed, in the long run, by whatever biological species or AIs are looking for more resources — even if we can’t say much today about what that species might look like, or about what ends the universe’s resources might be put toward.

Predicting the *details* seems difficult, verging on impossible. That’s a hard call. But predicting the transformation of the universe into a place where most matter is harvested and put toward *some *purpose, whatever that may be? That is an easier call, even if it’s counterintuitive and weird to a civilization that has barely begun to extract resources from stars at all.

A million years from now, we shouldn’t expect the future to look like the year 2025, with a bunch of hairless apes messing around on the surface of Earth. Long before that, either we’ll have killed ourselves, or our descendants will have gone out to explore the cosmos themselves.[*](#ftnt8)

It’s definitely going to get weird for humanity.**The question is when.
#### The future will hit us fast.

Technologies like AI mean that the future may come knocking at our door soon, and its effects may hit us hard.

The Industrial Revolution transformed the world very quickly, by the standards of pre-modern history. *Homo sapiens *reshaped the world very quickly, by the standards of evolutionary processes. Life reshaped the world very quickly, by the standards of cosmological and geological processes. New processes for changing the world can reshape the world very quickly, as measured by the old standard.

Humanity looks to be on the brink of another radical transformation, where machines can begin reshaping the world at machine speeds, which far outstrip biological speeds. We’ll have more to say in Chapters 1 and 6 about just how well machine intelligence would measure up against human intelligence. But minimally, we need to take seriously the possibility that the development of smarter-than-human machines would radically change the world at high speed. That sort of thing has happened over and over again throughout the course of time.

[*](#ftnt8_ref) Or they’ll have built tools or successors to do the exploring, in whatever way they find convenient with the benefits of more advanced science and technology.
#### Notes

[1] *nothing ever happens: *The phrase “nothing ever happens” appears to be common among people who participate in prediction markets. The heuristic itself is discussed by, e.g., the blogger Scott Alexander in his essay [Heuristics That Almost Always Work](https://www.astralcodexten.com/p/heuristics-that-almost-always-work).

[2] *no limitations: *See, for example, the paper [Eternity in six hours](https://www.sciencedirect.com/science/article/abs/pii/S0094576513001148?via%3Dihub), which discusses the limits on intergalactic colonization given the constraints of known physical law.[AI Experts on Catastrophe Scenarios→](/intro/ai-experts-on-catastrophe-scenarios)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/what-are-your-incentives-and-conflicts-of-interest-as-authors

What are your incentives and conflicts of interest, as authors? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What are your incentives and conflicts of interest, as authors?
#### We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.

We (Soares and Yudkowsky) take our salary from the Machine Intelligence Research Institute (MIRI), which is funded by donations from people who think these issues are important. Perhaps the book will drive donations.

That said, we have other opportunities to make money, and we are not in the book-writing business for the cash. The advance we got from this book was paid entirely toward publicity for this book, and royalties will go entirely to MIRI to pay it back for the staff time and effort invested.[*](#ftnt5)

And of course, both authors would be ecstatic to conclude that our civilization is not in danger. We’d love to simply retire, or make more money elsewhere.

We don’t think we’d have difficulty changing our minds, if in fact the evidence merited a change. It’s happened before. MIRI was founded (under the name “Singularity Institute”) as a project to *build* superintelligence. It took a year for Yudkowsky to realize that this wouldn’t *automatically* go well, and a couple more years for him to realize that making it go well would be rather tricky.

We’ve pivoted once, and we’d love to pivot again. We just don’t think that’s what the evidence merits.

We don’t think the situation is hopeless, but it does seem to us that there is a genuine problem here, and that the threat is extreme if the world *doesn’t* rise to the occasion.

It’s also worth emphasizing that to figure out whether AI is on track to kill us all, you have to think about *AI*. If you only think about people, you can come up with reasons to dismiss any source: Academics are out of touch; corporations are trying to drum up hype; the non-profits want to raise money; the hobbyists don’t know what they’re talking about.

But if you take that route, then your final beliefs will be determined by who you choose to dismiss, giving arguments and evidence no space to change your mind if you’re wrong. To figure out what’s true, there’s no substitute for evaluating the arguments and seeing if they stand on their own legs, separate from the question of who raised them.

Our book doesn’t open with the easy argument that the corporate executives running AI labs have an incentive to convince the populace that AIs are safe. It begins by discussing *AI*. And later in the book, we spend a little time *reviewing the history of human scientists being over-optimistic*, but we never say you should ignore someone’s argument because they work at an AI lab. We discuss some of the developers’ *actual plans, *and why those plans wouldn’t work on their own merits. We are doing our best to sit down and have a conversation about the actual arguments, because it’s the actual arguments that matter.

If you think we’re wrong, we invite you to engage with our arguments and point out the specific places where you think we’ve gotten things wrong. We think that’s a more reliable way to figure out what’s true than looking mainly at people’s character and incentives. The most biased person in the world may say that it’s raining, but that doesn’t mean it’s sunny.

[*](#ftnt5_ref) If the book performs so well as to pay off all those investments, there is a clause in our contract saying that the authors eventually get to share in the profits with MIRI, after MIRI is substantially paid back for its effort. However, MIRI has been putting so much effort into helping out with the book that, unless the book dramatically exceeds our expectations, we won’t ever see a dime.[Isn’t this AI stuff just science fiction?→](/intro/isnt-this-ai-stuff-just-science-fiction)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed

When is this worrisome sort of AI going to be developed? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## When is this worrisome sort of AI going to be developed?
#### Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.

Many of the things people ask us to try to predict for them, we in fact have no way of knowing. When Leo Szilard wrote a letter warning the USA about nuclear weaponry in 1939, he did not and could not include any note along the lines of, “The first atomic weapon will be ready to detonate for testing in six years.”

This would have been very valuable information! But even when you are the first person to correctly predict nuclear chain reactions, as Szilard was — even when you’re the very first one to see that a technology is possible**and will be consequential —**you cannot predict exactly when that technology will arrive.

There are easy calls and hard calls. We do not pretend to be able to make hard calls, such as exactly when the dangerous sort of AI will be produced.
#### Experts keep being surprised by how fast AI progress happens.

Not knowing when AI is coming is not the same as knowing that it’s a long way off.

In 2021, the forecasting community on the prediction website Metaculus [estimated](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) that the first “truly general AI” would arrive in 2049. One year later, in 2022, that aggregate community prediction had fallen by twelve years, to 2037. Another year later, in 2023, it had fallen by a further four years, to 2033. [Again](https://perma.cc/742G-7V2F) and [again](https://forecastingresearch.org/near-term-xpt-accuracy), forecasters have been surprised by the fast pace of AI progress, with their time estimates varying wildly year over year.

This phenomenon is not isolated to Metaculus. An organization called 80,000 Hours [documents](https://80000hours.org/2025/03/when-do-experts-expect-agi-to-arrive/) various other cases of rapidly shortening timelines from many groups of expert forecasters. And even superforecasters — who consistently win forecasting tournaments and often exceed domain experts in their ability to forecast the future — assigned only [2.3% probability](https://forecastingresearch.org/near-term-xpt-accuracy) to AIs achieving the International Math Olympiad gold medal by the year 2025. AIs [achieved](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) the International Math Olympiad gold medal in July of 2025.

Smarter-than-human AI might intuitively look like it’s decades off, but ChatGPT-level AI looked like it was decades off in 2021, and then suddenly it arrived. Who knows when new qualitative AI improvements will suddenly arrive? Maybe it’ll take another ten years. Or maybe a breakthrough will come tomorrow. We don’t know how long it will take, but a number of researchers have become increasingly worried that time might be running short. Without claiming special knowledge on this front, we think humanity should react soon. It’s not clear how much more warning we’re ever going to get.

See Chapter 1 for more discussion of ways that AI capabilities could cascade with very little warning. And see Chapter 2 for more discussion of modern AI paradigms, and whether they will or won’t be able to go “all the way.”
#### Be suspicious of media claims about what can and can’t happen soon. (It may have already happened!)

Two years after Wilbur Wright’s [dejected prediction](https://www.wright-brothers.org/History_Wing/Wright_Story/Inventing_the_Airplane/Not_Within_A_Thousand_Years/Not_Within_A_Thousand_Years.htm)**that powered flight would take a thousand years, the *New York Times* confidently asserted it would take a million. Two months and eight days later, the Wright brothers flew.

Today, skeptics continue to make over-the-top claims that AI could never possibly rival humans in some specific capability, even as recent progress with machine learning shows AIs matching (or exceeding) human performance on a growing list of benchmarks. It has been known since at least late 2024, for example, that modern AIs can often identify sarcasm and irony from [text](https://www.yomu.ai/resources/can-ai-essay-writers-understand-satire-irony-or-sarcasm-in-essays#) and even [nonverbal cues](https://dl.acm.org/doi/10.1145/3678957.3685723). But this didn’t stop the *New York Times* from [repeating](https://www.nytimes.com/2025/05/16/technology/what-is-agi.html) the claim in May 2025 that “scientists have no hard evidence that today’s technologies are capable of performing even some of the simpler things the brain can do, like recognizing irony.”[*](#ftnt4)

All of which is to say: Many will claim to have knowledge that smarter-than-human AI is imminent, or that it’s incalculably far off in the future. But the uncomfortable reality is that nobody knows right now.

Worse, there’s a strong chance that nobody *will ever* know until after it’s too late for the international community to do anything about the matter.

Timing the next technological breakthrough is incredibly difficult. We know that smarter-than-human AI is lethally dangerous, but if we also need to know what day of the week it’s coming on, then we’re out of luck. We need to be able to act from a position of uncertainty, or we won’t act at all.

[*](#ftnt4_ref) Yes, AIs can even [recognize the irony](https://perma.cc/EP4R-K85G) of the *New York Times *reporting that they can’t recognize irony. (To be fair to the *New York Times*, some of their reporters cover AI with somewhat [greater clarity](https://www.nytimes.com/2025/03/14/technology/why-im-feeling-the-agi.html).)
#### Notes

[1] *confidently asserted: *Quoting the 1903 article “[Flying Machines Which Do Not Fly](https://www.nytimes.com/1903/10/09/archives/flying-machines-which-do-not-fly.html)”:

The machine does only what it must do in obedience to natural laws acting on passive matter. Hence, if it requires, say, a thousand years to fit for easy flight a bird which started with rudimentary wings, or ten thousand for one which started with no wings at all and had to sprout them ab initio, it might be assumed that the flying machine which will really fly might be evolved by the combined and continuous efforts of mathematicians and mechanicians in from one million to ten million years — provided, of course, we can meanwhile eliminate such little drawbacks and embarrassments as the existing relation between weight and strength in inorganic materials. No doubt the problem has attractions for those it interests, but to the ordinary man it would seem as if effort might be employed more profitably.

[Can we use past progress to extrapolate when we’ll build smarter-than-human AI?→](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/when-leo-szilard-saw-the-future

When Leo Szilard Saw the Future | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/intro#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## When Leo Szilard Saw the Future

In September of 1933, a physicist named Leo Szilard was crossing the intersection where Southampton Row passes Russell Square when he had the idea of a nuclear chain reaction — the key idea behind atomic bombs.

There was, from there, a whole adventure as Szilard tried to figure out what to do with this momentous idea. He went to the more prestigious physicist Isidor Rabi, and Rabi went to the even more prestigious Enrico Fermi. Rabi asked Fermi whether he thought nuclear chain reactions were the real deal, and Fermi sent back a reply:

Nuts!

Rabi asked Fermi what “Nuts!” meant, and Fermi said that it was a remote possibility.

Rabi asked what Fermi meant by “remote possibility,” and Fermi said, “10 percent.”

To which Rabi replied, “10 percent is not a remote possibility if it means that we may die of it.”

Fermi reconsidered.

There are a few different morals one could take away from this story. A moral we *don’t *take away is, “Every remote possibility is worth worrying about if we may die of it.” There’s nothing “remote” about 10 percent, but if the possibility *were* sufficiently remote, then it would simply not be worth thinking about.

One moral that we* do *take from this story: It is sometimes possible to realize that a technology such as a radioactivity cascade is *possible, *and thus know (before everyone else) that the world is set for some sort of drastic change.

Another moral that we take from this story is that one’s initial intuitions are often not a good guide to anticipating and thinking about drastic changes. Not even if one is a renowned expert in the relevant field, like Enrico Fermi was.

Consider: Where did Fermi even get that “remote possibility” and “10 percent” stuff from in the first place?

Why did Fermi think that you *couldn’t* get radioactivity to induce more radioactivity in a chain reaction? Was it just that most big ideas don’t pan out?

Replying “Nuts!” seems to be saying something stronger than just that. It seems to reflect a sense that this particular big idea was *excessively* unlikely to pan out. But why? On what physical argument?

Did it just *feel *crazy? Yes, the possibility of nuclear weapons would have radical consequences for the world. But reality is not arranged so as to prevent events with large consequences from ever happening.[*](#ftnt12)

When Fermi first heard Szilard’s idea, he suggested that Szilard publish it and let the whole world know about it — including Germany and its new chancellor, Adolf Hitler.

Fermi lost that argument, and well that it was so, for nuclear weapons turned out to be possible after all. Fermi did ultimately join Szilard’s tiny conspiracy, though he remained a skeptic almost until the moment when he himself oversaw the creation of the first nuclear pile, Chicago Pile-1.

Sometimes, technologies upend the world. If you take for granted that radical new technologies are “nuts,” you can get blindsided by progress, even if you’re one of the smartest scientists in the world. It is a great credit to Fermi, then, that he sat down and had the argument with Szilard. And an even greater credit, that he was persuaded to change his behavior *before* the technology existed, before he could see it with his own eyes — when there was still time to do something about it.

A very large number of awful things have happened over the course of human history — but some of the awful things that haven’t happened were avoided because somebody sat down and had the conversation. Forced the conversation, in some cases, as Szilard did with Fermi.

[*](#ftnt12_ref) Faced with this criticism of Fermi, we’ve seen people defend him by inventing reasons why it’s totally plausible that Fermi did a lot of thinking before saying, “Nuts!” For example, they argue, Fermi knew that the Earth hadn’t previously exploded in a cascade of induced radioactivity — which someone might think the Earth ought to have already done if those sorts of cascades were physically possible.

  
These sorts of arguments, of course, are pointing toward a false conclusion. Fermi was *wrong* about nuclear chain reactions. Given that, we’d say the lesson to learn from the existence of arguments like that is: “You can always come up with arguments that sound at least that plausible against the truth of things that are in fact true.” That the Earth hasn’t exploded yet is not strong evidence that nuclear reactors are impossible; human engineers can carefully arrange atoms to split on purpose. So such arguments do not support a conclusion as wrong as saying, “Nuts!”
#### Notes

[1] *crossing the intersection: *A fuller profile and timeline is maintained by the [Atomic Heritage Foundation](https://ahf.nuclearmuseum.org/ahf/profile/leo-szilard/).[Humanity’s Special Power→](/1)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat

Why write a book about superhuman AI as an extinction threat? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/intro)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why write a book about superhuman AI as an extinction threat?
#### Because the situation seems genuinely serious and urgent.

If you carefully consider a topic, you can sometimes see one of history’s zigs or zags coming.

In 1933, a physicist named Leo Szilard was the first person to realize that nuclear chain reactions are possible.[*](#ftnt1) He thereby gained the ability to predict one of history’s zigs earlier than anyone else.

We think that if you look at AI from the right vantage point today, you can see one of history’s zags coming. And we think that events are set to go poorly if humanity does not change course.

AI labs are racing to build machines that are smarter than any human, and they’re making what appears to be significant progress in advancing the frontier. As we’ll discuss in the coming chapters, modern AIs are more *grown* than crafted. They exhibit behavior that nobody asked for and nobody wanted, and they’re on track to become more capable than any human. This looks like an extremely dangerous situation to us.

Top scientists in the field came together to sign an [open letter](https://aistatement.com/) warning the public that the threat of AI should be treated as a “global priority alongside other societal-scale risks such as pandemics and nuclear war.” This is not an isolated concern; it’s shared by [nearly half the field](/intro/ai-experts-on-catastrophe-scenarios). Even if you are initially skeptical of the dangers, we hope that the level of stated concern by AI experts, and the high stakes if these concerns turn out to be correct, make it clear why this is a topic that deserves to be seriously talked out.

This is a topic where we should weigh the arguments rather than just following our first intuition. If the letters and the warnings are correct, then the world has gotten itself into an incredibly dangerous position. And we’ll spend the rest of the book laying out the arguments and evidence behind those warnings.

We do not think the situation is hopeless. We wrote this book with the hope of changing the trajectory humanity seems to be on, because we think there’s hope that we can solve this.

The first step toward solving a problem is to understand it.

[*](#ftnt1_ref) We tell part of Leo Szilard’s tale in an [extended discussion](/intro/when-leo-szilard-saw-the-future).[Are you suggesting that ChatGPT could kill us all?→](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[Resources](/resources) › [Introduction](/intro)[
### Why write a book about superhuman AI as an extinction threat?Because the situation seems genuinely serious and urgent.2 min read](/intro/why-write-a-book-about-superhuman-ai-as-an-extinction-threat)[
### Are you suggesting that ChatGPT could kill us all?No. The worry is about forthcoming advances in AI.4 min read](/intro/are-you-suggesting-that-chatgpt-could-kill-us-all)[
### Aren’t people always panicking and overreacting to things?Yes. But this doesn’t mean that nothing is ever actually dangerous.1 min read](/intro/arent-people-always-panicking-and-overreacting-to-things)[
### When is this worrisome sort of AI going to be developed?Knowing that a technology is coming doesn’t grant knowledge of exactly when it’s coming.5 min read](/intro/when-is-this-worrisome-sort-of-ai-going-to-be-developed)[
### Can we use past progress to extrapolate when we’ll build smarter-than-human AI?We don’t have a good enough understanding of intelligence for that.2 min read](/intro/can-we-use-past-progress-to-extrapolate-when-well-build-smarter-than-human-ai)[
### What are your incentives and conflicts of interest, as authors?We don’t expect to make any money from the book in the average case. Separately, we would love to be wrong about the book’s thesis.3 min read](/intro/what-are-your-incentives-and-conflicts-of-interest-as-authors)[
### Isn’t this AI stuff just science fiction?We can’t learn much from a topic’s prevalence in fiction.9 min read](/intro/isnt-this-ai-stuff-just-science-fiction)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### AI Experts on Catastrophe Scenarios](/intro/ai-experts-on-catastrophe-scenarios)[
### When Leo Szilard Saw the Future](/intro/when-leo-szilard-saw-the-future)[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
