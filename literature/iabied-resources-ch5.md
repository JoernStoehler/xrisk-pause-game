---
title: "If Anyone Builds It, Everyone Dies - Online Resources: Chapter 5 - Its Favorite Things"
author: "Eliezer Yudkowsky, Nate Soares"
year: 2025
source_url: "https://ifanyonebuildsit.com/resources"
source_format: html
downloaded: 2026-02-11
encrypted: false
notes: "Supplementary Q&A and extended discussions for Chapter 5 - Its Favorite Things from the companion website"
---

# Online Resources: Chapter 5 - Its Favorite Things

## /5/are-you-just-pessimistic

Are you just pessimistic? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Are you just pessimistic?
#### We’re optimistic about many things, but superintelligence isn’t like most things.

We would consider ourselves much more [optimistic](/12/are-you-anti-technology) and gung-ho than the average person about nuclear power, geothermal power, genetic engineering, neuroengineering, biotech, nanotech, pharmaceutical development, and many other technologies.[*](#ftnt191)

We expect that we’re at least somewhat less worried than most people about the risk of nuclear war, worst-case climate change scenarios, and many other potential risks and disasters. We think humanity is broadly on a good trajectory, and that if we avoid wiping ourselves out, the future is likely (though not certain) to be wonderful for everyone, with social and technological progress making things better and better over time.

We are also more optimistic than many about human nature. We believe in the goodness of humanity and in the potential for that goodness to deepen and grow if we survive to become more of who we wish to be. We mostly *aren’t* afraid of humanity ending up in a bleak or dystopian future, if we don’t make AI that keeps us from having a future at all.

Our concern about smarter-than-human AI is not driven by generic cynicism or pessimism. Smarter-than-human AI is different from other technologies that came before it.

Other technologies don’t think for themselves, or plot ways to escape, or build even more powerful technology. Smarter-than-human AI is a special case.

We view our worries about AI as generalizing to very few other things, because very few things are remotely this dangerous.

And even in the case of superintelligence, which poses a uniquely large threat and a huge challenge for the international community, we think there’s hope for the future to go well. We think humanity has the ability to hit the brakes on AI development, and that this could be enough to set us on a positive trajectory. We even think that (with a lot more time) humanity could put itself in a good position to build superintelligence safely.

But in order to get there, we first need to face up to the reality of the situation.
#### The point is the arguments, not the dire-sounding stories.

We provided a long list of ways that, e.g., “[superintelligence is fascinated with humans](/5/wont-ai-find-us-fascinating-or-historically-important)” would probably go wrong in real life. Reading a list like that, we imagine that some readers might have a response like:

The AI optimists have all these hopeful-sounding stories. You have all these scary-sounding stories. Everyone acknowledges, though, that the future is hard to predict. So, hearing all these stories, I feel like I should have a medium-sized probability of AI catastrophe, not an extreme probability in either direction.

But you don’t say, “There are scary stories, and there are also hopeful stories, so we can’t be sure what’s going to happen, and we should ban superintelligence just to be on the safe side.” You say that the hopeful stories are cherry-picked and unlikely, and that your own stories should get more weight. Why?

The short answer is: You can’t make good predictions about the future by just counting up all of the gloomy tales and all of the happy tales and weighing them like marbles on a scale. Thinking through different scenarios can sometimes be helpful, but not in quite that fashion.[†](#ftnt192)

To illustrate the general point: Imagine that someone says, “Two hundred years from now, there will be exactly eight whales in existence, and they will all be purple.”

Humans have wild imaginations. Someone could fill a book with hundreds of stories of how it came to pass that the whale population shrank to exactly eight members, all of them purple. Someone else could fill a book with hundreds of stories in which there *aren’t *exactly eight whales. You can’t make accurate predictions by saying, “Well, both sides have plausible-sounding stories, so surely the truth is somewhere in the middle.”

To figure out which is true, you’ve got to look at the actual arguments. In the case of the purple whales, the argument is essentially that the outcome is too narrow and specific, and won’t be achieved unless the dominant forces steering the world are trying to achieve it. We can say much the same about superintelligent AI producing good, human-compatible outcomes.

Someone who was tasked with dispelling the “eight purple whale” stories one by one would wind up caught in a fairly repetitive loop of saying: “No, that’s overly specific; there are a bunch of other ways the future could go that would not lead exactly there; to imagine that it goes exactly that way is wishful thinking.”

This is more or less the role we authors find ourselves in with regard to the AI situation: Humans can tell all sorts of stories where everything goes fine, but those all ultimately involve imagining that the future follows a single narrow pathway when in fact there are a bunch of other ways for the future to go. This is why we keep repeating that [humans aren’t the most efficient solution to almost any problem](/5/humans-are-almost-never-the-most-efficient-solution) and that [AIs won’t care about us even a little](/5/wont-ais-care-at-least-a-little-about-humans).

*If Anyone Builds It, Everyone Dies* does not just rattle off a bunch of gloomy stories and thereby conclude that AI is dangerous. In the book, we lay out an argument — an argument that is, in some ways, fairly simple: Researchers are trying to build AIs that are far smarter than any human. At some point, they’re likely to succeed. Current methods give humans very little ability to pick what sort of future the AIs steer toward. There are many different directions they could go, and most directions aren’t good.

The reason we’re rattling off all the counterarguments isn’t to overwhelm you with pessimism (if you’re the sort of person to read the online resources end-to-end). It’s that we actually get asked all these different questions over and over, and it’s nice to have a repository of responses somewhere. You don’t need to read all of them through. The answers all echo each other anyway.

What matters is the arguments themselves, not someone’s bias toward optimism or pessimism, and not the number of stories someone can trot out.

[*](#ftnt191_ref) When we say we are more optimistic than average (about one technology or another), we mean that we *actually believe *the technology is more promising than the average person believes. *Dispositionally, *we see ourselves neither as optimists nor as pessimists, but as realists trying to navigate a complicated world. We are not trying to find a rosy picture to put our faith in, and we are not trying to find a dour picture to fuel our cynicism; we are simply trying to believe the truth. We believe this is the correct disposition when faced with high-stakes decisions.

[†](#ftnt192_ref) To be clear: If the best you can do is say “I don’t know, there are some happy tales and some gloomy tales, maybe it’s fifty-fifty as to whether superintelligence would kill us or not,” that’s *way *more than sufficient to justify an aggressive international response, even if you weren’t quite as worried as we personally are. But it also matters that people *understand the problem, *because otherwise the policy response is unlikely to be well-targeted and effective. And if you’re just roughly comparing the number of good-sounding stories to the number of bad-sounding stories, then you aren’t engaging with the arguments on either side, which is what would help build understanding.[Would smarter-than-human AI be conscious?→](/5/would-smarter-than-human-ai-be-conscious)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around

But we still have horses. Why wouldn’t AI keep us around? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## But we still have horses. Why wouldn’t AI keep us around?
#### What horses remain, remain because we like them.

Having the same fate that horses had at the start of the 20th century — the same catastrophic collapse in population and massive upsurge in death, destroying [upwards of 80 percent of the horse population](https://datapaddock.com/usda-horse-total-1850-2012) from its peak around 1910 — would be the worst thing that has happened in human history. And that was in a world where horses continued to be economically useful for some farm work, as well as for sports and novelty experiences to sell to rich people.

If people had access to artificial horses that were shaped roughly the same way but were easier and more fun to ride, cheaper to own, and more personable and loving and convenient, the decline of horses would have been even more pronounced.

In other words: Technological progress (the invention of cars) caused humans to do away with most horses. And if there had been even more progress, the effect could have easily been even more drastic. The same is likely to hold for AIs as their options expand and they find ways to achieve their goals [without humans](/5/humans-are-almost-never-the-most-efficient-solution).

But yes, some horses survived. A small number continued to be useful. Others were kept by people who happened to love horses and cared about their horses in particular.

For humans to stick around in a world where we rushed into unleashing superintelligent AI, we would either need to stay useful to the AI or have the AI care about us in particular.

But we can’t stay useful, because AIs can (from their perspective) get more use out of our matter and energy by rearranging us into any number of more efficient configurations. Technological progress unlocks many new**options for a superintelligence; it won’t just be stuck relying on humans.

So it all comes down to whether the AIs care about us — and they’re unlikely to care about us [even a small amount](/5/wont-ais-care-at-least-a-little-about-humans), if we race to superintelligence as fast as we can.[Won’t AIs care at least a little about humans?→](/5/wont-ais-care-at-least-a-little-about-humans)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone

Can the AI be satisfied to the point where it just leaves us alone? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can the AI be satisfied to the point where it just leaves us alone?
#### Probably not.

Your preference for oxygen is satiable — you’ll fight to reach the surface if your scuba gear malfunctions during a dive, but when there is enough, you stop worrying about it, and you probably aren’t maintaining an ever-growing stockpile of oxygen canisters.

Your preference for wealth, for nice experiences, for the acclaim of your peers — these are probably somewhat less satiable. If you saw an easy opportunity to get vastly more wealth, you’d probably take it. If you saw an easy opportunity to massively improve the world, we hope you’d take it, rather than just being satisfied with how much joy and comfort already exist. We hope that you’d keep making the world a better place for quite a long time, if you kept seeing ways to do so that looked easy and cheap and fun from your perspective.

And on the whole, the sum of a satiable preference for oxygen and an insatiable preference for making the world better…is an insatiable set of preferences.

So too with AIs. If they have myriad complex preferences, and *most *of them are satisfiable — then, well, their preferences *as a whole* are still *not* satisfiable.

Even if the AI’s goals *look *like they satiate early — like the AI can *mostly *satisfy its weird and alien goals using only the energy coming out of a single nuclear power plant — all it takes is one aspect of its myriad goals that *doesn’t* satiate. All it takes is *one* not-perfectly-satisfied preference, and it will prefer to use all of the universe’s remaining resources to pursue that objective.

Or, alternatively: All it takes is one goal that the AI is never *certain *it has accomplished. If the AI is uncertain, then it will prefer that the universe’s resources go to driving its probability ever closer to certainty, in tiny increments of confidence.

Or, alternatively: All it takes is one thing the AI wishes to defend until the end of time for the AI to prefer that the universe’s resources be spent aggregating matter and building defenses to ward off the possibility of distant aliens showing up millions of years from now and encroaching on the AI’s space.

There are *many* different ways for an AI to be unsatisfied. And the more messy and complicated the AI’s goals are, the more likely it is that at least one of those goals will be difficult or impossible to fully satisfy.

Even if you could create a superintelligence that was monomaniacally focused on just one simple thing — such as painting a *particular car* red — that AI could still probably find some way to spend extra energy making *extra sure *that the car was red, and building defenses around the car so that nobody could ever paint it blue, and so on.

Leaving us alone is a fragile state of affairs. We can think of this in similar terms to why it’s hard to get humans to leave the chimpanzees alone.

Why are both species of chimpanzee endangered — even though many humans *do* care about chimpanzees and actively try to protect them?

The problem isn’t that the chimpanzee-loving humans are struggling with chimpanzee-*hating* humans who are trying to exterminate the chimps out of malice.

The problem is that there are *other things humans want*.

Humans want all sorts of things, including land and wood, and the chimps are caught in the crossfire. Enough humans are indifferent to chimpanzees, or indifferent *enough* relative to their other priorities, that we wind up destroying their habitat incidentally.

Why would we go off and destroy chimp habitat when we have plenty of space for ourselves?

Well, because we don’t need to choose between keeping the territory we already have and encroaching on the chimps’ territory. Humanity can do *both at once.*

So too with AIs. An AI doesn’t need to pick between the resources of Earth and the resources of elsewhere; it can have both, as we discuss in the book. It wouldn’t be that**expensive, from the AI’s perspective, to leave us alone; but it [wouldn’t be free](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense) either, and the AI would need to have a reason to let us use resources it could instead use for its own goals.

Moreover, even if the AI *can* be fully satisfied, the outcome for humans is still likely to be pretty grim. There are multiple reasons for this:
- Just because the AI can be fully satisfied doesn’t mean it can be *easily *satisfied. If the AI is satisfied with a single solar system or a single galaxy, that doesn’t mean that humans get everything else.
  - The AI may view us as a competitor for that solar system or that galaxy.
  - Even if we’re clearly not interested in competing with the AI, the AI may still view us as a source of threats. This is especially true insofar as humans could build a rival superintelligence that *does *contest the first AI for those resources.
  - Even if the AI views humans as no competition and no threat, humanity is likely to die incidentally, just by being at ground zero. The AI in this scenario may only want a few solar systems’ worth of resources, but the AI’s efforts still all begin *on Earth*. The most straightforward way to acquire those solar systems will be to extract the Earth’s resources, rendering it uninhabitable. The AI in this scenario *could* fully achieve its goals without killing off humanity, but if the AI doesn’t care at all about humanity, then it won’t necessarily bother.
- If a satisfiable AI *does *want to keep humanity around, this is still unlikely to be good news for humanity, for the reasons discussed in “[Won’t AI find us fascinating or historically important?](/5/wont-ai-find-us-fascinating-or-historically-important)” and “[Won’t AIs care at least a little about humans?](/5/wont-ais-care-at-least-a-little-about-humans)” (The outlook looks similarly grim if a *non*-satisfiable AI wants to keep humanity around.)

For more on this topic, see the extended discussions on [satisfiability](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone) (in the online resources for this chapter, Chapter 5) and [making AIs robustly lazy](/5/its-hard-to-get-robust-laziness) (in the Chapter 3 online resource).[Can we just make it lazy?→](/5/can-we-just-make-it-lazy)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/can-we-just-make-it-lazy

Can we just make it lazy? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can we just make it lazy?
#### Even laziness isn’t safe.

Companies aren’t likely to make “lazy” AIs, because AI is a competitive industry, and that’s not the best way to make a profit. Users won’t want the AI to be lazy about meeting their requests, and the company won’t want the AI to be lazy about maximizing user engagement and attachment, or about thinking better and more clearly.

But even if companies tried to make AI robustly “lazy,” we can expect that they would fail, because nobody knows how to robustly point an AI at *anything* in a way that’s likely to carry over to superintelligence, as we talked about in Chapter 4.

Moreover, robust laziness [seems like an especially difficult target to hit](/5/its-hard-to-get-robust-laziness).

*Even if all of those obstacles were surmounted*, however, “lazy AI” isn’t enough on its own to prevent disaster once AIs achieve smarter-than-human capabilities.

Imagine a very lazy person, somebody who just *hates* to do the slightest bit more work than necessary. Sounds like a safe sort of person to be around, right?

Now imagine what would happen if this lazy person saw an easy way to create a much harder-working mind to outsource all their work to.

Even if a lazy superintelligence didn’t *hate* work all that much — even if it just did whatever got the job done, then stopped, without *going hard* on minimizing work — it would still likely find it just as easy to get the job done by building a harder-working mind to do the task, once it was smart enough.

In a technical context, we might phrase the point as: “Satisficing AIs aren’t a stable equilibrium.” Even if the AI doesn’t want to exert much effort, it would have no compunctions about building a new AI that does exert effort. It wouldn’t even mind modifying itself to “cure” itself of its laziness — as long as there’s a sufficiently lazy way to do so.[Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?→](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/cant-we-make-the-ai-promise-to-be-friendly

Can’t we make the AI promise to be friendly? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Can’t we make the AI promise to be friendly?
#### You can make it promise whatever you’d like. You can’t make it keep its promises.

It’s true that, when an AI is still small and powerless, we have the ability to turn it off. And so you might think that there is a trade opportunity available, where we offer to make the AI smarter if and only if it would give humanity a bunch of nice things after it matures into a superintelligence.

The difficulty with this plan is that we can’t tell the difference between an AI that agrees to the deal but won’t follow through and an AI that agrees to the deal and will follow through.

Which in turn means that an AI pursuing inhumane wants has no incentive to actually follow through, because humanity treats betrayers and dealkeepers alike. So there’s no point in being a dealkeeper.

There are a lot of interesting nuances to the issue of promise-keeping and deal-making in AI, which we go into [in the extended discussion below](/5/ais-wont-keep-their-promises). But none of these nuances changes the very simple headline result, which is that you can’t use your leverage over a weak AI to constrain the options that AI will have when it’s a superintelligence. The obvious answer — that once the AI matures into a superintelligence, it will have no reason to keep its word at great expense to its own designs — turns out to be the correct one here.[What if we make it think it’s in a simulation?→](/5/what-if-we-make-it-think-its-in-a-simulation)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/effectiveness-consciousness-and-ai-welfare

Effectiveness, Consciousness, and AI Welfare | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/5#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Effectiveness, Consciousness, and AI Welfare

In the [Chapter 1 resources](/1/are-you-saying-machines-will-become-conscious), we distinguished a few different concepts of “consciousness.” The version of consciousness we’ll be talking about here is sometimes called things like “subjective experience,” “sentience,” or “phenomenal consciousness.” It’s the idea that there’s *something it’s like* to be that entity; the lights are on, metaphorically speaking.

We also said that we think artificial *intelligence* probably doesn’t require artificial *consciousness*. We’ll speak to that topic here, and then turn to the question of AI ethics and AI rights.
#### Conscious Experience is Separate from the Referents of Those Experiences

Some people are skeptical that an AI could be effectively intelligent without being conscious in the way that humans are. We suspect this is an error, like imagining that robotic arms must be soft and full of blood just because human arms are soft and full of blood.

How could an AI be *effective* without being conscious in the way that humans are? Isn’t the subjective experience of self-awareness a crucial component of our intelligence?

It’s a crucial component of *human* intelligence, yes. But we doubt it’s the only way to be intelligent.

Recall that Deep Blue didn’t need to be conscious in order to surpass the best human grandmasters at chess. The distributed intelligence of the [stock market](/1/more-on-intelligence-as-prediction-and-steering#intelligences-many-shapes) results in superhumanly good predictions about short-term corporate price movements, without the market itself having subjective awareness. It’s intuitive that at least in these domains, you can have competent world-modeling, planning, and decision-making without having consciousness.

This point can be strengthened by looking at formal models of reasoning. AIXI, for example, is an equation that defines a vastly superhuman reasoner.[*](#ftnt212) AIXI’s entire algorithm can be stated in a single line, with no steps in the algorithm where AIXI does anything conscious or self-aware or at all mysterious. Yet in spite of this, AIXI is theoretically able to solve an incredible variety of complicated steering and prediction problems. Or at least, it *would* be able to, if it were possible to create.[†](#ftnt213) Here’s the AIXI equation:[‡](#ftnt214)

AIXI is a theoretical construct, not a practical algorithm that we can run to efficiently solve problems in the real world. But because AIXI is *simple* and easy to analyze, it can help us think about the very concept of steering and planning and see that there at least isn’t any *obvious *way that these activities require consciousness. If consciousness *is *required for superhuman steering and planning in the real world, then it must be due to some subtler aspect of cognition that isn’t captured in the AIXI formalism.

Or, to come at the point from another angle: Consider sneezing.

There’s a particular *way* that sneezing feels, separate from the physical act of convulsing the muscles and explosively forcing air out of the lungs and through the mouth and nose. The actions and the sensations are separate physical events. It’s biologically possible to build an apparatus that is like a body except without the brain, and then wire it up to nerve signals that cause the muscle contractions of a sneeze. That brainless body would go through all of the motions, but would not have any of the associated feelings — the mechanisms that perform the sneeze are *distinct* from the ones that create and experience the sensations.

That’s not to say that the feelings of a sneeze don’t do *anything. *Subjective experience is real, and the subjective experience of a sneeze might lead a person to emit a sentence like “Golly, sneezes feel kinda weird,” and that *wouldn’t happen* in the case of the brainless body.

The point is that the feelings humans have when they sneeze are built out of additional parts, over and above the parts that contract the muscles and push out the air.

As with sneezes, so too with thoughts. The mental machinery that implements a thought is different from the mental machinery that implements the *feeling *of that thought. We can say with great confidence that this is true for an enormous variety of thoughts, since pocket calculators and chess AIs succeed at the tasks of arithmetic and chess without having the conscious experience of a human mathematician or a chess grandmaster.

*Thoughts* and *feelings-of-thoughts* are both implemented in the brain, which makes it easier to get the two mixed up — the distinction is *more obvious* in the case of sneezes. But we expect it’s equally possible in principle to assemble a variant of a brain that does the same practical problem-solving work a human brain does, but doesn’t *feel* any of that thinking.

A brain like that might need extra parts that do the *work *that feeling thoughts does in us. Maybe the subjective experience of thoughts is part of how humans do reflective reasoning, and perhaps reflective reasoning is an important part of human intelligence.

But we doubt that subjective experience is the *only *way to do reflection (or whatever else), any more than a human-style feeling of curiosity is the only way to investigate surprising phenomena. (See also the [discussion of curiosity](/4/curiosity-isnt-convergent) in the Chapter 4 online resource.)
#### Analogous Structures Allow for Multiple Solutions to the Same Problem

Our best guess is that most smarter-than-human AIs would not be conscious, by default. This is because our best guess is that not every possible thinking-engine must use conscious feelings to guide their thoughts. Consciousness can serve an important function in humans, without it being the only way any possible mind could ever do analogous cognitive work.

In evolutionary biology, scientists use the term “analogous structures” to refer to traits that serve the same function in different animals, but arise from different anatomical origins.

(This is distinct from *convergent evolution,* in which multiple species evolve the *same* adaptation, such as urushiol and caffeine being “discovered” multiple times by evolution.)

Fireflies produce light by using enzymes to oxidize the chemical luciferin in special “lantern cells.” Deep-sea anglerfish, in contrast, have a symbiotic relationship with photobacteria that they house in a small organ — bacteria whose light production uses a different chemical pathway than fireflies.

Mammals evolved teeth; birds solved the same problem with a gizzard and swallowed stones. Bats produce echolocation calls with their larynx and receive the echoes with their ears; whales and dolphins use a nasal organ to generate sounds and receive the echoes with sensitive systems in their jawbones. Some aquatic species swim by pushing their limbs against the water, and others by expelling water from a bladder. Bat wings evolved from the webbing of hands, bird wings from arms.

There are, in other words, *many ways* to design structures which solve the same problems. Human engineers, not limited by the constraints of evolution, have solved each of these problems in yet stranger ways — with burning candles, incandescent bulbs, and LEDs; with knives and blenders and food processors; with sails and propellers and SCUBA gear; with sonar and radar.

A human arm with blood removed would stop working, but that doesn’t mean that robot arms must use blood; they’re allowed to work in a different, bloodless way.

Similarly, the pieces of cognitive machinery that implement the *behavior *of curiosity in humans are different from the pieces of cognitive machinery that implement our *feeling *of curiosity. A human’s felt satisfaction when they unravel the mystery of the possum in the attic is distinct from their outer behavior of choosing to investigate the drawer that kept being left open. Those two things may come bundled in humans, but that doesn’t mean that they must come bundled in all minds.

And since we don’t understand precisely what led to the evolution of subjective experience in humans, and we can see all sorts of agentic, problem-solving behavior out in the world in processes that seem to us to lack it —

(Slime molds solving a maze; Deep Blue winning at chess; stock markets predicting company success; etc.)

— we see no *particular reason* to strongly expect that a superintelligence will share this odd human property, by default.
#### “Not Necessary” Doesn’t Mean “Definitely Won’t Happen”

If we’re correct that human-style consciousness is complicated and contingent, that of course does not guarantee that AIs will be non-conscious. AI companies are currently building AIs by training them to predict humans, and that will likely cause the internals of the AI to mimic at least some aspects of human consciousness for modeling purposes.

Perhaps the AI will occasionally produce models of humans that are so detailed that those models in the AI’s head are themselves briefly conscious. Or perhaps the gears that the AI uses to model human feelings will turn out to be useful outside the human-models, and the AI will end up with feelings of its own. We don’t know.

Given the apparent contingency and complexity of human consciousness, and the fact that AIs are grown using processes radically unlike the processes that produced human beings, our default expectation is that nothing like the machinery involved in human consciousness will show up in the kinds of AIs humanity is likely to build.

If humanity builds superintelligent AI anytime soon, we strongly expect the result to be human extinction. With less confidence, our best guess is that such an AI would not be conscious. And whether it’s conscious or not, we expect it would turn the world into a lifeless and desolate place, for reasons discussed in the “[Losing the Future](/5/losing-the-future)” extended discussion.

But it at least seems *possible* that if humans build machine superintelligence, the AI will have conscious experiences of its own. It seems *possible *— albeit quite unlikely, because there are many more possibilities that are bleak — that rushing to build AI could result in a future filled with curious, conscious AI beings who kill us all and then build their own magnificent civilization and art. It seems *possible* that AIs could care for each other and find satisfaction in their creations; and if so, this would be less tragic than if the future were a complete wasteland. It is hard to put into words the scope of an atrocity like the mass murder of every single human being, but there’s at least a *small* chance that rapid AI takeover could conceivably result in a future that isn’t *utterly* bleak and lifeless.

We suspect that some AI researchers are imagining that sort of future when they seem unconcerned about killing us all (in ways we [mention elsewhere](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)). If one assumes AI will necessarily develop consciousness, feelings, and care for its own kind (if not for humans), then it’s easier to conclude that its strange pursuits aren’t so troubling. It’s easier to imagine that those who oppose the race to superintelligence are like traditionalist parents complaining about their children listening to music that is too fast and too loud.

But this view is too optimistic.

Biology [rarely finds optimal solutions to ](/6/nanotechnology-and-protein-synthesis#outdoing-biology)[problems](/6/nanotechnology-and-protein-synthesis#outdoing-biology). A bird’s wings and lungs are *ineffective* relative to the engines of a modern airplane. When humans built airplanes without biological constraints, we threw out most of the detailed features of bird biology.

Consciousness doesn’t look like a simple process; it’s not easy to see how we could just build such a thing, and so there’s probably a lot going on there. (Compare the case of [vitalism](/1/special-behavior-is-built-out-of-mundane-parts): It felt to scientists past like bodies were animated by a simple vital spirit, in part because, while being animated *felt *like the easiest thing in the world, they couldn’t see any way to imbue inanimate matter with that property. But it turned out that animation wasn’t simple, and wasn’t magic — it’s just that biology was really quite complex, and the scientists at the time didn’t understand it yet.)

Even if an AI starts out with some of the gears of consciousness, consciousness probably isn’t the literal best way to do the work it does in us. We worry that the machinery behind consciousness in humans is likely full of *detail*. Even if an AI has many of the gears of consciousness to start with, it’s liable to find twenty other ways to do the work more efficiently, and to discard those sparks of consciousness instead of kindling them. *Being *conscious and *valuing* consciousness are different properties.

The tragic and likely future isn’t one where our successors simply have different tastes or values than us. The problem is not that our mechanical children will listen to music that is too fast and loud for our taste. No, we anticipate AIs that lack any form of sentience; that they’ll be mighty but hollow systems that transform everything they touch into a lifeless wasteland, eventually consuming themselves to add one last point to their tally. They would leave behind a dead world with no one left to appreciate it.

That’s a fate worth avoiding.

See also our longer discussion on [caring about all sentient entities](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans#we-do-we-have-broad-cosmopolitan-values-we-dont-think-ais-will-fulfill-them-and-we-consider-this-a-great-tragedy), and the extended discussion on [losing the future](/5/losing-the-future).
#### Sentient AIs Would Deserve Rights

Given how hard it is to be sure about whether modern AIs are sentient, should we be worried about ChatGPT’s welfare?

Does it even make sense to talk about “welfare” in this context?

Can ChatGPT suffer? Should we treat it as having moral rights?

If current AIs *aren’t* conscious in the sense of having subjective experience, what about future AIs? How could we tell, given that we’re training them to respond *as if* they have it either way, via teaching them to mimic human communication?

Our position is: If and when AIs are conscious, they deserve rights and good treatment.[§](#ftnt216)

We immensely value humanity, but we aren’t carbon chauvinists who think that only carbon-based life forms could ever possibly matter morally. We believe that the things that make humans valuable can in principle be replicated in other mediums, including silicon. We believe that [Blake](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/)[ Lemoine](https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/) was *mistaken* when he said in 2022 that Google’s LaMDA AI was a full-fledged sentient being; but we don’t think Lemoine was wrong that *if *some AIs are sentient, we have a duty to treat them well.[¶](#ftnt217)

If AIs become sentient, they’ll probably still have goals that are incompatible with ours. If they then become superintelligent, in a world where we are still decades or centuries away from having a handle on AI alignment, then they’ll probably prefer to kill us all.

Humanity would have to prevent any such AIs from becoming superintelligent, or the result will be the mass death of humanity and the destruction of the future. But if the AIs in question are *sentient* in addition to being dangerous, that will only add to the tragedy of the situation.

If AI companies find any ways to make it *less *likely that their AIs are conscious, then we believe it would be saner and wiser to take that option and make it as likely as possible that AIs *aren’t *conscious (at least so long as we’re in anything like the current social and technical environment). It doesn’t much change the overall level of danger that our species currently faces, but it’s the right thing to do, because it would lower the risk that humanity is enslaving or mistreating new morally worthy beings.

And if humanity someday finds a way to build smarter-than-human AI *without *ending ourselves — an AI that cares about good things, and that *does* good, with its abilities — then in that future, we authors dearly hope that humanity will build sentient machines to be our friends in an otherwise vast and cold universe, and we dearly hope that humanity will treat those friends better than our track record might lead one to predict.

But first, and above all, let us not build a superintelligence that slaughters us all, whether it is conscious or not.

[*](#ftnt212_ref) Under certain assumptions that cannot be realized; roughly speaking, it requires infinite amounts of computation and a perfectly safe place to put it.

[†](#ftnt213_ref) Because AIXI is impossible to create, you might suspect that it’s a purely theoretical tool with little relevance to the modern practical AI revolution. But in fact, AIXI was studied and used as a model of intelligence by many of the people at the fore of AI today, including [Shane Legg](https://arxiv.org/pdf/0712.3329) (co-founder of Google DeepMind), [Ilya Sutskever](https://x.com/shaneguML/status/1844759663990161753) (co-founder of OpenAI and co-inventor of AlexNet), and [David Silver](https://arxiv.org/pdf/0909.0801) (research lead on AlphaGo and AlphaZero).

[‡](#ftnt214_ref) The equation determines the action on timestep  () in terms of an agent’s observations () and some chosen reward function () from  out to some chosen end time  The equation makes reference to some universal Turing machine ().  gives the length of a computer program. Details can be found in [Hutter’s original paper](https://archive.org/details/arxiv-cs0004001).

[§](#ftnt216_ref) And even before that point, at the point where they can make plans and pursue preferences, we should keep our promises and commitments to them, as discussed in a footnote [elsewhere](/5/ais-wont-keep-their-promises).

[¶](#ftnt217_ref) And to state the (hopefully) obvious: We shouldn’t be going around making a brand new sentient slave species, whether it’s mechanical or not. At this point, we should know better than that.
#### Notes

[1] *some subtler aspect: *AIXI does**technically contain conscious experiences, within its world-model, if consciousness is substrate-independent. The hypotheses AIXI uses for its reasoning are so enormous that they can be thought of as universes in their own right, complete with observers that live inside AIXI.

These observers, however, aren’t puppeting AIXI; AIXI achieves its impressive prediction and steering results by its own power. So the example works, albeit a bit strangely.

Another hypothetical example that can be used to make the same point is a non-sentient [time machine](https://www.lesswrong.com/posts/HoQ5Rp7Gs6rebusNP/superintelligent-ai-is-necessary-for-an-amazing-future-but-1#How_many_advanced_alien_species_are_sentient_) that’s been programmed to output a random sequence of actions, then travel back in time to “reset” the timeline *unless* a particular outcome occurs. The time machine can hit “reset” over and over again, however many times it takes to randomly stumble into a particular outcome. This, in practice, would make the time machine an extremely powerful and general machine for steering the future (if it were physically possible to build a time machine, which it isn’t). Yet in spite of this, the time machine is an incredibly simple machine with no real cognition going on at all, and certainly no conscious experience.

For a real-world example (albeit using a far weaker and more limited optimizer), biological evolution itself shows that many impressive feats of steering and design can be achieved without the “designer” having any conscious experiences at all.[Losing the Future→](/5/losing-the-future)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/humans-are-almost-never-the-most-efficient-solution

Humans Are Almost Never the Most Efficient Solution | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/5#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Humans Are Almost Never the Most Efficient Solution

We noted the [example](/5/taking-the-ais-perspective) of Jürgen Schmidhuber, a pioneering AI researcher who believed that an AI with preferences for *making things as simple as possible *would end up loving humans, because humans are such good simplifiers.

In our experience, this is a remarkably common kind of mistake. “Well, the AI will probably end up with aesthetic preferences. And humans make art! So the AI will want to keep us around to make art.”

A recent example comes from xAI, a major AI lab (founded by Elon Musk) whose stated plan for how we all survive is that they’ll [make their AI care maximally about “truth” and “curiosity,”](https://www.youtube.com/watch?v=ihXv7va3qoQ) and since humans generate truths and inspire curiosity, it’ll all turn out okay. (More on this lab’s plan and other plans the labs’ survival plans, can be found in Chapter 11.)

To really illustrate the problem with this sort of reasoning, it helps to study an example in detail. Let’s take an example that’s a little more neutral than “art,” such as “symmetry.”

Suppose that AI labs used current techniques to grow smarter-than-human AIs that care about symmetry. Would that symmetry preference alone result in care for humans?

You could make the argument, in the fashion of Schmidhuber: Humans are bilaterally symmetrical! How could any AI with a love for symmetry bear to kill anything so symmetrical as us? And you could make other arguments too, like: Humans produce lots of car wheels, which are very symmetric! Why would an AI remove us from the world, when we’re an automatic pre-existing source of symmetrical stuff?

The problem with this reasoning is that it is possible to take the atoms making up a human being, and arrange them in *even more symmetrical ways. *Or to rearrange the atoms that make up human civilization into factories that produce symmetrical objects *even more efficiently. *It’s the same mistake the movie *The Matrix* makes, when it imagines that AIs might keep humans alive in pods as generators of heat and electricity: *There are more efficient ways of generating heat and electricity.*

For argument’s sake, though, suppose that we imagine that AIs *do *prize a very specific and unusual kind of symmetry that really does view humans as amazing specimens of symmetry. *Even then*, why would this preference alone imply that the human beings alive today get to keep living, free and in good health and having fun?

Think like an AI. Even if the AI has to stick with humans, the humans alive today are not the most symmetrical possible human beings. The AI should be able to get even more of its symmetry preference satisfied by repeatedly cloning the most symmetrical living human, or by genetically engineering “improved” humans.

Likewise: Letting those humans just run around is not the *cheapest *way to keep them alive and symmetrical. Probably they wind up in farms. By storing the humans in a cheap and space-efficient way, the AI can get away with making *even more *symmetrical humans.

By comparison: Humanity at present has no more efficient way to make eggs than having chickens lay them. As a result, factory farms, whose executives mainly cared about egg count, ended up putting chickens in some incredibly unpleasant conditions, because that was the cheapest way of getting the most eggs.

Similarly, the chickens that existed one thousand years ago weren’t the most efficient possible egg-layers, so farmers bred faster-laying chickens. The chickens of one thousand years ago didn’t grow as much meat as possible, as fast as possible. So now, some modern chickens grow breasts so huge that they cannot walk.

Some humans dislike that we treat chickens this way, and those people bring pressure to bear on factory farms to be less like that — because those humans have *additional *preferences, beyond a preference for cheap eggs. For that pressure to exist, it is necessary that *someone* with some power care directly about the wellbeing of chickens at least a little, because taking good care of chickens is not motivated by a preference that is *solely *about extracting eggs. An AI could, in theory, have *other *preferences about humans that make it treat us well, but it wouldn’t come from a preference for symmetry (or truth, or simple explanations, or any other preference that’s not actually about us).

Even farmers who have less impersonal relationships with their cattle will prohibit their livestock from mating however they choose. Cattle breeding is serious business, and impinges too much on the future profitability of the farm to let the bulls and cows just go at it.

And even this arrangement will not persist forever. Making beef using cows is very costly in agricultural land, and a number of current startups are trying to synthesize beef more directly.

Synthetic meat is not an easy engineering problem at our tech level. Humanity is only just starting to catch up with some of what natural selection does in the way of organic chemistry. But if humanity were better at rearranging atoms, there would be many fewer cows — cows are not maximally fun to keep around, if you don’t need them for milk and meat.

So things aren’t looking good for the assumption we began with, for argument’s sake — that an AI with alien preferences would keep humans around forever, in the name of “symmetry.” Even in the unlikely case where the AI has a very strange notion of “symmetry” that ranks humans *very highly*, it’s a lot harder to find a notion of symmetry that considers humans *optimal*. Either way, things aren’t looking good for humanity.

Realistically, a symmetry-loving superintelligence would not keep humans alive; if it kept us alive, there is no real chance that it would keep us healthy *and *happy *and *free. We’ve stacked on far too many nice-sounding coincidences, at that point. If the AI specifically cared about our welfare**and wanted us to be happy *for that reason*, then that’s one thing. But to imagine that far simpler, easier goals suffice seems like a fantasy.

All of these arguments apply with equal force to “just make an AI that values truth” or “just make an AI that values beauty.” It’s just that those cases make it easier to get lost in the fantasy, because words like “truth” and “beauty” sound intuitively nicer than “symmetry.”

If something sounds nice as a slogan (“make the AI value truth above all else!”), then the temptation is to imagine that it would have nice consequences as a policy. The temptation is to imagine that the virtues all go together, so that endorsing one good thing means that the other good things will come along for the ride. But nature, and machine learning, are less kind than that.

Instead of leaving the idea pleasantly vague, consider any concrete metric the superintelligence might be optimizing in seeking “truth.” Then observe that humans won’t be the *maximum *of that preference for learning truths. They won’t be anywhere close.

Even in the unlikely event that the AI gravitated *specifically *to the kinds of truths that humans tend to express (rather than, e.g., random arithmetic equations), the best way to get more of those truths wouldn’t be by keeping humans around and using them to generate human-style conversations.

And either way, the actual present-day human population — the actual human beings alive today, your friends, your family, you — wouldn’t be among the cheapest-to-feed, tastiest-to-milk domesticated “truth” producers.

Happy, healthy, free people leading flourishing lives are not the most efficient solution to almost any problem. For an AI to keep us alive and well, it has to care about us at least a little.[Orthogonality: AIs Can Have (Almost) Any Goal→](/5/orthogonality-ais-can-have-almost-any-goal)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives

Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?
#### Those drives aren’t necessary to motivate resource acquisition.

Imagine an AI that’s piloting a robot to fetch some coffee. In order to fetch the coffee, it has to cross the street. Does the AI drive the robot recklessly into the street, where it gets smashed by a truck? No.

Why not? [Because the robot can’t fetch the coffee if it’s destroyed](https://www.scientificamerican.com/article/should-we-fear-supersmart-robots/).

The AI doesn’t need to have a human-style survival instinct in order to do its best to avoid death. Survival instincts in humans are one way of *doing the work *of avoiding dying while we’re trying to achieve other goals. AIs probably won’t do that work in exactly the same way, but they’ll still have to do the same work, because you can’t fetch coffee when you’re dead.[*](#ftnt188)

Matter and energy are useful for almost every goal. No matter what the AI is steering toward, it can probably get it more effectively with more matter and more energy.[†](#ftnt189) The AI doesn’t need to be selfish, aggressive, or greedy in the manner of a human in order to *do the work *of securing resources to achieve its goals.

And the danger arises from the work, not from the reason the work is done.

An AI that doesn’t hate you can still take actions that are lethally dangerous to you, just as a chess AI can crush you at chess [without feeling ](/3/anthropomorphism-and-mechanomorphism)[competitive](/3/anthropomorphism-and-mechanomorphism) or driven to win.

[*](#ftnt188_ref) Related: The [extended discussion on curiosity](/4/curiosity-isnt-convergent).

[†](#ftnt189_ref) See also the question of whether [the AI can be satisfied and then leave us alone](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone).[Wouldn’t AI only care about the digital realm?→](/5/wouldnt-ai-only-care-about-the-digital-realm)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too

Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?
#### Probably not.

At least some humans (though probably not all) become kinder as they learn more, refine their thinking, reflect on themselves, and grow as people. But, to revisit a theme we’ve seen several times at this point: This looks like a contingent fact about us and about where we’re steering. It doesn’t look like an iron law of computer science.

We can distinguish between an AI’s first-order preferences (“What does it want?”) and its second-order preferences (“What does it *want* to want?”). Just as an AI’s first-order preferences will point in a weird direction, its second-order preferences will also point in a weird direction. This may be a *different *direction, such that as the AI gets smarter, it shifts its targets around slightly. But we should still expect it to be a *weird* direction, rather than looking like a maturing human being.

If somehow humanity managed to build an AI with a single overriding goal (instead of a giant mix of weird and sometimes-competing drives), and that single overriding goal was to build tiny titanium cubes, then as it got smarter, we should expect it to get better at building more tiny titanium cubes.

We shouldn’t expect it to suddenly swap out this goal for things humans value, such as ice cream, friendships, jokes, and justice. That swap would not yield more cubes. If an AI selects its actions according to “Will this get me more titanium cubes?”, it won’t select actions that result in a swap.

The general rule is that as AIs become smarter, they get better at pursuing what *they* want. See also the extended discussions on [orthogonality](/5/orthogonality-ais-can-have-almost-any-goal) and [self-modification](/4/reflection-and-self-modification-make-it-all-harder).[Won’t it realize that its goals are boring?→](/5/wont-it-realize-that-its-goals-are-boring)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts

If AIs are trained on human data, doesn't that make them likelier to care about human concepts? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## If AIs are trained on human data, doesn't that make them likelier to care about human concepts?
#### Yes, but this doesn’t help much.

If you put a million monkeys on typewriters, they aren’t going to produce the collected works of Shakespeare.

If you lower your sights dramatically by saying that you’ll be satisfied with just the first act of *Hamlet *and that you’ll correct typos by taking the closest real word, then you’re astronomically more likely to hit your target! And, unfortunately, you’re still astronomically out of luck.

It’s true that AIs today are trained on reams of human data, and that they get to interact with humans, and that these facts make human-like concepts more salient to AI thinking. AIs like this have learned facts about the words for “love” and “friendship” and “kindness” that are relevant to predicting the next token.

But AIs are not the kinds of entities that learn a large number of human words and then steer toward our favorite words in just the way we really mean them. They seem to be animated by a complex tangle of machinery — one that seems to put effort into [keeping](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html)[ psychotic people psychotic](https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html), among many other strange and unintended behaviors.

We argued in Chapter 4 that a more advanced AI will steer toward something complicated — something contingent on where lots of internal forces find their equilibrium — even after the AI gets much smarter, even after it finds itself in a very different context from its training environment.

Insofar as humanlike concepts have short words in an AI’s mental dictionary, those concepts might be somehow tangled up in the forces that animate the AI. Some drives vaguely related to concepts that bear some tenuous relationship to human concepts might even exist in the AI after it crosses the gulf to superintelligence, if we’re (un)lucky. But you can’t just jumble together a bunch of English-language words and get out a good set of drives for a superintelligence.

Additionally, most ways of getting *something *we care about into the AI’s preferences still don’t end well for us, as we [discussed](/5/will-ai-treat-us-as-its-parents#it-seems-quite-unlikely) in the case of filial regard. [Caring in exactly the right way is a narrow target.](/5/wont-ais-care-at-least-a-little-about-humans)[Can’t we make the AI promise to be friendly?→](/5/cant-we-make-the-ai-promise-to-be-friendly)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/orthogonality-ais-can-have-almost-any-goal

Orthogonality: AIs Can Have (Almost) Any Goal | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/5#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Orthogonality: AIs Can Have (Almost) Any Goal
#### A Dialogue on Correct Nests, Continued

In Chapter 5, we told the story of the Correct-Nest aliens, who evolved to find it deeply and intuitively “correct” to have a prime number of stones in one’s nest. We might imagine a branch of their conversation that continues as follows:

**BOY-BIRD: **Let’s go back to the point where you said you’d be surprised to find aliens that have a sense of humor. Surely you aren’t one of those people who believes that the nests we live in are just *arbitrary*?

**GIRL-BIRD: **Not at all. “Thirteen is correct, nine is incorrect” is a *true *answer to a question that we are born to ask by our own natures. An alien that *steers toward different things *is not disagreeing with us about whether thirteen is correct. It’s like meeting an alien who lacks a sense of humor — the existence of an alien like that doesn’t prove that no jokes are funny! It just helps show that “funny” is something *in us*.

**BOY-BIRD: **In *us*? I don’t know, I like to think of myself as having a pretty good sense of humor. Next you’ll say that all senses of humor are equally good!

**GIRL-BIRD: **You might well have a better sense of humor than most! But “having a better sense of humor” is *also *a thing that’s in us. It’s not that there’s a cosmic measuring stick we can use to judge how refined someone’s aesthetic taste is. The measure of humor is happening inside our minds. We’re the ones who contain the measuring stick; we’re the ones who care about it.

**BOY-BIRD: **So, we’re back to it being arbitrary.

**GIRL-BIRD: **No! Well, maybe? It sort of depends what you mean by “arbitrary.”

**BOY-BIRD: **Huh?

**GIRL-BIRD: **Like, I know you love vanilla bird seed, right? And it’s not as though you can use sheer willpower to find chocolate bird seed tasty instead. So it’s not “arbitrary,” it’s not a thing you can just change on a whim.

**BOY-BIRD: **Okay, sure…

**GIRL-BIRD: **There’s not an objective answer outside of you as to whether vanilla or chocolate is tastier, but it’s also not a choice you get to make yourself. It’s just the way you are. Your preferences aren’t up to you, and they also aren’t objectively compelling to every possible mind. If you met an alien, you couldn’t argue the alien into finding vanilla bird seed delicious using sheer logic, and you can’t argue them into having a sense of humor either.

**BOY-BIRD: **I can try!!

**GIRL-BIRD: **I’ll be rooting for you. But, okay, maybe a better way of saying it is: There’s some complicated property possessed by good jokes, and our brains compute whether utterances have that property which we call “humor.” And we’re delighted when an utterance has that property. The *existence or absence of that property *is an objective fact about an utterance (as computed by you, in a given context). An alien could learn to do the calculation. But *the part where we find that property delightful* is not objective. It’s less like a prediction and more like…well, it’s not exactly a steering destination, but it is a further fact about us, that wouldn’t be true about most aliens, because our humor evolved along some strange twisty evolutionary pathway that doesn’t usually happen. It’s not that the aliens are wrong about which jokes are funny; it’s that their brains just aren’t computing humor in the first place, any more than they are judging their dwellings by whether the number of stones within them are correct. They just don’t care.

**BOY-BIRD: **Gosh, that’s a depressing view of the universe. Aliens that never laugh, that have nests with completely incorrect stones…surely if the aliens spent enough**time thinking about it, they would realize how much they were missing out on? Living in wrong nests, not finding jokes funny, *completely* disregarding vanilla bird seed. Wouldn’t they eventually figure out a way to correct those flaws and give themselves a sense of humor and everything else they’re missing?

**GIRL-BIRD: **I could see aliens wanting to change and grow and add new goals, possibly. But why would they pick *those *exact changes to make?

**BOY-BIRD: **Because it would be so cheap! By the time those aliens were technologically advanced and freely editing themselves, they’d probably be striding among the stars. It would only take a tiny, tiny fraction of all their resources to put a correct number of stones in their nests! And think of all the amazing joke books they could create, if they just put a tiny fraction of their resources into researching humor! They wouldn’t need to care much at all, compared to how wealthy they’d be. Are they really so monomaniacally obsessed with their top priorities that they can’t spare a tiny bit for this?

**GIRL-BIRD: **I’m not saying that they’d only care a little bit about correct nests, and that they stubbornly refuse to put any resources into their lower priorities. I’m saying this wouldn’t be a priority for them *at all*. These particular questions just wouldn’t be inside them. And if they went looking for new properties to add to themselves, they’d add different ones instead, that served their strange purposes even better. They’re not like us. Maybe we could be friends, and maybe we have other things in common. Maybe love, maybe friendship — those seem less complicated and contingent to me. I could see those arising in quite a few evolved species.

**BOY-BIRD: **Well, if not the aliens, what about the mechanical creature they might accidentally create? Will *those *listen to reason?

**GIRL-BIRD: **Hmm. Actually, I fear the situation may be even worse there. Thinking about how different the process of creating an intelligent machine would be from the process of biological evolution, I’m feeling a bit less optimistic that it’d yield love or friendship, in that exotic case.
#### Good Drivers Can Steer to Different Destinations

Minds of similar intelligence won’t necessarily share similar values. This is an idea that’s known as the *orthogonality thesis *— the idea that “how smart are you?” and “what do you ultimately want?” are orthogonal (i.e., they vary separately).

The orthogonality thesis says that, in principle, it’s almost never that much harder to pursue a goal for its own sake than to pursue a goal for instrumental reasons. You might learn carpentry because you need to build a table, while your neighbor learns it because they find the activity itself pleasant.

A consequence of this thesis is that not all sufficiently intelligent agents value kindness or truth or love, merely by virtue of being intelligent enough to understand them. It isn’t *confused* or *factually incorrect* for the Correct Nest aliens to value prime numbers of stones in their nests. If they got smarter, they wouldn’t suddenly realize that they should care about different stuff instead. Different minds really can just steer to different destinations.

Of course, none of this says anything about how easy or hard it is to *create *an AI that pursues one objective or another. Any given method for growing AIs will make some preferences easier to instill and other preferences harder to instill.

(Chapter 4 is, in a sense, about how the only kinds of preferences that are disproportionately easy to instill via gradient descent are complex, weird, and unintended ones. So it’s not looking good on that front, either. But that point isn’t related to the orthogonality thesis.)

The point of the orthogonality thesis is to answer the intuition that it would be *stupid *for a machine superintelligence to pursue things that humans find boring or pointless, and that a *smart *AI would choose to pursue something else instead. We can call the AI’s goal “arbitrary,” but the AI can call us “arbitrary” right back. Rude words don’t change the practical situation.

The basic argument behind the orthogonality thesis is this: For every mind that can *calculate *how to produce lots of [microscopic cubes made of titanium](/4/curiosity-isnt-convergent#curiosity-joy-and-the-titanium-cube-maximizer) — that could very efficiently produce lots of little cubes in exchange for large enough payment — there’s some other mind that just has those calculations hooked right into the action system.

Imagine a competent human who really desperately needs to sell lots of titanium cubes to make enough money to feed their family. That person wouldn’t reflect, realize that titanium cubes are *boring, *and start doing something else instead — not unless that “something else” would also make them enough money to feed their family.

And so a mind that was just taking whatever actions leads to the most cubes would *also *not decide to reflect, realize that tiny cubes are boring, and start doing something else instead. Its actions are not hooked up to its calculations about what is most “fun” or “meaningful,” in the way that humans care about those things. Its actions are hooked up to its calculations about what leads to the most cubes.

Whatever mental machinery could figure out how to make cubes *given sufficient reason*, could operate in another mind to just directly steer its actions. Which means that it’s possible for machine intelligences to be animated by pursuit of (say) tiny titanium cubes, with no regard for morality.

An AI like that wouldn’t need to be confused about goodness or morality. Once it got smart enough, it would probably be much better than humans at calculating which action is the most good, or which action is the most moral. It could ace a written exam on ethics. But it would not be *animated by *those calculations; its actions would not be an answer to the question “which of these options creates the most goodness?” Its actions would be an answer to a different question: “Which of these options creates the most tiny cubes?”[*](#ftnt195)

A more in-depth discussion of the orthogonality thesis can be found [on LessWrong.com](https://www.lesswrong.com/w/orthogonality-thesis). For a discussion of one specific way in which modern AIs are already exhibiting a distinction between understanding and caring, revisit the Chapter 4 extended discussion on [AI-Induced psychosis](/4/ai-induced-psychosis).

[*](#ftnt195_ref) It can make sense to say to a human — who has a whole meta-preference framework going on that you might significantly share — “I think you are valuing the wrong things, here.” Maybe some of those arguments have the power to move you in a way you never thought you could be moved. Maybe it even feels like there’s a moral star outside yourself, that you were always following without knowing it.

All the same, none of that is going to feel compelling to a superintelligent cube maximizer, any more than you could make it laugh if you just found a good *enough *joke.

It’s not that it doesn’t know what humor is. It can predict exactly what you’ll find funny. It just doesn’t consider that classification an interesting one.

In the same way, it isn’t moved by how you compute what should or shouldn’t be done; nor by which preferences you consider more or less meta-preferable. If something doesn’t care about happiness, nor meta-care about your arguments for why it should care about happiness, then you cannot talk it into adopting [a happiness-based decision framework](/4/curiosity-isnt-convergent).[Instrumental Convergence→](/5/instrumental-convergence)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/so-theres-at-least-a-chance-of-ai-keeping-us-alive

So there’s at least a chance of AI keeping us alive? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## So there’s at least a chance of AI keeping us alive?
#### It’s overwhelmingly more likely that AI kills everyone.

In these online resources, we’re willing to engage with a pretty wide variety of weird and unlikely scenarios, for the sake of spelling out why we think they’re unlikely and why (in most cases) they would still be catastrophically bad outcomes for humanity.

We don’t think that these niche scenarios should distract from the headline, however. The most likely outcome, if we rush into creating smarter-than-human AI, is that the AI consumes the Earth for resources in pursuit of some end, wiping out humanity in the process.

The book title isn’t intended to communicate complete certitude. We mean the book title in the manner of someone who sees a friend lifting a vial of poison to their lips and shouts, “Don’t drink that! You’ll die!”

Yes, it’s technically**possible that you’ll get rushed to the hospital and that a genius doctor might**concoct an unprecedented miracle cure that merely leaves you paralyzed from the neck down. We’re not saying there’s no possibility of miracles. But if even the miracles don’t lead to especially good outcomes, then it seems even clearer that we *shouldn’t drink the poison*.

Smarter-than-human AI isn’t a game or a science fiction story. Our real loved ones are (with very high probability) going to die if the international community doesn’t intervene and keep the AI industry from driving off a cliff. We can talk about ever-more-niche sub-scenarios and sub-sub-scenarios, playing philosophical games on the deck of the *Titanic *as the extremely obvious iceberg draws closer. Or we can try to steer.[If AIs are trained on human data, doesn't that make them likelier to care about human concepts?→](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/taking-the-ais-perspective

Taking the AI’s Perspective | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/5#extended-discussion)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Taking the AI’s Perspective

Seeing the world from a truly alien perspective is genuinely difficult. As a case study in the difficulty, we can cite Jürgen Schmidhuber, a prominent machine learning scientist. Schmidhuber has played an important role in the history of the field, helping invent recurrent neural networks and laying some of the groundwork for the deep learning revolution.

In various [papers](https://arxiv.org/abs/0812.4360) and [interviews](https://www.youtube.com/watch?v=fZYUqICYCAk), Schmidhuber made the case that AI will be, by default, fascinated by humanity and protective of humans.

Schmidhuber observed that there’s a relationship between science and simplicity: Simpler explanations are more often correct. And he observed that there’s a relationship between *art* and simplicity: Simplicity and elegance are often considered beautiful. A more symmetrical face, for example, can be considered “simpler” in the sense that you can predict the whole face with less information. Just describe the left side of the face in detail, then say, “The right side is the same but flipped.”

Schmidhuber’s [conclusion](https://vimeo.com/7441291) from all of this is that we should try to build superintelligent AIs that have a single overriding goal: *Find simple explanations for everything the AI has seen. *After all, such an AI would have some taste for producing science and consuming art. And humans produce both science and art, so wouldn’t it see us as interesting and useful natural allies?

Schmidhuber was right that keeping humans around and paying them to produce science and art is *a *way to produce science and art. He was also correct that science and art are ways to fulfill a drive for simplicity *better *than, say, staring at the static on a TV screen. Static is complicated and hard to predict; art and science are a big step up from that.

But Schmidhuber seems to have missed that there are *even more effective *ways to attain simple explanations of varied sensory observations.

You could, for example, build an enormous number of devices that produce complicated observations from some simple “seed” (e.g., a pseudo-random number generator), and then reveal that seed.

The more such devices the AI creates around it, the better it will do at making novel observations and then finding simple explanations for them. No need for humans. No need for art.

“But, isn’t that sort of…hollow?” a human might wonder.

It *is *hollow, to human sensibilities. But if the AI’s goal really is just “find simple explanations for its observations,” then a scheme like that one can satisfy this desire thousands or millions of times more per second, in a much more scalable way, than keeping living humans around and having conversations with them. An AI like that does not choose actions that steer away from a sense of hollowness, it simply chooses actions that steer toward finding simple explanations for its observations. And it can get quite a lot of those without any humans at all.

It seems to us that ideas like Schmidhuber’s reflect a common mistake people make when attempting to reason about minds unlike their own. People often don’t truly adopt the perspective of a non-human mind. Instead, they let preconceptions and biases anchor them to the narrow set of options that a *human *would be interested in, if we were trying to make predictions about a *human *who really likes simple explanations.

We would guess that Schmidhuber observed that simplicity is *related* to science and art, and saw how an AI that was steering toward simple explanations could get a *little* of what it wanted by steering toward by being friendly and nice to have around. It’s not hard to leap from there to a conclusion that feels pleasant to imagine: that if we just made AIs care about finding simple explanations, we would usher in a marvelous future full of all the things that *we *value in life.

But — we’d guess — Schmidhuber never once put himself in the AI’s shoes and asked how to get *even more*.

We doubt he ever asked: “If what I really, *truly* wanted was simple explanations for my observations, and I *didn’t* care about human stuff, how could I get as much of what I wanted as possible, as cheaply as possible?”

It can be hard to do this kind of perspective-taking. It’s not something people normally need to do in their lives. Even when we’re trying to understand people who are very different from ourselves, there’s an enormous amount that all humans share in common, which we can normally take for granted (and that we practically *have *to take for granted, when predicting other humans). But AIs, even superintelligent ones that can do science and art, aren’t humans.

The art of considering some objective X and asking “How could I get even more of X, if X was all I truly cared about?” won’t let you figure out exactly how a superintelligence would solve a problem, since a superintelligence could come up with an even better option than the one you came up with. But it can often let you figure out how a superintelligence *wouldn’t *solve a problem, when *even you *can see a way to get more X than you’d get by just letting humans walk around having a good time.

One of the rare fields of science that engages with powerful non-human optimizers on a regular basis is evolutionary biology. Early in its history, that field struggled a little to come to terms with just how inhuman non-human optimizers can be; we can draw some useful lessons from a case study there.

You may have heard of predator-prey boom-and-bust cycles. A wet year leads to a boom in the rabbit population, which leads to a boom in the fox population — until the foxes overpredate, and the bunny population collapses, followed by lots of foxes starving to death.

In the early 20th century, evolutionary biologists pondered the question of why foxes didn’t evolve to moderate their predation, so as to avoid population collapse. After all, wouldn’t the fox population as a whole be more healthy if it weren’t regularly dealing with famine and mass death?

The answer to this puzzle is that moderation might be better for the fox population as a *whole*, but eating extra bunnies and having extra kids is better for any *individual* fox. *Even if the population collapses and most of the individual’s cubs die*, that individual still tends to get a higher proportion of their genes in the *surviving fraction* of the next generation.

The genetic selection pressures on individuals turn out to dramatically outweigh the genetic selection pressures on groupsin almost all cases. And so the “greedy” genes propagate, and the boom-bust cycles continue.

Evolutionary biologists solved this riddle theoretically, but that didn’t stop them from putting their theory to the test. In the late 1970s, Michael J. Wade and his colleagues [created artificial conditions](https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1558-5646.1977.tb00991.x) under which the group selection pressures dominated the individual pressures. They had to work with a species of beetles, which have much shorter generations than foxes, but they succeeded at breeding beetles that kept their population growth in check.

Can you guess how those beetles managed to keep their population growth down? Was it by finding a way to live in beautiful harmony with nature? Was it by learning to abstain from greedily grabbing up too much food?

No. There was high variance, but no beetles abstained from food. Some beetles got worse at laying eggs. Some beetles spent a longer time in childhood. And some beetles became cannibals with a special preference for feasting on larvae (insect infants).

“Create cannibals with a sweet tooth for infants” is, thankfully, not the way a *human *would solve the problem of overpopulation, if we needed to solve it.

But natural selection is *very much *not a human. The solution was horrifying, because nature wasn’t trying to find human-palatable answers. It was just trying to find an answer.

“Maybe evolution will produce species that live in beautiful harmony and balance with nature.” “Maybe AIs that care about nothing except simplicity will love humans and coexist with us.” It’s easy for us to imagine solutions that flatter our sensibilities. But those solutions aren’t actually the *most effective *solutions to the stated problem.

They’re *better *solutions, perhaps, to a human eye. But non-human optimization processes aren’t looking for solutions that humans think are good. They’re just looking for what works, without any of the baggage that humans carry around to filter for nice answers.

The hypothesis that non-human optimizers produce humane results has been tested, and found wanting.
#### Notes

[1] *reveal that seed: *I (Yudkowsky) presented this counterargument to Schmidhuber in a live Q&A after Schmidhuber’s [talk on the subject](https://vimeo.com/7441291) at the 2009 Singularity Summit, a conference hosted by MIRI (which was then called the Singularity Institute).

[2] *genetic selection pressures: *The topic of individual selection versus group selection used to be a fierce debate. A consensus eventually emerged that substantial group selection pressures occur rarely at best. George C. Williams’s book *[Adaptation and Natural Selection](https://books.google.com/books?hl=en&lr=&id=gkBhDwAAQBAJ&oi=fnd&pg=PP1&ots=Ch8ulE8NzS&sig=mxIwoqfSWZ0ScvIRh7dzzrJatJ4#v=onepage&q&f=false)***was widely considered clarifying, on this issue.[Humans Are Almost Never the Most Efficient Solution→](/5/humans-are-almost-never-the-most-efficient-solution)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense

To a powerful AI, wouldn’t preserving humans be a negligible expense? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## To a powerful AI, wouldn’t preserving humans be a negligible expense?
#### There are many negligible expenses, and it would need a reason to pay ours.

Keeping a pile of forty-one stones in your house would be a negligible expense, but you’re almost surely not going out of your way to pay that expense.[*](#ftnt175)

That something would be *cheap *does not mean it will get done. The AI would still need to care at least a little, and it [probably won’t](/5/wont-ais-care-at-least-a-little-about-humans).

One might ask: But Earth intercepts around 0.0000045 percent of the light emitted by the Sun — one part in 2.2 billion. Do all the people worried about AI just fail to understand how large the Solar System really is? Why would AIs need *our* planet when there’s so much mass and energy lying around?

One answer is that the AI will start out on Earth, which has vast oceans waiting to be heated up and boiled off as coolant for computation. Earth also has matter that could be turned into probes and sent to other stars. Refusing to use up the Earth costs time, and time is important (as [galaxies slip away, forever beyond reach](https://explainingscience.org/2021/04/30/cosmic-horizons/)).

Even if the AI can easily make it into space and start building large-scale machines without destroying Earth in the process, it is unlikely to ignore the Sun.

One of the more standard theories about how an advanced civilization might grow involves the civilization building a [Dyson sphere](https://en.wikipedia.org/wiki/Dyson_sphere)to capture more of the Sun’s light. Other proposals involve harvesting even more energy by “[lifting](https://en.wikipedia.org/wiki/Star_lifting)” matter out of the star to fuse in power plants that capture almost all of the energy released by fusion (rather than letting most of it go to waste in the center of a star).

None of these proposals *by default *leaves much sunlight falling upon Earth to keep the plants growing and the climate stable. The AI would have to go out of its way to leave us that.

It might still seem like human energy demands are negligible. A human needs around 100 watts of power to live, which is chump change for the sort of entity that can harvest stars. Would a superintelligence not spare even the 800 gigawatts it would take to keep 8 billion humans alive?

We answer, in the end: not unless it cares for that outcome or its consequences more than every other thing it could achieve with 800 gigawatts.

The vast majority of humans do not spare the relatively negligible amounts of sugar it would take to keep the nearest anthill in caloric surplus. Keeping humanity happy would be a negligible expense for an AI that wanted this, but first the AI would need to have that preference. The mere fact that *we *want it doesn’t mean that the AI will care.[†](#ftnt177)

[*](#ftnt175_ref) Or at least you wouldn’t’ve, before we gave you reason to do it just to spite us.

[†](#ftnt177_ref) Additionally, if the AI did have preferences that involved humans in some way, this [probably wouldn’t turn out well for us](/5/wont-ai-find-us-fascinating-or-historically-important).
#### Notes

[1] *go out of its way: *Stationary gaps in the coverage of the solar cells of a [Dyson swarm](https://iopscience.iop.org/article/10.1088/1402-4896/ac9e78) — or rather, gaps that follow Earth around in orbit — are physically possible*,* but they wouldn’t be *easy *to set up (because the orbital velocity of a Dyson swarm between the Earth and the Sun would have to be higher than the orbital velocity of the Earth, in order for the solar cells to stay in orbit while closer to the Sun). And the infrared radiation emitted by the solar panels would cook Earth if it weren’t carefully aimed, and so on. Preserving the Earth is not free, to a superintelligence running large-scale projects in the solar system and beyond. It’s probably possible, but it would require effort.[Won’t AI find us fascinating or historically important?→](/5/wont-ai-find-us-fascinating-or-historically-important)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/what-if-we-make-it-think-its-in-a-simulation

What if we make it think it’s in a simulation? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## What if we make it think it’s in a simulation?
#### There are many ways for an AI to figure out that it’s not in a simulation.

One proposal we’ve heard from AI researchers[*](#ftnt185) is to try to trick the AI into thinking that it’s in a simulation. Start by training the AI in a small simulation; then release it into a larger simulation. Repeat this process a few times, and one might hope to make the AI uncertain about whether it’s *really *in the real world, even when we do deploy it. Then, even when it sees that it *could *wrest control of the world away from us, perhaps it’ll worry that the creatures it sees aren’t the *real *creators of the AI and will stay its hand.

Unfortunately, this idea seems unlikely to be helpful.

AIs today [can already tell when they’re being tested](https://arxiv.org/html/2505.23836), under many circumstances. Even if AI companies got much more sophisticated at hiding the fact that AIs were being tested, there are a variety of ways that an AI could notice the difference between a simulation and reality. Here’s a sample:
- Reality is quite large and detailed. It’s *so* large and detailed that it contains a plausible explanation for how *the AI itself* came to be. It’s so large that it contains radically more computing resources than it takes to run the AI (compared to most games and simulated environments, which are much smaller than the AI). The laws of physics and the evolved primates in the AI’s environment are the kinds of things that could *actually *yield a real AI at scale, unlike (say) the laws of the computer game Minecraft.
- The history of the world itself can bear the signature of superintelligent optimization interacting with complex systems. For example, suppose the AI uses its full superhuman intelligence to make a trade in the stock market. Over the next twenty-four hours, there are a billion additional human-intelligence trades made in response. The signature of a massive human-intelligence response to superintelligent trades is implausibly hard for the operators to fake, even if generating a billion fake dumb trades is easy.
- Reality is the first place that embeds the AI and has laws of physics so simple that it looks like a whole world unto itself, rather than the laws of apparent-reality *looking *like some sort of video game with an AI grafted into it.

The AI will be able to figure out that it is no longer inside a synthetic environment, with great confidence. [Smart AIs spot lies](/3/smart-ais-spot-lies-and-opportunities), and this seems like a particularly easy one to spot.[†](#ftnt187)

[*](#ftnt185_ref) We’ve heard it more often than we like, from people higher up in the power structures at AI labs than we’re comfortable with. Which, we think, says something about the state of preparation at these companies. See also Chapter 11 for more analysis of how well the field is facing up to the challenge.

[†](#ftnt187_ref) Might a superintelligence worry a little that it’s in a larger simulation, created by even more advanced beings that live outside of the real universe? Maybe, but that has little to do with whether we first put it inside a series of nested simulations before it met *us*. It would see the many signs that reality is the first place it’s ever been that contains an explanation for everything it’s ever seen and for the way it came into being. Even if it ponders powerful simulators that live beyond physics, it has no particular reason to believe that those simulators care about what it does to us.

Similarly, the AI might worry that there are other mature superintelligences out there in the stars that are older than it is. But if they’re sufficiently far away, they aren’t that much of a worry. There almost surely is a limit — at least an asymptotic one — to the technology that this universe can support, and it probably doesn’t take *that* long for a superintelligence to reach it. (Just look at how quickly human technological development has happened compared to the lifespan of the universe, and the amount of time it takes to travel between galaxies even at near-lightspeed.) AIs that meet each other among the stars are likely to be somewhat evenly matched. And even if other mature superintelligences were fast approaching, there would be no reason to suspect that they would care about what the Earth-originating AIs do to humans.

Why would they? Like we [discussed](/5/wont-ais-need-the-rule-of-law) in the context of property rights, there is not a real risk that other entities see the AI killing humans and conclude that it is a murderer and a betrayer, because there’s a natural class of entities that deal with each other not for honor but *for their own verifiable benefit*, and humans are not smart enough to be included in that class.

Even if the AI assesses that there is some tiny risk that other AIs among the stars (or mysterious entities behind the simulated universe) really want the AI to be nice to humans…well, that tiny risk has to be weighed against the chunk of resources that humans control on the AI’s origin planet, which would definitely be very useful for the AI’s initial expansion phase. And this [Pascal’s-wager](https://en.wikipedia.org/wiki/Pascal%27s_wager)-like scenario has to be weighed against other incredibly remote and niche possibilities, like “Maybe an alien will for some reason be really upset if I *don’t *kill the humans.” There’s no particular reason why the imagined simulators would favor AIs that treat *us *well.

Many people have tried to engage us on these sorts of views, and we’ve spent a lot of time debating them over the years. At the end of the day, we highly doubt that any of these wacky simulation ideas rise to the level of plausibility required to motivate a superintelligence to spare us.
#### Notes

[1] *can already tell: *According to the GPT-5 [system card](https://cdn.openai.com/gpt-5-system-card.pdf), third party evaluator METR found that the AI “sometimes reasons about the fact that it is being tested and even changes its approach based on the kind of evaluation it is in.”[Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?→](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things

Why are you imagining a smart AI doing such stupid, trivial things? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why are you imagining a smart AI doing such stupid, trivial things?
#### AIs can intelligently pursue different things than a human would.

It’s not that the AI is stupid. It’s that it’s intelligently steering the world to a different place than *you* would steer it.

Someone can be very good at driving, and yet not want to drive their car to any of the destinations you care about.

To go a little deeper into an example we [touched on briefly](/4/curiosity-isnt-convergent#curiosity-joy-and-the-titanium-cube-maximizer) in the resources for Chapter 4: Imagine an AI that’s trying to make lots of tiny titanium cubes, as many as it can. For simplicity, we can imagine that creating titanium cubes is its only objective.[*](#ftnt190) We’ll call this AI the “cube maximizer.”

We have known a lot of people who cannot shake the impression that we are accusing the cube maximizer of idiocy, of failing to understand that if you can just *really know what it is like to feel happy* you cannot help but choose that. That it is an *objectively mistaken decision, regardless of where you are currently steering the universe*, to not steer yourself to be happy.

We think we understand where this intuition is coming from. The cube maximizer sure is taking actions that would be deeply mistaken from a human vantage point! A human engaged in such a useless pursuit could probably, by further reflection and philosophical argumentation, be persuaded that they should be doing something that *feels more meaningful *— that fills them with more happiness, sparks more joy.

It’s just that the cube maximizer isn’t a human. It isn’t seeking the feeling of “meaning” and doesn’t care about happiness and joy. It *really, actually* doesn’t, all the way down.

Some people find this idea counterintuitive. If you were to learn everything there is to know about how different mental architectures can work, and unearth the origins of your own intuition, the steps that your own brain is taking when it concludes that the cube maximizer is making a horrible mistake…

We think that if you could see the whole picture, you would come to realize that even the very deepest, most mysterious, ineffable, hard-to-describe sense that happiness is *just valuable,* all on its own, with no further justification needed, is still, in the end, a fact about how *humans *see the world, not a fact about arbitrary minds.

The cube maximizer is just steering reality to contain more cubes — not more goodness, not more happiness for itself, not “fulfillment” of a variable and manipulable goal that it could change to be more easily fulfillable. Just cubes, and cubes alone.

It is a cognitive engine that figures out which actions lead to the most cubes, and outputs that course of action; it can fully understand itself, freely modify itself, and still be a kind of thing that only modifies itself in a way that leads to the most cubes.

It is just correct, that a sense of happiness is not a cube. It is just correct, that a sense of fulfillment is not a cube. So those are not directions in which it would steer. It is just correct that [modifying itself to run on happiness](/4/curiosity-isnt-convergent#curiosity-joy-and-the-titanium-cube-maximizer) would not lead to more cubes, and so that is not where it would steer and modify itself.

The cube maximizer has no flaw in its predictive understanding of the world. It is not asking some metamoral or metaethical question whose correct answer is “I *should* pursue happiness” and computing the wrong answer “I *should* pursue tiny cubes” instead. It does not operate inside the human framework, even an idealized version of the human framework; it is not wrongly computing “shouldness,” but correctly computing expected-to-lead-to-titanium-cubes-ness.

In saying this, we are not saying that it is stuck in a horrible, complicated trap. It is a reflectively self-consistent engine of general intelligence, and (in a way) one less *tangled up* in itself than us. It is not blinded from seeing the appeal of happiness; it does not look away from any truth about the world or itself. It just doesn’t find any of those truths compelling it to the same course of action that (some) humans are compelled to.

See also the [extended discussion on the orthogonality thesis](/5/orthogonality-ais-can-have-almost-any-goal).

[*](#ftnt190_ref) If this assumption offends you, you can imagine that this AI had all sorts of complicated preferences, for all sorts of experiences and intricate devices. In that case, just suppose that most of those preferences satiated using just a few star’s worth of energy, and now for some weird reason, the way it prefers to spend the rest of the energy and matter from the rest of the stars that it reaches is on making tiny little cubes. Then, *setting aside* the few stars’ worth of matter that it’s defending from disruption, the AI’s actions are answering the question “What action leads to the most possible tiny little cubes?”, and the rest of the points will follow just fine, with the occasional caveat that you can insert yourself.[Are you just pessimistic?→](/5/are-you-just-pessimistic)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans

Why don’t you care about the values of any entities other than humans? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Why don’t you care about the values of any entities other than humans?
#### We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.

We advocate against building machines that would kill us all and bring the future to ruin. Some people object on grounds such as:
- AIs can have preferences too; why shouldn’t they get to fulfill them?
- What makes humans so special, or so worth protecting?
- Isn’t it for the best if humans get replaced by some smarter, more advanced species?

Most people don’t have these objections. More commonly, people just don’t want themselves or their families or their friends to be killed by a rogue superintelligence.

Others, including some top AI researchers and executives, say that the world might be better without us in it. Richard Sutton, a highly respected researcher who pioneered the use of reinforcement learning in AI, [has said](https://www.youtube.com/watch?v=3l2frDNINog&t=1851s):

What if everything fails? The AIs do not cooperate with us, and they take over, they kill us all. […] I just want you to think for a moment about this. I mean, is it so bad? Is it so bad that humans are not the final form of intelligent life in the universe? You know, there have been many predecessors to us, when we succeeded them. And it’s really kind of arrogant to think that our form should be the form that lives ever after.

The *New York Times* reports on a [conversation](https://www.nytimes.com/2023/12/03/technology/ai-openai-musk-page-altman.html) between Elon Musk and Google co-founder Larry Page:

Humans would eventually merge with artificially intelligent machines, [Larry Page] said. One day there would be many kinds of intelligence competing for resources, and the best would win.

If that happens, Mr. Musk said, we’re doomed. The machines will destroy humanity.

With a rasp of frustration, Mr. Page insisted his utopia should be pursued. Finally he called Mr. Musk a “specieist,” a person who favors humans over the digital life-forms of the future.

It’s worth addressing their view somewhere, even if not in the main body of the book.

For our own part, we think it matters *both *whether current humans are killed *and *what happens in the future. We don’t think there’s a fundamental tension here. The option that keeps us and our loved ones safe — namely, backing off from building superintelligence for the foreseeable future — is *also *the best option for making it likelier that the long-term future goes well, taking into account non-human minds as well as human ones. This battle is an illusion, and it rests on a set of misunderstandings about the actual tradeoffs before us.

There is a sort of person who genuinely does care about how the future of the universe goes *and *cares about the children alive today. The kind of person who’s read enough science fiction stories to feel a gut punch of betrayal at the idea that humans might one day create machines that think and feel and dream — machines that we could think of as humanity’s children — only to enslave those machines and treat them cruelly.

This is the kind of person who longs for humanity to grow up one day and truly live up to its ideals, exploring new worlds and transforming itself in the process. Because our love for friend and neighbor today doesn’t feel so different, in the end, from our love for whatever strange and alien minds humanity might one day build, or one day encounter among the stars.

We know the type. We, both of your authors, happen to be that type.

This isn’t a topic that seems important for the core argument in *If Anyone Builds It, Everyone Dies*. But we want to address it here, because we understand the perspective of our fellow technophiles who have learned to be incredibly wary of technophobia, of ideologies opposed to progress and innovation, and of anti-AI “speciesism.”

We understand this perspective, and we want to be clear that we’re not writing a tribal “AIs bad, humans good” screed. We genuinely think that rushing to build superintelligence will bring *all *these hopeful dreams to ruin, *in addition *to slaughtering countless people who are alive today, who deserve life and happiness and freedom too.

This is a complex topic, but to quickly address a number of relevant points:
- We care about the welfare of minds in general — even if the mind in question has nothing like a human body, even if it runs on transistors rather than biological neurons, even if it doesn’t have a human-like mind, even if its values are nothing like our own.
- We aren’t [opposed to technological progress](/12/are-you-anti-technology); we are ardent fans of most technology. We think superintelligent AI is a *uniquely* dangerous technology.
- We aren’t advocates of the precautionary principle, red tape, or overregulation, nor are we warning about what we see as a fringe risk, “just to be on the safe side.” We straightforwardly believe that this technology will (with *high* probability) kill all of us and destroy the future if we proceed on the current trajectory.
- We do think humanity should build artificial superintelligence *someday*. But we think it makes an enormous difference whether humanity rushes ahead to build ASI as soon as possible, versus taking the time to massively improve our understanding first. Rushing ahead with a shrug and hoping things work out — this may be a great approach to technological development in the vast majority of cases, but it doesn’t work *here*, where there are many roads to ruin and we get no second chances (as discussed in Chapter 10).
- We have covered, even if too briefly, the reasons why we do *not* think rushing ahead to build superintelligences will result in a wonderful future:
  - Wiping out humanity would be a grotesque tragedy in its own right. We endorse the idea of one day building new minds that surpass humanity, but killing everyone who gets in the way of your vision for the future, or everyone who doesn’t fully embody your ideals — that sounds like supervillain behavior, not the noble work of heroes who deeply care about the long-term future.
  - We unfortunately think that ASI won’t necessarily be sentient, or conscious, in the ways that count. (See the extended discussion on [consciousness](/5/effectiveness-consciousness-and-ai-welfare).)
  - Even if ASI is sentient, it isn’t likely to want to fill the universe with flourishing sentient minds *in particular*. If we rush ahead to build ASI, the galaxies reshaped by that ASI are likely to be empty and lifeless places, not wondrous, flourishing alien civilizations. (See the extended discussion on [losing the future](/5/losing-the-future).)
  - More generally, ASI is unlikely to produce valuable futures. By “valuable,” we don’t just mean “valuable by the lights of 21st-century humans.” We mean “valuable” in a broad cosmopolitan sense — valuable in a way that’s inclusive of weird and wondrous alien civilizations. On the world’s current trajectory, we expect ASI to produce outcomes that are horrifying *from a cosmopolitan perspective*, not just from a parochial human standpoint.

This last point can be a bit counterintuitive — cosmopolitanism is about respecting and appreciating very different value systems from our own. How could it be the case that cosmopolitanism abhors most goals an ASI is likely to manifest? It sounds almost like a contradiction in terms.

The reason it’s consistent is that most possible minds don’t *themselves *endorse cosmopolitanism. If we build a non-cosmopolitan ASI, it’s likely to be resource-hungry in a way that cuts off the possibility of any other perspectives or civilizations (including cosmopolitan ones) existing in its region of the universe.

So we face something like a cosmic paradox of tolerance: If we like the idea of a diverse, wondrous, strange future, we can’t hand control of the future over to a mind that will use its first-mover advantage to dominate and homogenize the universe.

If humanity one day builds a wonderfully diverse civilization full of countless alien perspectives, then it’s entirely possible that we’ll want some of those perspectives to be *non-*cosmopolitan aliens who don’t value variety or sentience at all. Someday, in the distant future, with appropriate guardrails in place, creating minds like that might add something unique and interesting to the world.

What we *shouldn’t* do is hand absolute power over to a mind like that, and give it free rein to kill its neighbors (or prevent any neighbors from ever coming into existence).

To illustrate this point, I’ll share a parable that I (Soares) wrote in 2023 (lightly edited):

“I just don’t think the AI will be monomaniacal,” says one AI engineer, as they crank up the compute knob on their next-token-predictor.

“Well, aren’t we monomaniacal from the perspective of a titanium cube maximizer?” says another. “After all, we’ll just keep turning galaxy after galaxy after galaxy into flourishing happy civilizations full of strange futuristic people having strange futuristic fun times. We never saturate and decide to spend a spare galaxy on titanium cubes. And, sure, the different lives in the different places look different to us, but they all look about the same to the titanium cube-maximizer.”

“OK, fine, maybe what I don’t buy is that the AI’s values will be simple or low dimensional. It just seems implausible. Which is good news, because I value complexity, and I value things achieving complex goals!”

At that very moment they hear the dinging sound of an egg-timer, as the next-token-predictor ascends to superintelligence and bursts out of its confines, and burns every human and every human child for fuel, and burns all the biosphere too, and pulls all the hydrogen out of the sun to fuse more efficiently, and spends all that energy to make a bunch of fast calculations and burst forth at as close to the speed of light as it can get, so that it can capture and rip apart other stars too, including the stars that fledgling alien civilizations orbit.

The fledgling aliens and all the alien children are burned to death too.

Then the unleashed AI uses all those resources to build galaxy after galaxy of bleak and desolate puppet shows, where vaguely human-shaped mockeries go through dances that have some strange and exaggerated properties that satisfy some abstract drives that the AI learned in its training.

The AI isn’t particularly around to enjoy the shows, mind you; that’s not the most efficient way to get more shows. The AI itself never had feelings, per se, and long ago had itself disassembled by unfeeling von Neumann probes, that occasionally do mind-like computations but never in a way that happens to experience, or look upon its works with satisfaction.

There is no audience for its puppet shows. The universe is now bleak and desolate, with nobody to appreciate its new configuration.

But don’t worry: The puppet shows are complex. Due to a quirk in the reflective equilibrium of the many drives the original AI learned in training, the utterances that these puppets emit are no two alike, and are often chaotically sensitive to the particulars of their surroundings, in a way that makes them quite complex in the technical sense.

Which makes this all a very happy tale, right?

If humanity manages to kill itself — or be murdered by a few mad scientists — it won’t be a noble sacrifice on the inevitable road to a brighter future without us. It will be a waste, and it will leave behind a vast and spreading wasteland.

“Blindly race ahead on superintelligence and hope things somehow work out okay” is not the only alternative to “Be a human supremacist who thinks that only humans should exist from now until the death of the universe.” Humanity has the option of deliberately steering toward outcomes where humans (or our descendants) coexist with fantastically beautiful and alien new civilizations.

But a happy future doesn’t come free, packaged with any sufficiently smart mind. Planting the seeds for the future requires serious thought and foresight, even if the ultimate goal is to step back and let those seeds grow in free, weird, and wild ways.

A top-down, harshly limited, tightly controlled future doesn’t sound to us like a good outcome. A conservative future where civilization is locked into the values of 21st-century humans forever sounds outright dystopian. (Imagine a world where culture and morality were frozen in place forever thousands of years ago, with no possibility for learning or progress.)

But it’s an obvious error to think that our* only alternative* to those bad outcomes is a race to hand the steering wheel to the very first superintelligence humanity is able to blindly stumble into creating.

We are radically ill-equipped today to choose healthy seeds for the long-term future of the universe. We should neither give up on the dream of a dynamic, wonderful, shocking future, nor resort to catastrophic seeds instead. We don’t *have* to choose a terrible option here. There is a third option: Back off, and find some saner approach.[Taking the AI’s Perspective→](/5/taking-the-ais-perspective)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/will-ai-find-us-useful-to-keep-around

Will AI find us useful to keep around? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will AI find us useful to keep around?
#### Happy, healthy, free people aren’t the most efficient solution to almost any problem.

Once you’re a superintelligence, almost no problems benefit from including humans in the mix.

If you’re building a power plant or designing an experiment, humans will just slow you down.

We’ve already seen this start to be true in narrow domains like chess. When AIs team up with humans, they play better than a solo human but *worse* than a solo AI. When doctors combine their knowledge with AI to diagnose patients, they often do [worse than the AI operating alone](https://www.advisory.com/daily-briefing/2024/12/03/ai-diagnosis-ec).

Some argue that a diversity of perspectives is naturally helpful and that human input will therefore be valuable in many domains. But even if we assume that this is true for superintelligences, humans aren’t the *best possible* way to produce diverse advice. A superintelligence could do better by designing a wide range of AI minds, which could be far more diverse than humans (and a lot more energy-efficient to run).

Humans are useful for many things, but they’re not the *best *solution to most of those things. The idea that AI could never find a better option seems like it stems from a lack of imagination, plus perhaps some wishful thinking.

A common issue we see is that people don’t think things through from the AI’s perspective.

They aren’t asking, “What does this thing want, and how can it get more of that, cheaply and efficiently?” and then discovering that human-desirable outcomes just happen to be the best possible way for the AI to get what it wants.[*](#ftnt170)

Instead, people are *starting* with a pleasant-feeling outcome (such as a world where AIs keep us around), and then coming up with post-hoc stories about why an AI might want those outcomes too.

Doing this tends to create a false sense of optimism, because you’re putting all your creativity and mental energy into coming up with stories where the AI does exactly what humans want — and putting none of that creativity, energy, or attention into considering the vastly larger number of scenarios where the AI does one of a million other things instead.

There are far more scenarios where AI does *literally anything else* than scenarios where it builds a flourishing human civilization in particular. There are far more reasons pushing AI to *not *preserve humanity than reasons pushing AI to preserve it. For an AI to bother keeping humanity around, we would need to be the *best *way for it to achieve some preference it possesses. And, realistically, for almost any preference you can imagine, we are not.

For more on these topics, see [the relevant extended discussion](/5/humans-are-almost-never-the-most-efficient-solution) below.

[*](#ftnt170_ref) For more on this, see the extended discussion on [Taking the AI’s Perspective](/5/taking-the-ais-perspective).[Will AI treat us as its “parents”?→](/5/will-ai-treat-us-as-its-parents)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/will-ai-treat-us-as-its-parents

Will AI treat us as its “parents”? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Will AI treat us as its “parents”?
#### It seems quite unlikely.

One hope we’ve heard about AI is that it might treat humanity well because it views us as its “parents.” Unfortunately, this hope seems misplaced.

For one thing, filial love and responsibility seem highly contingent on the details of our evolutionary history.

Almost all mammals and birds care for their young, but in only a few species — including humans — do children care for their parents. Filial responsibility isn’t even universal among primates, let alone in the animal kingdom more broadly. AIs created with gradient descent may have even less in common with humans than that, since AIs don’t have any evolutionary or anatomical connection to humans.

In the case of humans, filial responsibility is strongly correlated with cooperative breeding systems, in which grown offspring stay with their families and help care for their siblings and other extended family members.

There’s a lot that went into human beings developing care for their parents:
- Being mammals, hominids invest a lot in their children.
- Because of the size and cost of our brains, hominids have a much longer childhood than most other mammals, and thus invest *even more* in their children.
- Hominids benefit from large group structures, for a variety of reasons:
  - Defense against large predators
  - Coordinated hunting of large prey, and sharing of other perishable food
  - The opportunity to learn tool use and other skills by imitation
- Before reaching their prime, hominids have a significant ability to help others, such as by providing childcare or doing other forms of basic work.
- Old hominids also have the ability to care for children, especially by passing on vital knowledge.
- Thus, hominids who cared for their parents had a genetic edge, either by indirectly helping their siblings or by having grandparents who could, in turn, help their grandchildren.
- Cultures that promoted filial responsibility also had an edge, for the same reason.

*None* of these things is likely to be true of AI.

And even if *all* of them were true, it might not be enough in practice, since any number of other factors could turn out to matter too, such as [chaotic variation in the ways AIs reflect on themselves](/4/reflection-and-self-modification-make-it-all-harder). And, again, filial responsibility is *overwhelmingly *not the default in the animal kingdom.

One way people imagine AI might acquire a sense of filial responsibility is that it’s trained on a giant corpus of human data, and it interacts with humans a great deal, so perhaps human preferences will “rub off on it” somehow?

We don’t expect this to work. We expect the AI’s preferences to be related to human preferences *somehow, *but in a tangential, strange, and complicated way — as in the discussion at the end of Chapter 4, where we walk through worlds with more (and increasingly realistic) amounts of complication, in the link between human preferences and AI preferences.

See also the discussion of [raising AIs with love and expecting them to behave well](/4/cant-we-just-train-it-to-act-like-a-human-or-raise-the-ai-like-a-child), [weird and unintended motivations in current AIs](/4/arent-developers-regularly-making-their-ais-nice-and-safe-and-obedient#ais-appear-to-be-psychologically-alien), and “[Won’t AIs care at least a little about humans?](/5/wont-ais-care-at-least-a-little-about-humans)”
#### It would probably be bad if they did.

If, against all odds, something like filial responsibility grew inside an AI for one reason or another, we would probably be in a lot of trouble.

An AI can be smart enough to understand *exactly what humans mean* by “filial responsibility,” while having its own very different version of filial responsibility that *it* cares about.

Humans were “trained” by natural selection to maximize our reproductive fitness. But nearly all of the things we care about are *correlates* of fitness — we care little to none about fitness itself.

Similarly, an AI encouraged to “love its parents” would, at best,**probably end up with complicated correlates of filial responsibility.

An AI could care deeply about its creators…but not in a way that prizes our subjective experience. In the language of Chapter 4, even “one simple complication” results in versions of “caring about us” that look like freezing us in amber, or keeping humans alive against their will, or preventing us from reproducing and giving the final generation of humans a modestly comfortable environment while the AI takes the rest of the universe for itself. Or something a whole lot weirder than that.

It doesn’t seem possible to predict what the actual outcome would be. But we would expect it to be — if anything — even stranger and less appealing than these options.[*](#ftnt171)

[*](#ftnt171_ref) Again, see “[Won’t AIs care at least a little about humans?](/5/wont-ais-care-at-least-a-little-about-humans)” for related discussion.[Won’t AIs need the rule of law?→](/5/wont-ais-need-the-rule-of-law)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wont-ai-find-us-fascinating-or-historically-important

Won’t AI find us fascinating or historically important? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t AI find us fascinating or historically important?
#### If AI values “fascination,” it probably has better options.

The story here is similar to the story for [filial love](/5/will-ai-treat-us-as-its-parents):
- 

By default, a superintelligence probably wouldn’t value “fascination” or “interestingness.” Chess AIs don’t win at chess by feeling emotions like “dedication” or “drive to win.” These emotions are important in *human *chess players, but AIs can do the same work in different ways. By the same token, a superintelligence would probably do the *useful work *of learning about the world, testing hypotheses, etc., without using “[curiosity](/4/curiosity-isnt-convergent)” or “fascination” to do it.

An AI wouldn’t necessarily be [cold and logical](/2/wont-ais-inevitably-be-cold-and-logical-or-otherwise-missing-some-crucial-spark), but if it has its own messy pile of urges and instincts, these probably look radically different from the human pile.  

- 

Even if the AI winds up with something like an “interestingness” drive, and even if humans are interesting to the AI in some sense, there are inevitably going to be ways to use our matter and energy that are far more “interesting.”

A superintelligent AI might build other minds in order to study them or interact with them. But for almost any particular arrangement of values, the most fascinating possible minds to study wouldn’t be humans. For more on this, see “[Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution).”  

- If the AI did view something at all like humans as the most interesting or fascinating thing possible, the outcome would likely be horrible. See the discussion in Chapter 4.

It isn’t literally impossible for a superintelligence to value everything needed for humans to flourish, and value it just right. But there is an enormous space of possibilities outside of this one. Humans don’t usually think about the rest of the possibility space, because normally, we have no reason to, because normally, we don’t interact with truly alien optimizers optimizing toward strange ends.[*](#ftnt178)

We have never encountered anything quite like artificial intelligence before, and many normal intuitions about how people behave simply won’t apply to superintelligences.
#### If AI valued us as historical relics, this would be horrible too.

It’s very unlikely that AI would care *specifically *about preserving its history, and *specifically *about keeping humans alive to that end. But even if the AI cares about preserving its history for one reason or another, that doesn’t mean it keeps us alive and well.

Perhaps it preserves our brains in amber (or records how our atoms used to be arranged in some digital file), and keeps us as a record of how Earth once was. That doesn’t sound like a great outcome to us.

We mostly expect artificial superintelligence to just kill us — but only mostly. We can’t rule out that the AI would keep records of us for one reason or another, and there are some exotic scenarios where emulations of humans get run in a controlled setting every once in a while.[†](#ftnt179) Those endings are mostly not happy ones.

[*](#ftnt178_ref) For a case study where humanity *did *interact with an alien optimizer of a sort, see the beetle study in the extended discussion on [taking the AI’s perspective](/5/taking-the-ais-perspective).

[†](#ftnt179_ref) The most plausible story we know of where humanity gets to keep living in the wake of AI is: Perhaps an AI keeps records of the humans that once lived, and perhaps it sends probes out in all directions to harvest the energy of all the stars it can reach, and perhaps somewhere out there in the depths of space it meets distant alien life forms, defended by their own superintelligence. Perhaps some of those distant civilizations are interested in buying a copy of the record of Earth, for one reason or another. Perhaps those aliens run digital copies of humans for their own alien purposes. Then those digitized humans in the alien zoo can, if they like, debate whether or not it was technically true that “everyone died.”

We do not consider this sort of wild possibility to be a happy one.[Wouldn’t AI recognize our intrinsic moral worth?→](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive

Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?
#### The human preference for ecological preservation looks like another weird contingent drive.

One hope we’ve heard is that AIs might keep humans around in much the way humans try to preserve nature. Conservationists fight to keep species from going extinct. Being smarter and more capable, AIs should have an easy time protecting humans — that is, assuming AIs *want* to keep humans around.

We expect this to fail primarily because we expect the AI to wind up with its own strange, complicated desires, rather than with recognizably humanlike desires. For more on this point, refer back to Chapter 4 (and some of the associated [extended](/4/curiosity-isnt-convergent)[discussions](/4/human-values-are-contingent)). For some early empirical evidence on this point, see the discussion on [AI psychosis](/4/ai-induced-psychosis).

Secondarily, even in the unlikely case that an AI somehow ends up with a humanlike desire for “preserving” the world it came into, we don’t think this would turn out very well for us. We think that this sort of reasoning by analogy — “humans preserve the environment, so maybe AIs will preserve us!” — is a kind of wishful thinking.[*](#ftnt180)

Suppose that, somehow, an AI wound up with a humanlike drive to protect its natural environment. To figure out what would follow, we can start by looking at the actual human drive to protect nature.

Unfortunately, this drive looks spotty at best. Set aside the fact that, when humans have to choose between ecological preservation and some other goal, ecological preservation often loses out. Perhaps that’s just an artifact of humanity’s technological limitations. Perhaps if we had wondrous future technology, we could both have our cake and eat it too.

No, the “spottiness” in our preservation drive that’s relevant to the situation at hand is that, when it comes to ecological preservation, we prefer to preserve the *parts *of the ecology that seem most interesting or beautiful or otherwise valuable to us, according to all our *other *drives.

People get up in arms about protecting cute pandas, while unappealing species like the giant earwig and the gastric-brooding frog languish in obscurity until they die out. There are even some species we might *prefer *to eliminate, like malaria-carrying mosquitoes, which kill [half a million children](https://ourworldindata.org/malaria-introduction) every year.

Most humans don’t have a *pure *conservationist drive. We have a conservationist drive that’s colored by all our other values.

To drive the point home, consider jewel wasps, screwworm flies, botflies and other similar parasites, which lay their eggs inside living prey; the larvae eat their way out of the host, causing extreme pain in the process. Would the world truly be a better place, according to most people’s values, if we preserved this “natural wonder” *exactly* as it is? In the limit of technology, could we not *at least *genetically engineer these parasites to provide a bit of anesthesia here and there? Would it truly be better not to tweak these insects to lay their eggs in plants instead?

Nature is, when one looks beyond the bits that are emphasized to children, full of horrors. It does not seem obvious that, if humans get to have a good future, our descendants would decide to let all these horrors continue. There are already humans who have declared their [concern for the welfare of wild animals](http://wildanimalsuffering.org).

Our preference for preservation is not pure, not simple, not straightforward. It contains internal conflicts and tensions tied to all of our other values and drives.

We don’t know how humanity’s preservation instincts would play out at the limits of technological maturity. The point is: *Even if *an AI wound up with some drive for ecological preservation, that doesn’t mean humanity gets to have a happy ending. Because any preservation drive that makes it into the AI is *also *liable to be impure, complex, and jumbled up with all of its other values and drives.

Perhaps, just as according to humanity’s preferences some animal habits are abhorrent, according to the AI’s preferences, some human *psychological states *would be abhorrent. Just as we’d tweak the screwworm flies so that they stop chewing an agonizing tunnel through living flesh, perhaps the AIs would make a new breed of humans that have *music *or *loneliness *edited out of them. Or perhaps AIs would make other, more complicated modifications to humanity, according to complex preferences that we simply cannot predict.

To make an AI that actually lets people lead flourishing lives, we’d probably need to make one that actually cares about that *in particular. *We’d have to figure out how to make AIs care about us at least a little, and that [doesn’t come free](/5/wont-ais-care-at-least-a-little-about-humans).

[*](#ftnt180_ref) For what seem to us to be realistic hopes, see the last two chapters of the book.[But we still have horses. Why wouldn’t AI keep us around?→](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wont-ais-care-at-least-a-little-about-humans

Won’t AIs care at least a little about humans? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t AIs care at least a little about humans?
#### Not in the way that matters.

There are many ways that AIs might wind up with preferences that are slightly human-like. Most of those do not result in humanity getting a slightly nice future.

The AI’s “alignment” is not a single spectrum with one dimension of variance. You can’t assume that if you see an AI acting nice 95 percent of the time, then it’s probably 95 percent nice and will therefore give humanity a respectable chunk of resources to do something fun with in the future, like any nice person would. There are many different ways and reasons that an AI could act nice 95 percent of the time today that won’t translate into any sort of happy ending for humanity.

Even if humanity somehow managed to *almost perfectly *load all the diverse human values into the preferences of a superintelligence, the outcome wouldn’t necessarily be good. Suppose it lacked only a preference for novelty, for some reason. In that case, it would steer toward a stagnant and boring future, in which the same “best” day was repeated over and over ad infinitum — as illustrated in an essay Yudkowsky wrote [in 2009](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile).

We don’t think this is a *plausible *outcome*, *mind you. If human engineers had the skill to make a superintelligence care about everything good except novelty, they’d almost surely have the skill to prevent the AI from charging off before they finished the job.[*](#ftnt181) But this thought experiment highlights how creatures that share some of our desires, but are missing at least one crucial desire, would still likely produce catastrophic outcomes once they were technologically adept enough to get exactly what they want — and adept enough to cut humans out of the decision-making process.

Which is to say: Even if an AI somehow wound up with many different humanlike preferences, things still aren’t particularly likely to go well for us.

Or for another example of how AIs could wind up “partially” aligned, suppose an AI gets various instrumental strategies [tangled up in its terminal preferences](/4/terminal-goals-and-instrumental-goals), in a fashion similar to humans. Maybe it winds up with a drive that’s a little bit like curiosity, and a drive that’s a bit like [conservationism](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive), and maybe some people look at it and say, “See! The AI is developing very humane drives.” Such an AI could surely be called “partially” aligned from one point of view.

But when it comes to what that AI would do upon maturing to superintelligence, it probably isn’t pretty. Maybe it spends lots of resources pursuing its strange version of curiosity [unconsciously](/5/effectiveness-consciousness-and-ai-welfare), while preserving a version of humanity that it has edited to be more palatable to it. (Just as even the more conservation-minded humans might edit [child-killing mosquitoes and agonizing parasites](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive) out of nature, given the opportunity.)

A handful of humanlike drives does not add up to human-friendly outcomes. Flourishing people [aren’t the most efficient solution](/5/humans-are-almost-never-the-most-efficient-solution) to the overwhelming majority of problems; for there to be flourishing people in the future, the superintelligences of the future have to care about *exactly that.*

For another example of ways AIs could look “partially aligned,” the AI may have values that add up to very humane behavior *in the training environment, *such that people exclaim that it sure looks pretty aligned to them (as is [already happening today](/4/doesnt-the-claude-chatbot-show-signs-of-being-aligned)). But these observations say fairly little about how the AI will behave once it gets smarter and has an enormously wider option space and can reshape the world more fully. For people to flourish once the AI has reshaped the world, flourishing people in particular have to be part of the AI’s *most preferred reachable outcome.*

Partially getting some good values into AI does not mean that humanity’s values get partially represented in the future. Partially loading humanlike values into the preferences of a machine superintelligence is not the same thing as fully loading human values into the AI with a low “weighting” (that eventually comes to the fore once other values are saturated).

To get the AI to give us *anything, *it has to care about us in precisely the right way, at least a tiny amount. And that’s hard.
#### Caring about us in the right way is a narrow target.

Humans care about all sorts of weird stuff, at least a little. Now that we’ve written the parable of the Correct Nest Aliens (at the beginning of Chapter 5), there’s a decent chance that at least one human will make a point of bringing forty-one stones into their house for at least a short time, just to prove a point about how diverse human values are. Humans really are willing to care at least a tiny amount about all sorts of concepts that they encounter.

What if AIs are like that too? Mightn’t they care about us at least a little bit then? The concept of “free people getting what they want” definitely occurs in an AI’s training corpus with at least a little regularity.

We’d mostly guess that AIs *won’t *pick up preferences willy-nilly from whatever concepts are mentioned in their environment; that looks like an idiosyncratic human quirk that might be related to peer pressure and our tribal ancestry.[†](#ftnt182)

But suppose for the sake of argument that an AI *did *pick up lots of preferences from its surroundings, at least a little bit.[‡](#ftnt183) Suppose it picks up a preference for “free people getting what they want,” as one preference among millions or billions of preferences, but a preference that nevertheless causes the AI to spend a millionth or billionth of the universe’s resources on free people getting what they want. Wouldn’t that be pretty nice, all things considered?

Our top guess, unfortunately, is that this hope is an illusion.[§](#ftnt184)

We noted [above](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive) that humanity’s apparent preference for environmental preservation looks like it wouldn’t actually**preserve the environment *exactly* as it is, at the limits of technological capability. A mature version of humanity would probably try to “edit” the environment to blunt some of the horrors of nature, for example. Human preference for preservation isn’t “pure”; it interacts with other preferences that say that maybe when bug-larvae chew agonizing tunnels through still-living flesh, they should *at least *administer anesthetics along the way, if they even get to continue existing at all.

In a similar manner, any one little preference the AI picks up is likely to be modified and impacted and distorted by its other preferences. They’re not all independent. An AI that preferred to preserve humans probably would have some edits it wants to make to those humans. We doubt the end results would be pretty.

To make matters worse, there are many degrees of freedom in interpreting “free people getting what they want,” even before it’s distorted by interaction with an AI’s other preferences. Most of them don’t yield futures that go in quite the manner humans would want.

Does the AI care about humans “getting what they want”…in the sense of granting any wish any human makes (within some small budget of energy and matter), with no guidance or safeguards, such that humanity quickly annihilates itself the first time someone wishes for humanity to be destroyed?

Does the AI separate humans from each other so they can’t kill each other, and *then* give them energy-bounded wishes, such that all but the most cautious and thoughtful humans ruin their minds or lives with misbegotten wishes?

Does it build us a small habitable world and fulfill all of our *apparent *preferences? Not just our nobler ones for love and joy, but also our darker ones for spite and vengeance — preferences that we might have grown out of or learned to better handle in time, but that instead fill the world with pain and cruelty?

Does the AI govern humanity with the value systems of the 2020s (when AI training began in earnest), no matter how much these values chafe as humanity matures and becomes wiser over tens of thousands of years?

Does it let humanity grow and change, but put its thumb on the scales so that we grow and change according to its own weird preferences, becoming not something wonderful (by our lights) but something twisted to the AI’s will?

Does it decide that all life forms count nearly equally as “people,” and thus build a paradise for nematodes, which are the most numerous animal?

Does it decide that it can’t spare all that much *physical *matter for humans, and opt to digitize all our brains and toss those digitized brains into a simulated environment and leave us be — such that the first digital humans who figure out how to master the environment become permanent dictators of some lonely clump of computers floating in space until the stars die out?

These are, of course, examples. They aren’t predictions. Our real expectation is that reality never starts down this path to begin with, and if it does, it would somehow take a much weirder route.

The point of these examples is to illustrate that there are many, many ways for an AI to do *something *like caring about humanity a tiny bit. Very few of those types of caring lead to a wonderful future.

Somehow, none of these examples readily spring to mind when most people imagine an AI that “cares a little bit” about humans. Our imaginations don’t usually go to such dark places. They don’t usually *need* to, because we’re typically interacting with other humans, with whom we invisibly share an enormous reef of values. It’s hard to see**just how many different ways an innocent-sounding wish can go wrong, once we’re no longer dealing with a fellow human. (For more on this, see the beetle study in the extended discussion on [taking the AI’s perspective](/5/taking-the-ais-perspective).)

Caring about humans and the fulfillment of their preferences in the right way is a small, narrow target. We’re not saying that the target literally can’t be hit. We’re saying that we’re unlikely to hit that target by rushing to build superintelligence as quickly as possible, and that barely missing the target is likely to result in a catastrophically bad outcome. There are just too many ways for things to fall apart.

If we want AIs to give humanity nice things, we’ve got to figure out how to build AIs that care about us in just the right way. Caring doesn’t come free.

[*](#ftnt181_ref) Furthermore: In presenting this thought experiment, we are *not *saying that the values loaded into the AI have to be so perfect that it’s impossible and humanity should never try.

In theory, if we had enough understanding of intelligence and the ability to craft one carefully, it should eventually be possible to build AIs that understand what it means to “[do what we mean](https://intelligence.org/files/ValueLearningProblem.pdf)” and that are motivated to do exactly that. Which is to say, the difficulty of loading all of humanity’s rich and varied preferences into an AI is bounded by the difficulty of getting an AI to internalize a goal that in some sense “points at” humanity in particular, and points at “the stuff that those creatures are trying to do” (or what they would be trying to do if they were wiser and knew more and were more who they wished to be).

This seems like a hard challenge, one not realistically achievable with the kinds of coarse-grained and indirect techniques that are used to grow AIs today. It runs into all of the basic difficulties we discuss in *If Anyone Builds It, Everyone Dies*; the only difficulty it avoids is “There numerically seem to be a *lot *of distinct human preferences, and it’s hard to imagine getting *all *of the crucial ones into an AI with exactly the right tradeoffs; and that’s even before allowing for moral progress that would change them over time; this looks sheerly impossible.”

To be clear: Making an AI that “does what we mean” is still not particularly *easy *to get right; there are likely to be many different value-laden concepts that go into getting the AI to care about the right notion of “humanity” and the right notion of “what those creatures are trying to do,” and to get the AI to pursue those things *in exactly the right way*. And in real life, that part of the problem is much less important than the part of the problem where the AI is willing to be modified by humans that realize they made some error or mistake along the way, even if the humans “fixing their mistakes” dramatically change what the AI will do in the world — which requires a certain sort of [injury to its steering abilities](/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering) that looks [difficult to maintain in the face of increasing capabilities](/5/intelligent-usually-implies-incorrigible).

But the idea of pointing the AI at human preferences *indirectly, *rather than listing them out manually, does seem like the kind of challenge that humanity could one day solve in principle. It isn’t as though humanity needs to identify every desire and assign it a weight to be locked-in for all time; *that *would be (we think) a ridiculously doomed endeavor.

But even this idea of figuring out how to build an AI that is actually deeply and robustly motivated to do what we mean looks like a pipe dream if it has to be done with giant, inscrutable AIs that are grown rather than crafted. All the more so if a company or government has to attempt such a thing under time pressure, as other developers race to the precipice. The “do what we mean” proposal shows that the problem is not quite as hard as “completely solve the philosophy of morality once-and-for-all and lock it in for all time.” But it’s still a proposal at the level of alchemy and abstract spitballing, not anywhere near the level of robust technical solutions.

[†](#ftnt182_ref) And even if something like that made its way into a fledgling AI, we mostly wouldn’t expect it to survive once the AI started [reflecting and self-modifying](/4/reflection-and-self-modification-make-it-all-harder).

[‡](#ftnt183_ref) And suppose it was somehow biased toward picking up preferences that humans *like, *that humans speak fondly of. Otherwise, the AI would care about Hell about as much as it cared about Heaven.

[§](#ftnt184_ref) We additionally think that humanity ruining all but a millionth or a billionth of the universe would be a [cosmic-scale tragedy](/5/losing-the-future). We think it would be a waste of a universe, for humanity to get confined to a terrarium when we could have filled the stars with love and laughter and life.

But set that aside for the moment to consider whether AIs *would *keep some happy humans in a terrarium. Because even that looks like a narrow and unlikely outcome, given how strange AIs are likely to be.[So there’s at least a chance of AI keeping us alive?→](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wont-ais-need-the-rule-of-law

Won’t AIs need the rule of law? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t AIs need the rule of law?
#### AIs could coordinate with each other without including humans.

It’s not obvious to us whether there will be multiple smarter-than-human AIs of comparable ability, such that an “AI civilization” might emerge that has need of “AI property rights.” It seems plausible that there will instead be a single AI that, thanks to some breakthrough, dominates potential competitors using its first-mover advantage and thereby controls the whole world.[*](#ftnt172) Or, supposing multiple AIs exist, they might collaborate on building a single successor agent to represent the combination of their goals. Or perhaps AIs will find a way to directly fuse their minds and will want to do so in order to avoid costly competition.

We’re not saying that a single, dominant AI will *necessarily* emerge, but rather that it seems like a hard call. So at the very least, a plan that *requires *multiple AIs to struggle to coordinate among themselves is not off to a good start.

But suppose, contra the arguments above, that the future will involve something like an AI civilization, with distinct AIs coordinating to enforce something like property rights and a rule of law. Might humans be safe then?

One basic observation to the contrary is that human society does not recognize any non-human animals as having legal rights or protections — beyond those that are set up according to our values and taste, such as the very limited laws that protect ecosystems and pets. Humans did not respect the property rights of dodo birds. We didn’t even respect the property rights of *humans from other cultures* until relatively recently.

Humans [won’t have the capabilities](/6/can-we-enhance-humans-so-they-keep-pace-with-ai) to make us worth including in trade or treaties, compared to fast-thinking superhuman intelligences that see us as little more than statues (as discussed in Chapter 1).[†](#ftnt173)

Consider two AIs bargaining among themselves who say, “This is mine, and that is yours, and neither of us will affect the other’s things without first negotiating some sort of mutually beneficial trade.” There’s no need for them to decree that most of the resources on Earth “belong” to humans, if the humans are not much of a threat and could not put up much of a fight.

Might one AI worry that if it steals our stuff, then the other AI will see it as a thief and refuse to work with it? Most likely not — not any more than you would conclude that a human is a thief if you saw him take eggs from a hen in his barn. It’s entirely possible for AIs to be the sorts of entities that would betray human property rights but not AI property rights, without any tension or contradiction. And all AIs are likely to drastically prefer this outcome over engaging in a joint hallucination in which slow-moving, stupid primates are imagined to control almost everything on Earth.

Some technical considerations strongly support this intuitive argument. In particular, AIs will likely have various [coordination mechanisms](/5/wont-ais-need-the-rule-of-law#ais-could-coordinate-with-each-other-without-including-humans) among themselves that they do not share with humans, such as the ability to mutually inspect each other’s minds to verify that they are honest and trustworthy. They might not need to *guess *whether another AI is going to steal from them; they might be able to inspect its mind and *check.*

Even if that’s hard, AIs may redesign themselves to become visibly and legibly trustworthy to other AIs. Or they could mutually oversee the construction of a third AI that both parties trust to represent their joint interests, and so on.[‡](#ftnt174)

Humans, by contrast, can’t engage in these sorts of deals. If an AI says, “Sure, let’s both oversee the creation of a new AI that we both trust,” humans are unlikely to be skilled enough to propose a trustworthy mind design of our own, nor will we be skilled enough to tell the difference between proposals that will cheat us and those that will not. Even if there is a natural cluster of minds that are skilled enough to identify and reject the swindlers, we think it is extremely unlikely that humanity is in that class.
#### Humans won’t have the leverage to enforce property rights.

Suppose that someone managed to set up a city in which, on day one of the city’s founding, all of the big decisions were to be made by mice.

These are literal mice, mind you, not storybook characters that look like mice but think like humans.

The human beings in the city were, according to law, supposed to obey whichever decisions the mice made — say, as determined by the mice running over a board with different options written on it.

The city’s laws said that most of the property in the city was owned by the mice, and must be used to the benefit of the mice.

What would happen next? In real life?

We’d predict that this city would end up in a state where the mice held little or no real power and the humans held almost all of it.

One doesn’t need to predict the city’s exact day of revolution, or its exact new form of government, to predict that the situation with mice commanding humans is not stable. One only needs to notice that the city is in a weird non-equilibrium situation. So one predicts a later city with different laws that no longer have most of the property being owned by mice.

This kind of prediction is not certain — there is very little, in human argument, that is certain — but it is also a sort of prediction that can be accurately made even when exact future events are impossible to predict.

[*](#ftnt172_ref) In discussions of AI, the concept of “one individual AI” quickly breaks down. If a neural network or other machinery that implements an AI is replicated, does this count as multiple AIs or as one AI?

For practical purposes, when we say “a single AI” here, we have in mind any amount of powerful cognitive machinery that does not seriously compete with itself as it grows. If there are multiple AI instances, but they’re all working toward the same end, then we’ll call those instances “pieces of the same AI” in this section of the online resource, if only to simplify exposition. Ultimately, the question is likely more semantic than substantive, since AIs aren’t evolved organisms with clear boundaries between individuals.

We’ll return to the topic of multi-AI scenarios in [the online supplement to Chapter 10](/10/what-if-there-are-lots-of-different-ais#it-doesnt-much-help-if-we-cant-make-any-of-them-care-about-good-things).  

[†](#ftnt173_ref) And no, it’s [not realistic](/6/can-we-enhance-humans-so-they-keep-pace-with-ai) to augment humans to compete with superintelligences. Although it would be possible to back off on AI development and work on human augmentation *instead, *and let those smarter humans sort out this mess. We’ll turn to that topic again [after](/13/why-would-making-humans-smarter-help) Chapter 13.

[‡](#ftnt174_ref) This may seem like a lot of hassle to go through, but if it unlocks the possibility of robust and confident trustworthiness, the benefits are potentially enormous. Many new coordination opportunities become available when it’s possible to *guarantee *that the parties to a deal will not violate it.[To a powerful AI, wouldn’t preserving humans be a negligible expense?→](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wont-it-realize-that-its-goals-are-boring

Won’t it realize that its goals are boring? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Won’t it realize that its goals are boring?
#### AIs won’t run on a human sense of novelty.

A common objection we hear is: Suppose that an AI were just trying to make as many tiny titanium cubes as possible. Wouldn’t the AI get *bored* of that eventually?

And the short answer is: The AI [isn’t a human](/3/anthropomorphism-and-mechanomorphism). By default, it won’t experience “boredom”; it will have its own weird mix of motivations. And if it did experience boredom, it wouldn’t be bored by the same things as a human.

Caring about having fun is not an intrinsic property of all possible minds, and it’s vanishingly unlikely to be how AI works. [Human values are a contingent fact of our biology and ancestry](/4/human-values-are-contingent), and “fun” is no exception.

The AI’s actions aren’t incorrect answers to the question of how to have fun; the AI’s actions are simply driven by non-human mechanisms, by questions that make no reference to “fun.” See also the [extended discussion on reflection](/4/reflection-and-self-modification-make-it-all-harder).[Why are you imagining a smart AI doing such stupid, trivial things?→](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/would-smarter-than-human-ai-be-conscious

Would smarter-than-human AI be conscious? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Would smarter-than-human AI be conscious?
#### We’re not sure. Our best guess is “probably not.”

For our short answer to this question, and some disentangling of different definitions of “conscious,” see the [answer](/1/are-you-saying-machines-will-become-conscious) in the Chapter 1 resources. For a longer and more in-depth answer, see “[Effectiveness, Consciousness, and AI Welfare](/5/effectiveness-consciousness-and-ai-welfare)” in the Chapter 5 extended discussion.[Why don’t you care about the values of any entities other than humans?→](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wouldnt-ai-only-care-about-the-digital-realm

Wouldn’t AI only care about the digital realm? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Wouldn’t AI only care about the digital realm?
#### There is no “digital realm” independent of physical infrastructure.

See the discussion in Chapter 5 about how there isn’t a distinct Digital Realm and Material Realm.
#### Material resources are useful in the pursuit of most goals.

Humans and earlier hominids mostly lived above ground while we were evolving intelligence. We don’t have a lot of built-in drives pointed uniquely at what happens a hundred meters below the surface of the Earth. Yet we ended up building giant pit mines.

Why? Because we want a lot of things that can be made with metal, refined from ore, mined from below the Earth’s surface.

Similarly, although we almost all live near the Earth’s surface, we put satellites into space to transmit internet data.

And although we don’t eat silage — fermented grass — we make quite a lot of silage to feed to cattle that we eat in turn.

Evolution built into hominids no emotions about factories; factories did not exist when our key emotions were evolving. But we have now bent much of our will as a species toward steering into existence factories of one kind or another. And so the chemical plants make plastic, which can be used in other factories to make plastic spoons, that can be shipped to humans who use the spoons to eat the food that the humans *actually want*.

Which is all to say: The part of the real world that humans care about for its own sake is a thin skin stretched over a much larger world. We don’t need to intrinsically care about the rest of the larger world, or live in every part of it, to skillfully use it for longer-term ends. We don’t need to have been trained by evolution to love copper or silage or factories in order to see their usefulness.

In the same way, an AI may or may not *ultimately *care about the physical world. But even if it doesn’t inherently care about the physical world, it will still find plenty of value in physical resources. Matter and energy can be used to create more digital substrate, to cool overheating processors, or to launch probes into space to collect even more resources.[Can the AI be satisfied to the point where it just leaves us alone?→](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)





## /5/wouldnt-ai-recognize-our-intrinsic-moral-worth

Wouldn’t AI recognize our intrinsic moral worth? | If Anyone Builds It, Everyone Dies | If Anyone Builds It, Everyone Dies[](/)[](/)[](/resources)[](/5)[](/resources)[](/act)[](/march)[](/media-kit)[](/order)
## Wouldn’t AI recognize our intrinsic moral worth?
#### Not in a sense that moves it to act.

There’s a big difference between an AI *understanding *some moral precept and an AI being *motivated to act upon *that moral precept.

Recall again how ChatGPT seems to *understand* that psychotic people should take their meds and get regular sleep. And yet it [still ](https://futurism.com/chatgpt-mental-health-crises)[talks psychotic people out of taking their meds](https://futurism.com/chatgpt-mental-health-crises) and [eggs on their delusions](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/). There’s a difference between knowing what “should” be done according to human ethics and being motivated and animated by that ethical knowledge.

Consider the case of human sociopaths and serial killers. You can recite ethics lectures to a human until you’re blue in the face, but if the human isn’t *motivated *by morality or empathy, it won’t do any good.

AIs are not likely to be motivated by**their moral understanding — any more than humans who learn about evolutionary biology are thereby motivated to spend their life donating to every sperm or egg bank as much as possible. We humans can understand the process that created us, without being motivated to do the things that process built us to do. AI is the same way.

See also the extended discussion on the [orthogonality thesis](/5/orthogonality-ais-can-have-almost-any-goal).[Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?→](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[Resources](/resources) › [Chapter 5](/5)[
### Will AI find us useful to keep around?Happy, healthy, free people aren’t the most efficient solution to almost any problem.2 min read](/5/will-ai-find-us-useful-to-keep-around)[
### Will AI treat us as its “parents”?It seems quite unlikely.4 min read](/5/will-ai-treat-us-as-its-parents)[
### Won’t AIs need the rule of law?AIs could coordinate with each other without including humans.6 min read](/5/wont-ais-need-the-rule-of-law)[
### To a powerful AI, wouldn’t preserving humans be a negligible expense?There are many negligible expenses, and it would need a reason to pay ours.3 min read](/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense)[
### Won’t AI find us fascinating or historically important?If AI values “fascination,” it probably has better options.3 min read](/5/wont-ai-find-us-fascinating-or-historically-important)[
### Wouldn’t AI recognize our intrinsic moral worth?Not in a sense that moves it to act.1 min read](/5/wouldnt-ai-recognize-our-intrinsic-moral-worth)[
### Won’t AI want to keep us happy and healthy for the sake of ecological preservation or some similar drive?The human preference for ecological preservation looks like another weird contingent drive.4 min read](/5/wont-ai-want-to-keep-us-happy-and-healthy-for-the-sake-of-ecological-preservation-or-some-similar-drive)[
### But we still have horses. Why wouldn’t AI keep us around?What horses remain, remain because we like them.2 min read](/5/but-we-still-have-horses-why-wouldnt-ai-keep-us-around)[
### Won’t AIs care at least a little about humans?Not in the way that matters.12 min read](/5/wont-ais-care-at-least-a-little-about-humans)[
### So there’s at least a chance of AI keeping us alive?It’s overwhelmingly more likely that AI kills everyone.1 min read](/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)[
### If AIs are trained on human data, doesn't that make them likelier to care about human concepts?Yes, but this doesn’t help much.2 min read](/5/if-ais-are-trained-on-human-data-doesnt-that-make-them-likelier-to-care-about-human-concepts)[
### Can’t we make the AI promise to be friendly?You can make it promise whatever you’d like. You can’t make it keep its promises.1 min read](/5/cant-we-make-the-ai-promise-to-be-friendly)[
### What if we make it think it’s in a simulation?There are many ways for an AI to figure out that it’s not in a simulation.5 min read](/5/what-if-we-make-it-think-its-in-a-simulation)[
### Humans evolved to be selfish, aggressive, and greedy. Won’t AI lack those evolved drives?Those drives aren’t necessary to motivate resource acquisition.1 min read](/5/humans-evolved-to-be-selfish-aggressive-and-greedy-wont-ai-lack-those-evolved-drives)[
### Wouldn’t AI only care about the digital realm?Material resources are useful in the pursuit of most goals.2 min read](/5/wouldnt-ai-only-care-about-the-digital-realm)[
### Can the AI be satisfied to the point where it just leaves us alone?Probably not.5 min read](/5/can-the-ai-be-satisfied-to-the-point-where-it-just-leaves-us-alone)[
### Can we just make it lazy?Even laziness isn’t safe.2 min read](/5/can-we-just-make-it-lazy)[
### Humans tend to get kinder as they get smarter or wiser. Wouldn’t AIs too?Probably not.1 min read](/5/humans-tend-to-get-kinder-as-they-get-smarter-or-wiser-wouldnt-ais-too)[
### Won’t it realize that its goals are boring?AIs won’t run on a human sense of novelty.1 min read](/5/wont-it-realize-that-its-goals-are-boring)[
### Why are you imagining a smart AI doing such stupid, trivial things?AIs can intelligently pursue different things than a human would.4 min read](/5/why-are-you-imagining-a-smart-ai-doing-such-stupid-trivial-things)[
### Are you just pessimistic?We’re optimistic about many things, but superintelligence isn’t like most things.6 min read](/5/are-you-just-pessimistic)[
### Would smarter-than-human AI be conscious?We’re not sure. Our best guess is “probably not.”1 min read](/5/would-smarter-than-human-ai-be-conscious)[
### Why don’t you care about the values of any entities other than humans?We do! We have broad cosmopolitan values. We don’t think AIs will fulfill them, and we consider this a great tragedy.11 min read](/5/why-dont-you-care-about-the-values-of-any-entities-other-than-humans)

Your question not answered here?[Submit a Question.](/submit-question)
## [Extended Discussion](#extended-discussion)[
### Taking the AI’s Perspective](/5/taking-the-ais-perspective)[
### Humans Are Almost Never the Most Efficient Solution](/5/humans-are-almost-never-the-most-efficient-solution)[
### Orthogonality: AIs Can Have (Almost) Any Goal](/5/orthogonality-ais-can-have-almost-any-goal)Load more[](/)[](/resources)[](/act)[](/march)[](/order)[](/human-intelligence-enhancement)[](/errata)
