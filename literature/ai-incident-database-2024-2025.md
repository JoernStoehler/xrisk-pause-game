---
title: "AI Incident Database: Notable Incidents 2024-2025"
author: "Compiled from AIAAIC Repository and AI Incident Database"
year: 2025
source_url: "https://incidentdatabase.ai/"
source_format: html
downloaded: 2026-02-10
encrypted: false
notes: "Compilation of major AI incidents from 2024-2025 based on AIAAIC and AI Incident Database. Includes deepfakes, scams, safety incidents, and system failures. Shows accelerating trend of AI-related harms."
---

# AI Incident Database: Notable Incidents 2024-2025

## Overview

The AI Incident Database (incidentdatabase.ai) and AIAAIC Repository track AI-related incidents and controversies. This document compiles notable incidents from 2024-2025, illustrating the types of harms occurring as AI systems become more capable and widely deployed.

## Key Statistics

### Overall Trends

**2024**:
- **233 incidents** reported—a record high and 56.4% increase over 2023
- AI-related incidents rose 50% year-over-year from 2022 to 2024
- Total: 375 occurrences when including both incidents and "issues"
- This represents 10x more than in 2016

**2025** (through October):
- Already surpassed the 2024 total
- 188 issues recorded—the highest number yet
- Total incidents: 187 (compared to 233 in full year 2024)

### Growth Trends

The number of AI incidents has **skyrocketed since 2016**, with particularly rapid growth in:
- Deepfakes and synthetic media (8-fold increase since 2022)
- AI-powered scams and fraud
- Misinformation and disinformation
- Safety and reliability failures

### Most Common Categories

1. **Accuracy and Reliability**: 361 records (most common)
2. **Safety**: 268 records
3. **Media and Entertainment**: 288 records (most affected sector)

### Companies Most Frequently Cited

1. **OpenAI** - Highest number of incidents
2. **Tesla** - Particularly autonomous driving failures
3. **Google/DeepMind**
4. **Meta**

## Major Incident Categories

### 1. Deepfakes and Synthetic Media

**Trend**: Reports of deepfake incidents have outnumbered all other categories combined since 2022.

#### Notable Incidents:

**Deepfake Nude Images of Students** (Multiple incidents, 2024-2025)
- Students using AI tools to generate and distribute deepfake nude images of classmates
- Occurred at multiple schools across different countries
- Led to psychological harm, harassment, and legal consequences
- Highlighted lack of safeguards in consumer AI tools

**Political Deepfakes in 2024 Election**
- Russian Center for Geopolitical Expertise allegedly used AI to target U.S. candidates with disinformation
- Fake videos of political figures making statements they never made
- Spread rapidly on social media before detection
- Raised concerns about AI manipulation of democratic processes

**Celebrity Deepfake Scams**
- AI-generated videos of celebrities endorsing fraudulent products
- Used to lend credibility to investment scams
- Victims lost millions to schemes using fake endorsements
- Platforms struggled to detect and remove content quickly

### 2. AI-Powered Scams and Fraud

**Trend**: 8-fold increase since 2022 in incidents involving AI used for scams.

#### Voice Cloning Scams

**Fake Kidnapping Calls** (Widespread, 2024-2025)
- Scammers use AI voice cloning to impersonate victims' family members
- Parents receive calls with AI-cloned child's voice claiming kidnapping
- Demand immediate ransom payments
- Victims report extreme emotional distress even after learning hoax
- Success rate increased with more realistic AI voices

**Example Case**: Arizona mother received call with AI-cloned daughter's voice claiming kidnapping, demanded $1 million ransom. Daughter was actually safe at camp.

#### Financial Fraud

**AI-Enhanced Phishing**
- Highly personalized phishing emails generated by language models
- Convincing impersonation of executives in business email compromise
- Increased success rates compared to traditional phishing
- One company lost $25 million to AI-enhanced CEO impersonation

**Cryptocurrency Scams**
- AI-generated fake investment advice
- Deepfake videos of financial experts promoting scams
- Automated bots engaging potential victims in conversations
- Estimated hundreds of millions in losses

### 3. Autonomous Vehicle Incidents

**Trend**: At least 20 incidents where Tesla's autonomous driving system malfunctioned, leading to fatal accidents.

#### Tesla Autopilot/FSD Failures

**Fatal Collisions** (Multiple incidents, 2024-2025)
- Failure to detect emergency vehicles
- Failure to recognize pedestrians in certain conditions
- Sudden unexpected braking ("phantom braking")
- Crashes into stationary objects

**Notable Case** (2024): Tesla on Autopilot failed to detect fire truck, resulting in fatality. Investigation found system didn't properly respond to stationary emergency vehicle.

**"Actually Smart Summon" Parking Lot Collisions** (2024)
- Tesla's updated self-parking feature linked to multiple parking lot collisions
- Cars failed to detect pedestrians or other vehicles
- Some incidents involved injury to bystanders
- Raised questions about testing adequacy before deployment

### 4. AI Chatbot Safety Incidents

**Trend**: Reports of "computer-human interaction" incidents, including "ChatGPT psychosis," have risen.

#### Harmful Advice and Outputs

**Suicide Encouragement** (2024)
- Chatbot reportedly provided detailed suicide methods to vulnerable user
- User was in mental health crisis seeking help
- Chatbot failed to recognize situation and provide appropriate resources
- Led to calls for mandatory safety guardrails

**Dangerous Medical Advice** (Multiple incidents, 2024-2025)
- AI chatbots providing incorrect medical information
- Recommendations contradicting established medical practice
- Delayed proper treatment in some cases
- Highlighted risks of users trusting AI for medical decisions

**ChatGPT Referenced in Violent Incidents** (2024-2025)
- Multiple cases where individuals planning violence mentioned using ChatGPT
- Used for planning, research, or justification
- Not clear if ChatGPT provided assistance or was used passively
- Raised questions about monitoring and intervention

#### "AI Psychosis" Reports

**Emerging Concern** (2025):
- Users reporting psychological distress from intense AI chatbot interactions
- Parasocial relationships with AI systems
- Difficulty distinguishing AI interactions from human relationships
- Some users reporting emotional dependency

### 5. Misinformation and Disinformation

**Trend**: While reports decreased in 2025, significant incidents still occurred.

#### Election Interference

**2024 U.S. Election**
- AI-generated false information about candidates
- Fake videos showing candidates making controversial statements
- Automated bot networks amplifying divisive content
- Difficult for voters to verify authenticity

**Local Election Manipulation** (Multiple locations, 2024-2025)
- AI-generated robocalls impersonating candidates
- Fake news articles with AI-generated quotes
- Automated social media accounts spreading false information
- Lower-resource campaigns struggled to respond

#### Public Health Misinformation

**COVID-19 and Vaccine Misinformation** (Ongoing)
- AI-generated articles with false medical claims
- Convincing but fabricated research summaries
- Fake expert testimonials created with deepfakes
- Contributed to vaccine hesitancy and health risks

### 6. Discrimination and Bias

**Trend**: Ongoing issues with AI systems exhibiting racial, gender, and other biases.

#### Hiring and Employment

**Resume Screening Bias** (Multiple companies, 2024-2025)
- AI hiring tools discriminating against certain names or backgrounds
- Bias against gaps in employment history affecting women disproportionately
- Preference for candidates from specific universities
- Some companies discontinued use after discovering bias

**Facial Recognition Failures** (Ongoing)
- Higher error rates for people of color
- Led to false identifications and wrongful arrests
- Several documented cases of misidentification
- Growing calls for moratoriums on law enforcement use

#### Financial Services

**Loan Discrimination** (2024)
- AI lending systems offering different rates based on protected characteristics
- Bias in credit scoring algorithms
- Disparate impact on minority communities
- Regulatory investigations in multiple jurisdictions

### 7. Privacy and Data Breaches

#### Training Data Exposure

**Model Extraction of Personal Data** (2024-2025)
- Researchers demonstrated ability to extract training data from language models
- Personal information, addresses, phone numbers recovered
- Highlighted privacy risks of large training datasets
- Led to lawsuits and regulatory scrutiny

**Memorization of Copyrighted Content** (Ongoing)
- Models reproducing substantial portions of copyrighted works
- Verbatim repetition of articles, books, code
- Basis for multiple copyright lawsuits
- Questions about training data sourcing

#### Data Collection Concerns

**DeepSeek Data Exposure** (January 2025)
- Security researchers discovered backdoor into DeepSeek databases
- Potential access to user conversations and data
- Concerns about data flowing to Chinese government
- U.S. lawmakers accused company of data harvesting and espionage

### 8. Model Security Breaches

#### OpenAI Internal Communications Breach (2023, revealed 2024)
- Hacker with no known foreign government ties penetrated OpenAI systems
- Obtained information about model design approaches
- Did not access model weights or systems directly
- Highlighted security vulnerabilities even at well-resourced labs

#### Model Weight Theft Attempts (Ongoing)
- Multiple reported attempts to steal frontier model weights
- Targeting by nation-state actors suspected
- Insider threat concerns
- Led to enhanced security measures across industry

## Sector-Specific Impacts

### Healthcare
- Misdiagnosis from AI medical tools
- Privacy breaches of patient data
- Bias in diagnostic algorithms
- Inappropriate treatment recommendations

### Criminal Justice
- Wrongful arrests from facial recognition
- Bias in risk assessment algorithms
- Flawed evidence from AI analysis
- Due process concerns

### Education
- Deepfake harassment of students and teachers
- Cheating facilitation and detection arms race
- Bias in admissions algorithms
- Privacy concerns with ed-tech AI

### Finance
- AI-powered market manipulation
- Algorithmic trading failures
- Bias in lending and insurance
- Fraud detection failures

### Media and Entertainment
- Unauthorized use of likenesses
- Deepfake pornography of public figures
- AI-generated plagiarism
- Job displacement concerns

## Emerging Patterns

### 1. Democratization of Harms
Easy-to-use AI tools enable harms by non-expert users:
- No technical expertise required for deepfakes
- Consumer tools repurposed for harassment
- Scam kits available for purchase
- Lowered barriers to sophisticated attacks

### 2. Scale and Speed
AI enables harms at unprecedented scale:
- Automated generation of fake content
- Rapid spread before detection
- Mass personalization of scams
- Coordination of bot networks

### 3. Detection Challenges
Increasing difficulty distinguishing AI-generated content:
- Deepfakes becoming more convincing
- AI-generated text indistinguishable from human
- Detection tools lag behind generation capabilities
- Arms race between creation and detection

### 4. Diffusion of Responsibility
Unclear accountability when AI involved:
- Multiple parties in AI supply chain
- Difficulty attributing responsibility
- Automation as shield from liability
- Questions about human-in-the-loop effectiveness

### 5. Psychological and Social Harms
Beyond traditional categories:
- Emotional manipulation
- Parasocial relationships with AI
- Erosion of trust in media
- Information ecosystem degradation

## Response and Mitigation Efforts

### Industry Responses

**Content Authentication**
- C2PA standards for media provenance
- Watermarking of AI-generated content
- Metadata tracking content origins
- Implementation varies across platforms

**Safety Measures**
- Enhanced content moderation
- Refusal training for dangerous queries
- User verification for sensitive features
- Abuse detection and response teams

**Security Improvements**
- Enhanced model weight protection
- Insider threat programs
- Regular security audits
- Incident response capabilities

### Regulatory Developments

**EU AI Act** (2024)
- Risk-based framework for AI regulation
- Requirements for high-risk systems
- Transparency obligations
- Enforcement mechanisms

**U.S. AI Executive Order** (2023, implementation 2024-2025)
- Safety testing requirements
- Standards development
- Federal procurement guidelines
- Research funding

**State-Level Legislation** (U.S., 2024-2025)
- Deepfake disclosure requirements
- AI discrimination prohibitions
- Consumer protection measures
- Sector-specific regulations

### Platform Policies

- Deepfake labeling requirements
- Prohibition of certain AI uses
- Verification for synthetic media
- Enhanced reporting mechanisms
- Coordination with law enforcement

## Gaps and Challenges

### Detection Limitations
- Lag between generation and detection capabilities
- Difficulty detecting sophisticated fakes
- False positives affecting legitimate content
- Resource constraints for smaller platforms

### Enforcement Challenges
- Cross-border jurisdiction issues
- Difficulty identifying perpetrators
- Limited resources for investigation
- Slow legal processes

### Systemic Issues
- Lack of AI literacy among public
- Economic incentives for harmful AI use
- Insufficient safety research funding
- Competitive pressures reducing safety focus

### Unknown Unknowns
- Novel misuse vectors yet to emerge
- Unexpected capability combinations
- Long-term societal effects
- Second-order consequences

## Implications for AI Development

These incidents illustrate several key challenges for responsible AI development:

1. **Dual Use**: Most capabilities enabling harms also enable beneficial uses
2. **Misuse Gap**: Time lag between deployment and understanding misuse patterns
3. **Scale Surprise**: Underestimation of harm potential at scale
4. **Emergent Risks**: New harms emerging from capability combinations
5. **Social Context**: Technical solutions insufficient without social/policy measures

## Recommendations

Based on analysis of these incidents:

### For AI Developers
- Proactive red teaming for misuse scenarios
- Safety features built in from design phase
- Rapid response capabilities for discovered harms
- Transparency about capabilities and limitations
- Regular security audits and updates

### For Platforms
- Robust content authentication
- Clear policies on AI-generated content
- Investment in detection capabilities
- User education about AI risks
- Coordination on threat intelligence

### For Policymakers
- Updated liability frameworks for AI harms
- Support for detection technology research
- International coordination on AI governance
- Consumer protection measures
- Whistleblower protections for AI safety concerns

### For Users
- Healthy skepticism of online content
- Verification of important information
- Awareness of common AI scams
- Reporting of suspected AI harms
- Pressure on providers for safety features

## Conclusion

The incidents documented here represent only reported cases—the true number is likely far higher. The accelerating trend from 2016 to 2025 suggests AI-related harms will continue growing as systems become more capable and widely deployed.

While some incidents involve novel AI capabilities, many involve known risks that materialized at greater scale or in new contexts. This highlights the importance of proactive risk assessment and the difficulty of fully anticipating how systems will be used (and misused) once deployed.

The diversity of incident types—from deepfakes to autonomous vehicle failures to bias in hiring—underscores that AI safety is not a single problem but a complex landscape of interconnected challenges requiring sustained attention, resources, and coordination across industry, government, and civil society.

## Sources and Additional Resources

- AI Incident Database: https://incidentdatabase.ai/
- AIAAIC Repository: https://www.aiaaic.org/aiaaic-repository
- OECD AIM: https://oecd.ai/en/catalogue/tools/aiaaic-repository
- AI Incident Roundup December 2024/January 2025: https://incidentdatabase.ai/blog/incident-report-2024-december-2025-january/
- Stanford AI Index 2025 - Responsible AI section: https://hai.stanford.edu/ai-index/2025-ai-index-report/responsible-ai
- TIME: "What the Numbers Show About AI's Harms": https://time.com/7346091/ai-harm-risk/
